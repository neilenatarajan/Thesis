@misc{noauthor_index_nodate,
	title        = {Index - {Human}-{Learn}},
	url          = {https://koaning.github.io/human-learn/},
	urldate      = {2021-09-28},
	file         = {Index - Human-Learn:/Users/neilnatarajan/Zotero/storage/PXG36WQR/human-learn.html:text/html}
}
@misc{noauthor_climate_nodate,
	title        = {Climate change governance, legislation and litigation {Archives}},
	journal      = {Grantham Research Institute on climate change and the environment},
	url          = {https://www.lse.ac.uk/granthaminstitute/research-areas/climate-change-governance-legislation-and-litigation/},
	urldate      = {2021-10-15},
	abstract     = {This research area covers the governance and political economy of transformations to low-carbon and climate-resilient societies at the international and domestic levels, including the implementation of the Paris Agreement. It also covers the specific roles of legislation, litigation, sub-national government and the private sector in these processes. Our Climate Change Laws of the World database is part of this research area.},
	language     = {en-GB},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/9NXWLHZD/climate-change-governance-legislation-and-litigation.html:text/html}
}
@inproceedings{ijcai2023p819,
	title        = {Human-AI Collaboration in Recruitment and Selection},
	author       = {Natarajan, Neil},
	year         = 2023,
	month        = 8,
	booktitle    = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, {IJCAI-23}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organisation},
	pages        = {7089--7090},
	doi          = {10.24963/ijcai.2023/819},
	url          = {https://doi.org/10.24963/ijcai.2023/819},
	note         = {Doctoral Consortium},
	editor       = {Edith Elkind}
}
@misc{noauthor_river_nodate,
	title        = {River {Action} {UK}},
	journal      = {River Action UK},
	url          = {https://riveractionuk.com/},
	urldate      = {2021-10-15},
	abstract     = {River Action is a campaigning body committed to addressing the severe problem of river pollution across the UK.},
	language     = {en-US},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/QHUVWVWT/riveractionuk.com.html:text/html}
}
@misc{noauthor_wye_nodate,
	title        = {Wye {CitSci} {Water} {Quality} {Monitoring} {Map}},
	url          = {https://www.brecon-and-radnor-cprw.wales/WyeCitSci/},
	urldate      = {2021-10-15},
	file         = {Wye CitSci Water Quality Monitoring Map:/Users/neilnatarajan/Zotero/storage/NCX6CY9B/WyeCitSci.html:text/html}
}
@misc{noauthor_chef_nodate,
	title        = {Chef {Watson} - {IBM}},
	url          = {https://researcher.watson.ibm.com/researcher/researcher.watson.ibm.com/researcher/view_group.php},
	urldate      = {2021-11-04},
	copyright    = {© Copyright IBM Corp. 2016},
	abstract     = {IBM Research},
	language     = {en-US},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/B8RXQEXV/view_group.html:text/html}
}
@misc{noauthor_ai_nodate,
	title        = {{AI} {Alignment}, {Philosophical} {Pluralism}, and the {Relevance} of {Non}-{Western} {Philosophy} - {AI} {Alignment} {Forum}},
	url          = {https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of},
	urldate      = {2022-04-09},
	abstract     = {This is an extended transcript of the talk I gave at EAGxAsiaPacific 2020. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it d…},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/CQSQKGBB/ai-alignment-philosophical-pluralism-and-the-relevance-of.html:text/html}
}
@misc{noauthor_how_nodate,
	title        = {How to explain {AI} systems to end users: a systematic literature review and research agenda {\textbar} {Emerald} {Insight}},
	url          = {https://www.emerald.com/insight/content/doi/10.1108/INTR-08-2021-0600/full/html},
	urldate      = {2022-05-11},
	file         = {How to explain AI systems to end users\: a systematic literature review and research agenda | Emerald Insight:/Users/neilnatarajan/Zotero/storage/96MA57PV/html.html:text/html}
}
@misc{noauthor_welcome_nodate,
	title        = {Welcome to {Spinning} {Up} in {Deep} {RL}! — {Spinning} {Up} documentation},
	url          = {https://spinningup.openai.com/en/latest/},
	urldate      = {2022-08-03},
	file         = {Welcome to Spinning Up in Deep RL! — Spinning Up documentation:/Users/neilnatarajan/Zotero/storage/5XCZV9R4/latest.html:text/html}
}
@misc{noauthor_cs_nodate,
	title        = {{CS} 294 {Deep} {Reinforcement} {Learning}, {Fall} 2017},
	url          = {http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html},
	urldate      = {2022-08-03},
	file         = {CS 294 Deep Reinforcement Learning, Fall 2017:/Users/neilnatarajan/Zotero/storage/S96LC5M8/index.html:text/html}
}
@misc{noauthor_side_nodate,
	title        = {Side effects project idea},
	journal      = {Google Docs},
	url          = {https://docs.google.com/document/d/1wy_ObScI1az0R3gZ4u1Y_XVJno2opj84-Egu8PHVOuI/edit?usp=embed_facebook},
	urldate      = {2023-01-09},
	abstract     = {Preventing catastrophe by avoiding unintended consequences   Joshua D Greene, Nikola Jurkovic, Neil Natarajan, Xavier Roberts-Gaal  Background  We want to build intelligent systems that can do a range of tasks well. But, as these systems get more capable of doing tasks, they become increasingly ...},
	language     = {en}
}
@misc{noauthor_doing_nodate,
	title        = {Doing {Design} {Ethnography} {\textbar} {SpringerLink}},
	url          = {https://link.springer.com/book/10.1007/978-1-4471-2726-0},
	urldate      = {2023-02-24}
}
@misc{noauthor_contextual_nodate,
	title        = {Contextual {Design} - an overview {\textbar} {ScienceDirect} {Topics}},
	url          = {https://www.sciencedirect.com/topics/computer-science/contextual-design},
	urldate      = {2023-02-24},
	file         = {Contextual Design - an overview | ScienceDirect Topics:/Users/neilnatarajan/Zotero/storage/T8DUDCVS/contextual-design.html:text/html}
}
@misc{noauthor_thematic_nodate,
	title        = {Thematic {Analysis} {\textbar} {SAGE} {Publications} {Ltd}},
	url          = {https://uk.sagepub.com/en-gb/eur/thematic-analysis/book248481},
	urldate      = {2023-02-24},
	file         = {Thematic Analysis | SAGE Publications Ltd:/Users/neilnatarajan/Zotero/storage/NEKDU69G/book248481.html:text/html}
}
@misc{noauthor_ai_nodate-1,
	title        = {{AI} {Overreliance} {Is} a {Problem}. {Are} {Explanations} a {Solution}?},
	journal      = {Stanford HAI},
	url          = {https://hai.stanford.edu/news/ai-overreliance-problem-are-explanations-solution},
	urldate      = {2023-04-05},
	abstract     = {Stanford researchers show that shifting the cognitive costs and benefits of engaging with AI explanations could result in fewer erroneous decisions due to AI overreliance.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/5L84WL9L/ai-overreliance-problem-are-explanations-solution.html:text/html}
}
@misc{noauthor_towards_nodate,
	title        = {Towards a {Science} of {Human}-{AI} {Decision} {Making}: {An} {Overview} of {Design} {Space} in {Empirical} {Human}-{Subject} {Studies} {\textbar} {Proceedings} of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	url          = {https://dl.acm.org/doi/10.1145/3593013.3594087},
	urldate      = {2023-07-28},
	keywords     = {\_tablet},
	file         = {Towards a Science of Human-AI Decision Making\: An Overview of Design Space in Empirical Human-Subject Studies | Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency:/Users/neilnatarajan/Zotero/storage/9KB3NJJA/3593013.html:text/html;Towards a Science of Human-AI Decision Making.pdf:/Users/neilnatarajan/Zotero/storage/5GF78IM9/Towards a Science of Human-AI Decision Making.pdf:application/pdf}
}
@misc{noauthor_ai_nodate-2,
	title        = {{AI} {Detection} {Accuracy} {Studies} — {Meta}-{Analysis} of 6 {Studies} – {Originality}.{AI}},
	url          = {https://originality.ai},
	urldate      = {2024-08-01},
	abstract     = {A comprehensive overview and meta-analysis of academic research and studies that demonstrate the exceptional performance of Originality.ai in detecting AI-generated text.},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/79M5BDZG/ai-detection-studies-round-up.html:text/html}
}
@misc{fb,
	url          = {https://www.facebook.com/facebook/videos/1292820370894105/}
}
@book{hartigan_fairness_1989,
	title        = {Fairness in employment testing:  {Validity} generalization, minority issues, and the {General} {Aptitude} {Test} {Battery}},
	shorttitle   = {Fairness in employment testing},
	year         = 1989,
	publisher    = {National Academy Press},
	address      = {Washington, DC, US},
	series       = {Fairness in employment testing:  {Validity} generalization, minority issues, and the {General} {Aptitude} {Test} {Battery}},
	note         = {Pages: xii, 354},
	abstract     = {This volume is one of a number of studies conducted under the aegis of the National Research Council/National Academy of Sciences that deal with the use of standardized ability tests to make decisions about people in employment or educational settings.  Because such tests have a sometimes important role in allocating opportunities in American society, their use is quite rightly subject to questioning and not infrequently to legal scrutiny.  At issue in this report is the use of a federally sponsored employment test, the General Aptitude Test Battery (GATB), to match job seekers to requests for job applicants from private- and public-sector employers.  Developed in the late 1940s by the U.S. Employment Service (USES), a division of the Department of Labor, the GATB is used for vocational counseling and job referral by state-administered Employment Service (also known as Job Service) offices located in some 1,800 communities around the country. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	editor       = {Hartigan, John A. and Wigdor, Alexandra K.},
	keywords     = {\_tablet, Educational Measurement, Employment Tests, General Aptitude Test Battery, Legal Processes, Private Sector, Public Sector, Test Battery, Test Validity},
	file         = {1989_Fairness in employment testing.pdf:/Users/neilnatarajan/Zotero/storage/N4XRPQWN/1989_Fairness in employment testing.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/8BREAF4E/1989-97795-000.html:text/html}
}
@misc{noauthor_6_2021,
	title        = {6 - {Debate} and {Imitative} {Generalization} with {Beth} {Barnes}},
	year         = 2021,
	month        = apr,
	journal      = {AXRP - the AI X-risk Research Podcast},
	url          = {https://axrp.net/episode/2021/04/08/episode-6-debate-beth-barnes.html},
	urldate      = {2022-08-03},
	abstract     = {Google Podcasts link},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/3MTAV2JA/episode-6-debate-beth-barnes.html:text/html}
}
@misc{noauthor_ai-written_2022,
	title        = {{AI}-{Written} {Critiques} {Help} {Humans} {Notice} {Flaws}},
	year         = 2022,
	month        = jun,
	journal      = {OpenAI},
	url          = {https://openai.com/blog/critiques/},
	urldate      = {2022-08-03},
	abstract     = {Showing model-generated critical comments to humans helps them find flaws in summaries.},
	language     = {en}
}
@article{adeeb_m_jarrah_using_2023,
	title        = {Using {ChatGPT} in academic writing is (not) a form of plagiarism: {What} does the literature say?},
	author       = {{Adeeb M. Jarrah} and {Yousef Wardat} and {Patrícia Fidalgo}},
	year         = 2023,
	month        = oct,
	volume       = 13,
	number       = 4,
	pages        = {e202346--e202346},
	doi          = {10.30935/ojcmt/13572},
	note         = {MAG ID: 4385757151},
	abstract     = {This study aims to review the existing literature on using ChatGPT in academic writing and its implications regarding plagiarism. Various databases, including Scopus, Google Scholar, ScienceDirect, and ProQuest, were searched using specific keywords related to ChatGPT in academia, academic research, higher education, academic publishing, and ethical challenges. The review provides an overview of studies investigating the use of ChatGPT in academic writing and its potential association with plagiarism. The results of this study contribute to our understanding of the use and misuse of ChatGPT in academic writing, considering the growing concern regarding plagiarism in higher education. The findings suggest that ChatGPT can be a valuable writing tool; however, it is crucial to follow responsible practices to uphold academic integrity and ensure ethical use. Properly citing and attributing ChatGPT’s contribution is essential in recognizing its role, preventing plagiarism, and upholding the principles of scholarly writing. By adhering to established citation guidelines, authors can maximize ChatGPT’s benefits while maintaining responsible usage.},
	keywords     = {\_tablet},
	file         = {Adeeb M. Jarrah et al_2023_Using ChatGPT in academic writing is (not) a form of plagiarism.pdf:/Users/neilnatarajan/Zotero/storage/CD7SJCBW/Adeeb M. Jarrah et al_2023_Using ChatGPT in academic writing is (not) a form of plagiarism.pdf:application/pdf}
}
@article{catherine_a_gao_comparing_2022,
	title        = {Comparing scientific abstracts generated by {ChatGPT} to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers},
	author       = {{Catherine A. Gao} and {Frederick M. Howard} and {N. Markov} and {E. Dyer} and {S. Ramesh} and {Yuan Luo} and {Alexander T. Pearson}},
	year         = 2022,
	journal      = {bioRxiv},
	doi          = {10.1101/2022.12.23.521610},
	note         = {S2ID: b36acdfc67612d707c95d1ed282672d3ca262be7},
	abstract     = {Background Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. Methods We gathered ten research abstracts from five high impact factor medical journals (n=50) and asked ChatGPT to generate research abstracts based on their titles and journals. We evaluated the abstracts using an artificial intelligence (AI) output detector, plagiarism detector, and had blinded human reviewers try to distinguish whether abstracts were original or generated. Results All ChatGPT-generated abstracts were written clearly but only 8\% correctly followed the specific journal’s formatting requirements. Most generated abstracts were detected using the AI output detector, with scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98\% [12.73, 99.98] compared with very low probability of AI-generated output in the original abstracts of 0.02\% [0.02, 0.09]. The AUROC of the AI output detector was 0.94. Generated abstracts scored very high on originality using the plagiarism detector (100\% [100, 100] originality). Generated abstracts had a similar patient cohort size as original abstracts, though the exact numbers were fabricated. When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68\% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14\% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, but that the generated abstracts were vaguer and had a formulaic feel to the writing. Conclusion ChatGPT writes believable scientific abstracts, though with completely generated data. These are original without any plagiarism detected but are often identifiable using an AI output detector and skeptical human reviewers. Abstract evaluation for journals and medical conferences must adapt policy and practice to maintain rigorous scientific standards; we suggest inclusion of AI output detectors in the editorial process and clear disclosure if these technologies are used. The boundaries of ethical and acceptable use of large language models to help scientific writing remain to be determined.}
}
@article{jacob_devlin_bert_2018,
	title        = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	author       = {{Jacob Devlin} and Devlin, Jacob and {Ming‐Wei Chang} and Chang, Ming-Wei and {Kenton Lee} and Lee, Kenton and {Kristina Toutanova} and Toutanova, Kristina},
	year         = 2018,
	month        = oct,
	pages        = {4171--4186},
	note         = {MAG ID: 2963341956},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@article{kalpesh_krishna_paraphrasing_2023,
	title        = {Paraphrasing evades detectors of {AI}-generated text, but retrieval is an effective defense},
	author       = {{Kalpesh Krishna} and {Yixiao Song} and {Marzena Karpinska} and {J. Wieting} and {Mohit Iyyer}},
	year         = 2023,
	note         = {ARXIV\_ID: 2303.13408 S2ID: 2969e8a14237f8244d3c825ff19bdfb3cc7fddf1},
	abstract     = {To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3\% to 4.6\% (at a constant false positive rate of 1\%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80\% to 97\% of paraphrased generations across different settings, while only classifying 1\% of human-written sequences as AI-generated. We will open source our code, model and data for future research.},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/26Z4LSTB/2303.html:text/html;Krishna et al_2023_Paraphrasing evades detectors of AI-generated text, but retrieval is an.pdf:/Users/neilnatarajan/Zotero/storage/7HN24CJJ/Krishna et al_2023_Paraphrasing evades detectors of AI-generated text, but retrieval is an.pdf:application/pdf}
}
@article{lav_r_varshney_limits_2020,
	title        = {Limits of {Detecting} {Text} {Generated} by {Large}-{Scale} {Language} {Models}},
	author       = {{Lav R. Varshney} and Varshney, Lav R. and {Nitish Shirish Keskar} and Keskar, Nitish Shirish and {Richard Socher} and Socher, Richard},
	year         = 2020,
	pages        = {1--5},
	doi          = {10.1109/ita50056.2020.9245012},
	note         = {MAG ID: 3097149262},
	abstract     = {Some consider large-scale language models that can generate long and coherent pieces of text as dangerous, since they may be used in misinformation campaigns. Here we formulate large-scale language model output detection as a hypothesis testing problem to classify text as genuine or generated. We show that error exponents for particular language models are bounded in terms of their perplexity, a standard measure of language generation performance. Under the assumption that human language is stationary and ergodic, the formulation is ex-tended from considering specific language models to considering maximum likelihood language models, among the class of k-order Markov approximations; error probabilities are characterized. Some discussion of incorporating semantic side information is also given.},
	keywords     = {\_tablet},
	file         = {Lav R. Varshney et al_2020_Limits of Detecting Text Generated by Large-Scale Language Models.pdf:/Users/neilnatarajan/Zotero/storage/33944WKU/Lav R. Varshney et al_2020_Limits of Detecting Text Generated by Large-Scale Language Models.pdf:application/pdf}
}
@misc{preserve_knowledge_nips_2018,
	title        = {{NIPS} 2017 {Test} of {Time} {Award} "{Machine} learning has become alchemy.” {\textbar} {Ali} {Rahimi}, {Google}},
	author       = {{Preserve Knowledge}},
	year         = 2018,
	month        = mar,
	url          = {https://www.youtube.com/watch?v=x7psGHgatGM},
	urldate      = {2021-11-25}
}
@article{Kitchin_2017,
	title        = {Thinking critically about and researching algorithms},
	author       = {Kitchin, Rob},
	year         = 2017,
	journal      = {Information, Communication & Society},
	publisher    = {Routledge},
	volume       = 20,
	number       = 1,
	pages        = {14–29},
	doi          = {10.1080/1369118X.2016.1154087}
}
@article{Barocas_Hood_Ziewitz_2013,
	title        = {Governing Algorithms: A Provocation Piece},
	author       = {Barocas, Solon and Hood, Sophie and Ziewitz, Malte},
	year         = 2013,
	journal      = {SSRN Electronic Journal},
	doi          = {10.2139/ssrn.2245322},
	issn         = {1556-5068},
	url          = {http://www.ssrn.com/abstract=2245322},
	language     = {en}
}
@article{Spitzer_Holstein_Morrison_Holstein_Satzger_Kühl,
	title        = {Don’t be Fooled: The Misinformation Effect of Explanations in Human-AI Collaboration},
	author       = {Spitzer, Philipp and Holstein, Joshua and Morrison, Katelyn and Holstein, Kenneth and Satzger, Gerhard and Kühl, Niklas},
	language     = {en}
}
@book{Yousafzai_Lamb_2023,
	title        = {I am malala: The girl who stood up for education and was shot by the Taliban},
	author       = {Yousafzai, Malala and Lamb, Christina},
	year         = 2023,
	publisher    = {Weidenfeld & Nicolson},
	address      = {Oxford, England},
	isbn         = 9781399608992,
	language     = {en}
}
@article{NISSENBAUM1998237,
	title        = {Will computers dehumanize education? A grounded approach to values at risk},
	author       = {Helen Nissenbaum and Decker Walker},
	year         = 1998,
	journal      = {Technology in Society},
	volume       = 20,
	number       = 3,
	pages        = {237--273},
	doi          = {https://doi.org/10.1016/S0160-791X(98)00011-6},
	issn         = {0160-791X},
	url          = {https://www.sciencedirect.com/science/article/pii/S0160791X98000116},
	abstract     = {In this paper we examine the concern that computers will dehumanize education, one aspect of the general concern over possible threats to social and ethical values resulting from the computerization of schools. Concern over computerization of schools has become a battleground for ideological debates. Our paper does not enter this fray. Rather, we devise an alternative approach, called a grounded analysis, which addresses core concerns of practicing educators and administrators in their own terms. First we examine what people seem to mean when they worry that computers may dehumanize education. We identify four versions of this concern: that children may withdraw from people and society; that the teacher-student relationship may break down; that the teaching of important values may be jeopardized; and that education may become overly standardized. We systematically evaluate each of these concerns. Reaching no simple conclusion, we find that although dehumanization is not an inevitable consequence of using computers in education, it does pose some genuine risks serious enough to justify caution. Importantly, our analysis suggests that the actions of educators and policy makers may significantly raise or lower these risks.}
}
@article{tharindu_kumarage_stylometric_2023,
	title        = {Stylometric {Detection} of {AI}-{Generated} {Text} in {Twitter} {Timelines}},
	author       = {{Tharindu Kumarage} and {Joshua Garland} and {Amrita Bhattacharjee} and {K. Trapeznikov} and {Scott W. Ruston} and {Huan Liu}},
	year         = 2023,
	journal      = {ArXiv},
	doi          = {10.48550/arxiv.2303.03697},
	note         = {ARXIV\_ID: 2303.03697 S2ID: 18e0b11dd5b1b413d33308da0379836752aaaec1},
	abstract     = {Recent advancements in pre-trained language models have enabled convenient methods for generating human-like text at a large scale. Though these generation capabilities hold great potential for breakthrough applications, it can also be a tool for an adversary to generate misinformation. In particular, social media platforms like Twitter are highly susceptible to AI-generated misinformation. A potential threat scenario is when an adversary hijacks a credible user account and incorporates a natural language generator to generate misinformation. Such threats necessitate automated detectors for AI-generated tweets in a given user's Twitter timeline. However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline. In this paper, we present a novel algorithm using stylometric signals to aid detecting AI-generated tweets. We propose models corresponding to quantifying stylistic changes in human and AI tweets in two related tasks: Task 1 - discriminate between human and AI-generated tweets, and Task 2 - detect if and when an AI starts to generate tweets in a given Twitter timeline. Our extensive experiments demonstrate that the stylometric features are effective in augmenting the state-of-the-art AI-generated text detectors.},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/I3GLTGM6/2303.html:text/html;Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines_annotated.pdf:/Users/neilnatarajan/Zotero/storage/QYB42FR6/Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines_annotated.pdf:application/pdf;Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines.pdf:/Users/neilnatarajan/Zotero/storage/QYB42FR6/Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines.pdf:application/pdf}
}
@article{vinu_sankar_sadasivan_can_2023,
	title        = {Can {AI}-{Generated} {Text} be {Reliably} {Detected}?},
	author       = {{Vinu Sankar Sadasivan} and {Aounon Kumar} and {S. Balasubramanian} and {Wenxiao Wang} and {S. Feizi}},
	year         = 2023,
	journal      = {ArXiv},
	doi          = {10.48550/arxiv.2303.11156},
	note         = {ARXIV\_ID: 2303.11156 S2ID: fb47aa3c541fc2a9b340c9d2a3572860811767d6},
	abstract     = {The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/5DDI8322/2303.html:text/html;Sadasivan et al_2023_Can AI-Generated Text be Reliably Detected.pdf:/Users/neilnatarajan/Zotero/storage/Z7NQAG3L/Sadasivan et al_2023_Can AI-Generated Text be Reliably Detected.pdf:application/pdf}
}
@article{vivian_lai_why_2020,
	title        = {"{Why} is '{Chicago}' deceptive?" {Towards} {Building} {Model}-{Driven} {Tutorials} for {Humans}},
	author       = {{Vivian Lai} and Lai, Vivian and Liu, Han and Tan, Chenhao},
	year         = 2020,
	month        = jan,
	journal      = {arXiv: Human-Computer Interaction},
	doi          = {10.1145/10.1145/3313831.3376873},
	note         = {MAG ID: 2999438821},
	abstract     = {To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a training phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI.}
}
@inproceedings{abdu2023empirical,
	title        = {An empirical analysis of racial categories in the algorithmic fairness literature},
	author       = {Abdu, Amina A and Pasquetto, Irene V and Jacobs, Abigail Z},
	year         = 2023,
	booktitle    = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {1324--1333}
}
@techreport{acemoglu2022automation,
	title        = {Automation and the workforce: A firm-level view from the 2019 Annual Business Survey},
	author       = {Acemoglu, Daron and Anderson, Gary W and Beede, David N and Buffington, Cathy and Childress, Eric E and Dinlersoz, Emin and Foster, Lucia S and Goldschlag, Nathan and Haltiwanger, John C and Kroff, Zachary and others},
	year         = 2022,
	institution  = {National Bureau of Economic Research}
}
@inproceedings{acuna2021ai,
	title        = {Are AI ethics conferences different and more diverse compared to traditional computer science conferences?},
	author       = {Acuna, Daniel E and Liang, Lizhen},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {307--315}
}
@techreport{agarwal2023combining,
	title        = {Combining human expertise with artificial intelligence: Experimental evidence from radiology},
	author       = {Agarwal, Nikhil and Moehring, Alex and Rajpurkar, Pranav and Salz, Tobias},
	year         = 2023,
	institution  = {National Bureau of Economic Research}
}
@article{ahmed_relationship_2009,
	title        = {The {Relationship} between {Behavioral} and {Attitudinal} {Trust}: {A} {Cross}-cultural {Study}},
	shorttitle   = {The {Relationship} between {Behavioral} and {Attitudinal} {Trust}},
	author       = {Ahmed, Ali M. and Salas, Osvaldo},
	year         = 2009,
	journal      = {Review of Social Economy},
	publisher    = {Taylor & Francis, Ltd.},
	volume       = 67,
	number       = 4,
	pages        = {457--482},
	issn         = {0034-6764},
	url          = {https://www.jstor.org/stable/41288480},
	urldate      = {2022-12-06},
	note         = {Publisher: Taylor \& Francis, Ltd.},
	abstract     = {We study the relationship between trust in an experiment and trust measured by means of popular survey items in different countries. Students from Chile, Colombia, India, Mexico and Sweden participate in a public goods game experiment and answer a set of standard attitudinal survey questions about trust. We find that behavioral trust and attitudinal trust significantly differ among countries. Behavioral trust is highest in Sweden, followed by Latin America, and lowest in India. Attitudinal trust is highest in Chile and Sweden, followed by India and Mexico, and lowest in Colombia. Further, the predictive power of survey items also differs among countries. Trust measured by survey items is significantly related to behavioral trust in some but not in all societies. No single survey item predicts actual trust across all countries. Plausible explanations of the inconsistent relationship between behavioral and attitudinal trust across countries are discussed.},
	file         = {JSTOR Full Text PDF:/Users/neilnatarajan/Zotero/storage/EYD3TP56/Ahmed and Salas - 2009 - The Relationship between Behavioral and Attitudina.pdf:application/pdf},
	abstractnote = {We study the relationship between trust in an experiment and trust measured by means of popular survey items in different countries. Students from Chile, Colombia, India, Mexico and Sweden participate in a public goods game experiment and answer a set of standard attitudinal survey questions about trust. We find that behavioral trust and attitudinal trust significantly differ among countries. Behavioral trust is highest in Sweden, followed by Latin America, and lowest in India. Attitudinal trust is highest in Chile and Sweden, followed by India and Mexico, and lowest in Colombia. Further, the predictive power of survey items also differs among countries. Trust measured by survey items is significantly related to behavioral trust in some but not in all societies. No single survey item predicts actual trust across all countries. Plausible explanations of the inconsistent relationship between behavioral and attitudinal trust across countries are discussed.}
}
@book{Ahmed_2012,
	title        = {On Being Included: Racism and Diversity in Institutional Life},
	author       = {Ahmed, Sara},
	year         = 2012,
	month        = mar,
	publisher    = {Duke University Press},
	doi          = {10.1215/9780822395324},
	isbn         = {978-0-8223-9532-4},
	url          = {https://read.dukeupress.edu/books/book/2209/On-Being-IncludedRacism-and-Diversity-in},
	abstractnote = {What does diversity do? What are we doing when we use the language of diversity? Sara Ahmed offers an account of the diversity world based on interviews wi},
	language     = {en}
}
@misc{aivodji_fairwashing_2019,
	title        = {Fairwashing: the risk of rationalization},
	shorttitle   = {Fairwashing},
	author       = {Aïvodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, Sébastien and Hara, Satoshi and Tapp, Alain},
	year         = 2019,
	month        = may,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1901.09749},
	url          = {http://arxiv.org/abs/1901.09749},
	urldate      = {2023-10-13},
	note         = {arXiv:1901.09749 [cs, stat]},
	abstract     = {Black-box explanation is the problem of explaining how a machine learning model -- whose internal logic is hidden to the auditor and generally complex -- produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {Aïvodji et al_2019_Fairwashing_annotated.pdf:/Users/neilnatarajan/Zotero/storage/Y8EFH6JN/Aïvodji et al_2019_Fairwashing_annotated.pdf:application/pdf;Aïvodji et al_2019_Fairwashing.pdf:/Users/neilnatarajan/Zotero/storage/Y8EFH6JN/Aïvodji et al_2019_Fairwashing.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LENDNTHG/1901.html:text/html}
}
@article{ajunwa_paradox_2019,
	title        = {The {Paradox} of {Automation} as {Anti}-{Bias} {Intervention}},
	author       = {Ajunwa, Ifeoma},
	year         = 2019,
	journal      = {Cardozo Law Review},
	volume       = 41,
	number       = 5,
	pages        = {1671--1742},
	url          = {https://heinonline.org/HOL/P?h=hein.journals/cdozo41&i=1711},
	urldate      = {2024-07-10},
	language     = {eng},
	keywords     = {\_tablet},
	file         = {Ajunwa_2019_The Paradox of Automation as Anti-Bias Intervention.pdf:/Users/neilnatarajan/Zotero/storage/67ZG4NJR/Ajunwa_2019_The Paradox of Automation as Anti-Bias Intervention.pdf:application/pdf}
}
@article{albright2023hidden,
	title        = {The hidden effects of algorithmic recommendations},
	author       = {Albright, Alex},
	year         = 2023,
	journal      = {Preprint. Last accessed March},
	volume       = 28,
	pages        = 2023
}
@article{alkaissi_artificial_2023,
	title        = {Artificial {Hallucinations} in {ChatGPT}: {Implications} in {Scientific} {Writing}},
	shorttitle   = {Artificial {Hallucinations} in {ChatGPT}},
	author       = {Alkaissi, Hussam and McFarlane, Samy I},
	year         = 2023,
	month        = feb,
	journal      = {Cureus},
	doi          = {10.7759/cureus.35179},
	issn         = {2168-8184},
	url          = {https://www.cureus.com/articles/138667-artificial-hallucinations-in-chatgpt-implications-in-scientific-writing},
	urldate      = {2023-04-08},
	abstract     = {While still in its infancy, ChatGPT (Generative Pretrained Transformer), introduced in November 2022, is bound to hugely impact many industries, including healthcare, medical education, biomedical research, and scientific writing. Implications of ChatGPT, that new chatbot introduced by OpenAI on academic writing, is largely unknown. In response to the Journal of Medical Science (Cureus) Turing Test - call for case reports written with the assistance of ChatGPT, we present two cases one of homocystinuria-associated osteoporosis, and the other is on late-onset Pompe disease (LOPD), a rare metabolic disorder. We tested ChatGPT to write about the pathogenesis of these conditions. We documented the positive, negative, and rather troubling aspects of our newly introduced chatbot’s performance.},
	language     = {en},
	file         = {Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications.pdf:/Users/neilnatarajan/Zotero/storage/ZQPY9WXN/Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications.pdf:application/pdf}
}
@article{allensworth2020high,
	title        = {High school GPAs and ACT scores as predictors of college completion: Examining assumptions about consistency across high schools},
	author       = {Allensworth, Elaine M and Clark, Kallie},
	year         = 2020,
	journal      = {Educational Researcher},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 49,
	number       = 3,
	pages        = {198--211}
}
@article{altonji2001employer,
	title        = {Employer learning and statistical discrimination},
	author       = {Altonji, Joseph G and Pierret, Charles R},
	year         = 2001,
	journal      = {The quarterly journal of economics},
	publisher    = {MIT Press},
	volume       = 116,
	number       = 1,
	pages        = {313--350}
}
@inproceedings{Alvarado_Waern_2018,
	title        = {Towards Algorithmic Experience: Initial Efforts for Social Media Contexts},
	author       = {Alvarado, Oscar and Waern, Annika},
	year         = 2018,
	month        = apr,
	booktitle    = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI ’18},
	pages        = {1–12},
	doi          = {10.1145/3173574.3173860},
	isbn         = {978-1-4503-5620-6},
	url          = {https://dl.acm.org/doi/10.1145/3173574.3173860},
	abstractnote = {Algorithms influence most of our daily activities, decisions, and they guide our behaviors. It has been argued that algorithms even have a direct impact on democratic societies. Human - Computer Interaction research needs to develop analytical tools for describing the interaction with, and experience of algorithms. Based on user participatory workshops focused on scrutinizing Facebook’s newsfeed, an algorithm-influenced social media, we propose the concept of Algorithmic Experience (AX) as an analytic framing for making the interaction with and experience of algorithms explicit. Connecting it to design, we articulate five functional categories of AX that are particularly important to cater for in social media: profiling transparency and management, algorithmic awareness and control, and selective algorithmic memory.},
	collection   = {CHI ’18}
}
@inproceedings{alvero_ai_2020,
	title        = {{AI} and {Holistic} {Review}: {Informing} {Human} {Reading} in {College} {Admissions}},
	shorttitle   = {{AI} and {Holistic} {Review}},
	author       = {Alvero, A.J. and Arthurs, Noah and antonio, anthony lising and Domingue, Benjamin W. and Gebre-Medhin, Ben and Giebel, Sonia and Stevens, Mitchell L.},
	year         = 2020,
	month        = feb,
	booktitle    = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{AIES} '20},
	pages        = {200--206},
	doi          = {10.1145/3375627.3375871},
	isbn         = {978-1-4503-7110-0},
	url          = {https://doi.org/10.1145/3375627.3375871},
	urldate      = {2024-07-10},
	abstract     = {College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.},
	keywords     = {\_tablet},
	file         = {Alvero et al_2020_AI and Holistic Review.pdf:/Users/neilnatarajan/Zotero/storage/U6II5S8K/Alvero et al_2020_AI and Holistic Review.pdf:application/pdf},
	abstractnote = {College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant’s fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.},
	collection   = {AIES ’20}
}
@article{amodei_concrete_2016,
	title        = {Concrete {Problems} in {AI} {Safety}},
	author       = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	year         = 2016,
	month        = jul,
	journal      = {arXiv:1606.06565 [cs]},
	url          = {http://arxiv.org/abs/1606.06565},
	urldate      = {2022-04-09},
	note         = {arXiv: 1606.06565},
	abstract     = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/I5LAWUP5/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GR4MDXXQ/1606.html:text/html}
}
@article{anderson_ai_2023,
	title        = {{AI} did not write this manuscript, or did it? {Can} we trick the {AI} text detector into generated texts? {The} potential future of {ChatGPT} and {AI} in {Sports} \&amp; {Exercise} {Medicine} manuscript generation},
	shorttitle   = {{AI} did not write this manuscript, or did it?},
	author       = {Anderson, Nash and Belavy, Daniel L. and Perle, Stephen M. and Hendricks, Sharief and Hespanhol, Luiz and Verhagen, Evert and Memon, Aamir R.},
	year         = 2023,
	month        = feb,
	journal      = {BMJ Open Sport \& Exercise Medicine},
	volume       = 9,
	number       = 1,
	pages        = {e001568},
	doi          = {10.1136/bmjsem-2023-001568},
	issn         = {2055-7647},
	url          = {https://bmjopensem.bmj.com/content/9/1/e001568},
	urldate      = {2023-04-06},
	copyright    = {© Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	note         = {Publisher: BMJ Specialist Journals Section: Editorial},
	abstract     = {
		Researching a topic and generating an academic paper is a nuanced skill. It can take months or years to produce and publish one, if it is ever published at all. What if there were a way to make this happen instantly? Artificial intelligence (AI) may hold a flame to quickly analyse a research topic and generate an academic paper. There are many forms of AI; this editorial discusses natural language model-based AI, such as ChatGPT, and their potential ability to generate academic papers.

		Natural language model-based AI, in particular ChatGPT, is generating new content and a lot of controversies. This AI software is innovative. It generates, de novo, content that has a natural conversational flow. It can quickly answer questions and write poems, fan fiction and children’s books.1 ChatGPT has even passed the United States Medical Licensing Examination theory section with no additional training and/or years of studying medicine.2

		Language-based AI has already entered the scientific community. Nature reported that four manuscripts in preprint credit ChatGPT as an author.3 Also, an article reported that AI had been used to generate an academic paper.4

		In this editorial, we discuss the pros and cons of AI for manuscript generation in sports and exercise medicine (SEM), generate an academic paper using AI and bypass AI-generation detection, and discuss potential concerns regarding natural language model-based AI. We aim to get insights on how AI, in particular ChatGPT and similar language model-based AI, will impact the future of manuscript generation in SEM. To achieve such purpose, we ought to consider what is an academic paper, whether AI should write academic papers, what the issues are, what our stance should be on AI-generated texts and how we deal with them.

		An academic paper has a thesis and aims to persuade readers of …
	},
	language     = {en},
	keywords     = {Medical Ethics, Position statement, Research, Sports, Sports \& exercise medicine},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/ANISXSR5/Anderson et al. - 2023 - AI did not write this manuscript, or did it Can w.pdf:application/pdf}
}
@techreport{angelova2023algorithmic,
	title        = {Algorithmic recommendations and human discretion},
	author       = {Angelova, Victoria and Dobbie, Will S and Yang, Crystal},
	year         = 2023,
	institution  = {National Bureau of Economic Research}
}
@article{angrave_hr_2016,
	title        = {{HR} and analytics: why {HR} is set to fail the big data challenge},
	author       = {Angrave, David and Charlwood, Andy and Kirkpatrick, Ian and Lawrence, Mark and Lawrence, Mark T and Stuart, Mark},
	year         = 2016,
	month        = jan,
	journal      = {Human Resource Management Journal},
	volume       = 26,
	number       = 1,
	pages        = {1--11},
	doi          = {10.1111/1748-8583.12090},
	note         = {MAG ID: 2222612072},
	abstract     = {The HR world is abuzz with talk of big data and the transformative potential of HR analytics. This article takes issue with optimistic accounts which hail HR analytics as a ‘must have’ capability that will ensure HR’s future as a strategic management function while transforming organisational performance for the better. It argues that unless the HR profession wises up to both the potential and drawbacks of this emerging field, and engages operationally and strategically to develop better methods and approaches, it is unlikely that existing practices of HR analytics will deliver transformational change. Indeed, it is possible that current trends will seal the exclusion of HR from strategic, board level influence while doing little to benefit organisations and actively damaging the interests of employees.},
	keywords     = {\_tablet},
	file         = {Angrave et al_2016_HR and analytics.pdf:/Users/neilnatarajan/Zotero/storage/JYUEBXTF/Angrave et al_2016_HR and analytics.pdf:application/pdf}
}
@techreport{angrist2020marginal,
	title        = {Marginal Effects of Merit Aid for Low-Income Students},
	author       = {Angrist, Joshua and Autor, David and Pallais, Amanda},
	year         = 2020,
	institution  = {National Bureau of Economic Research}
}
@article{autor2008does,
	title        = {Does job testing harm minority workers? Evidence from retail establishments},
	author       = {Autor, David H and Scarborough, David},
	year         = 2008,
	journal      = {The Quarterly Journal of Economics},
	publisher    = {MIT Press},
	volume       = 123,
	number       = 1,
	pages        = {219--277}
}
@article{avery2024does,
	title        = {Does artificial intelligence help or hurt gender diversity? Evidence from two field experiments on recruitment in tech},
	author       = {Avery, Mallory and Leibbrandt, Andreas and Vecci, Joseph},
	year         = 2024,
	publisher    = {CESifo Working Paper}
}
@article{babina2024artificial,
	title        = {Artificial intelligence, firm growth, and product innovation},
	author       = {Babina, Tania and Fedyk, Anastassia and He, Alex and Hodson, James},
	year         = 2024,
	journal      = {Journal of Financial Economics},
	publisher    = {Elsevier},
	volume       = 151,
	pages        = 103745
}
@misc{bai_training_2022,
	title        = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	author       = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	year         = 2022,
	month        = apr,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2204.05862},
	urldate      = {2022-08-03},
	note         = {arXiv:2204.05862 [cs]},
	abstract     = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/HR26S4D9/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/VLIDQAEN/2204.html:text/html}
}
@misc{bai_constitutional_2022,
	title        = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle   = {Constitutional {AI}},
	author       = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	year         = 2022,
	month        = dec,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2212.08073},
	url          = {http://arxiv.org/abs/2212.08073},
	urldate      = {2023-11-17},
	note         = {arXiv:2212.08073 [cs]},
	abstract     = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/6CLABGJV/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/W8UE75XV/2212.html:text/html}
}
@article{banerjee_microcredit_2013,
	title        = {Microcredit {Under} the {Microscope}: {What} {Have} {We} {Learned} in the {Past} {Two} {Decades}, and {What} {Do} {We} {Need} to {Know}?},
	shorttitle   = {Microcredit {Under} the {Microscope}},
	author       = {Banerjee, Abhijit Vinayak},
	year         = 2013,
	month        = aug,
	journal      = {Annual Review of Economics},
	volume       = 5,
	number       = 1,
	pages        = {487--519},
	doi          = {10.1146/annurev-economics-082912-110220},
	issn         = {1941-1383, 1941-1391},
	url          = {https://www.annualreviews.org/doi/10.1146/annurev-economics-082912-110220},
	urldate      = {2021-10-22},
	abstract     = {Research on microcredit is now two decades old. There has been enormous progress in understanding both what microcredit does and how. Yet a lot of what we have learned has raised new and often quite fundamental questions about its nature: Is microcredit primarily about investment, consumption, or savings? Why is it that the investments financed by microcredit do not always lead to income growth, and does this have to do with the structure of microlending? What are the roles of social capital, reputation, and group lending? This article attempts to take stock of this significant body of work and tries to identify the most important questions for future research.},
	language     = {en},
	file         = {Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:/Users/neilnatarajan/Zotero/storage/NFHC8VCE/Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:application/pdf;Markup:/Users/neilnatarajan/Zotero/storage/CML4FSHM/Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:application/pdf}
}
@inproceedings{bansal_does_2021,
	title        = {Does the {Whole} {Exceed} its {Parts}? {The} {Effect} of {AI} {Explanations} on {Complementary} {Team} {Performance}},
	shorttitle   = {Does the {Whole} {Exceed} its {Parts}?},
	author       = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
	year         = 2021,
	month        = may,
	booktitle    = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} '21},
	pages        = {1--16},
	doi          = {10.1145/3411764.3445717},
	isbn         = {978-1-4503-8096-6},
	url          = {https://doi.org/10.1145/3411764.3445717},
	urldate      = {2022-01-26},
	abstract     = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
	keywords     = {\_tablet, Explainable AI, Augmented intelligence, Human-AI teams, User Study},
	file         = {Bansal et al_2021_Does the Whole Exceed its Parts.pdf:/Users/neilnatarajan/Zotero/storage/L9AYG6A7/Bansal et al_2021_Does the Whole Exceed its Parts.pdf:application/pdf}
}
@book{barocas2023fairness,
	title        = {Fairness and machine learning: Limitations and opportunities},
	author       = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year         = 2023,
	publisher    = {MIT press}
}
@article{barocas_hidden_2020,
	title        = {The {Hidden} {Assumptions} {Behind} {Counterfactual} {Explanations} and {Principal} {Reasons}},
	author       = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
	year         = 2020,
	month        = jan,
	journal      = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {80--89},
	doi          = {10.1145/3351095.3372830},
	url          = {http://arxiv.org/abs/1912.04930},
	urldate      = {2022-01-05},
	note         = {arXiv: 1912.04930},
	abstract     = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant--and withholding others. These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world--and the subjective choices necessary to compensate for this--must be understood before these techniques can be usefully implemented.},
	keywords     = {Computer Science - Computers and Society},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/ID6DMXNL/1912.html:text/html;Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:/Users/neilnatarajan/Zotero/storage/7UCITN3E/Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:application/pdf},
	abstractnote = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established “principal reason” explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant--and withholding others. These “feature-highlighting explanations” have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world--and the subjective choices necessary to compensate for this--must be understood before these techniques can be usefully implemented.}
}
@techreport{barocas_big_2016,
	title        = {Big {Data}'s {Disparate} {Impact}},
	author       = {Barocas, Solon and Selbst, Andrew D.},
	year         = 2016,
	address      = {Rochester, NY},
	number       = {ID 2477899},
	doi          = {10.2139/ssrn.2477899},
	url          = {https://papers.ssrn.com/abstract=2477899},
	urldate      = {2021-10-11},
	type         = {{SSRN} {Scholarly} {Paper}},
	abstract     = {Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.},
	language     = {en},
	institution  = {Social Science Research Network},
	keywords     = {algorithms, big data, civil rights, data mining, discrimination, disparate impact, disparate treatment, employment discrimination, inequality, procedural fairness, substantive fairness, Title VII},
	file         = {Barocas and Selbst - 2016 - Big Data's Disparate Impact.pdf:/Users/neilnatarajan/Zotero/storage/VGJXFHCQ/Barocas and Selbst - 2016 - Big Data's Disparate Impact.pdf:application/pdf;Barocas_Selbst_2016_Big Data's Disparate Impact.pdf:/Users/neilnatarajan/Zotero/storage/B3DAQ4R7/Barocas_Selbst_2016_Big Data's Disparate Impact.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/X8NTSPKV/papers.html:text/html}
}
@misc{Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024,
	title        = {On the consistent reasoning paradox of intelligence and optimal trust in AI: The power of I don't know},
	author       = {Bastounis, Alexander and Campodonico, Paolo and van der Schaar, Mihaela and Adcock, Ben and Hansen, Anders C.},
	year         = 2024,
	journal      = {arXiv.org},
	url          = {https://arxiv.org/abs/2408.02357v1}
}
@inproceedings{benami_distributive_2021,
	title        = {The {Distributive} {Effects} of {Risk} {Prediction} in {Environmental} {Compliance}: {Algorithmic} {Design}, {Environmental} {Justice}, and {Public} {Policy}},
	shorttitle   = {The {Distributive} {Effects} of {Risk} {Prediction} in {Environmental} {Compliance}},
	author       = {Benami, Elinor and Whitaker, Reid and La, Vincent and Lin, Hongjin and Anderson, Brandon R. and Ho, Daniel E.},
	year         = 2021,
	month        = mar,
	booktitle    = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAccT} '21},
	pages        = {90--105},
	doi          = {10.1145/3442188.3445873},
	isbn         = {978-1-4503-8309-7},
	url          = {https://doi.org/10.1145/3442188.3445873},
	urldate      = {2021-10-15},
	abstract     = {Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.},
	keywords     = {environmental justice, environmental protection, fairness, government, risk models},
	file         = {Benami et al_2021_The Distributive Effects of Risk Prediction in Environmental Compliance.pdf:/Users/neilnatarajan/Zotero/storage/NFH4JBFZ/Benami et al_2021_The Distributive Effects of Risk Prediction in Environmental Compliance.pdf:application/pdf}
}
@book{bergman2021seven,
	title        = {A Seven-College Experiment Using Algorithms to Track Students: Impacts and Implications for Equity and Fairness},
	author       = {Bergman, Peter and Kopko, Elizabeth and Rodriguez, Julio E},
	year         = 2021,
	publisher    = {National Bureau of Economic Research},
	date-added   = {2023-09-12 16:33:20 -0400},
	date-modified = {2023-09-12 16:33:20 -0400}
}
@article{binns_its_2018,
	title        = {'{It}'s {Reducing} a {Human} {Being} to a {Percentage}'; {Perceptions} of {Justice} in {Algorithmic} {Decisions}},
	author       = {Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
	year         = 2018,
	month        = apr,
	journal      = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--14},
	doi          = {10.1145/3173574.3173951},
	url          = {http://arxiv.org/abs/1801.10408},
	urldate      = {2022-01-06},
	note         = {arXiv: 1801.10408},
	abstract     = {Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.},
	keywords     = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, H.5.m, K.4.1, \_tablet, User Study},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/UALDRMYZ/1801.html:text/html;Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:/Users/neilnatarajan/Zotero/storage/VIS3KNGK/Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:application/pdf}
}
@misc{binns_apparent_2019,
	title        = {On the {Apparent} {Conflict} {Between} {Individual} and {Group} {Fairness}},
	author       = {Binns, Reuben},
	year         = 2019,
	month        = dec,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/1912.06883},
	urldate      = {2024-04-24},
	note         = {arXiv:1912.06883 [cs, stat]},
	abstract     = {A distinction has been drawn in fair machine learning research between `group' and `individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on theoretical discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artifact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of `unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/FLUPAGS7/1912.html:text/html;Full Text PDF:/Users/neilnatarajan/Zotero/storage/9VZE48FJ/Binns - 2019 - On the Apparent Conflict Between Individual and Gr.pdf:application/pdf}
}
@article{binns2022human,
	title        = {Human Judgment in algorithmic loops: Individual justice and automated decision-making},
	shorttitle   = {Human {Judgment} in algorithmic loops},
	author       = {Binns, Reuben},
	year         = 2022,
	journal      = {Regulation \& Governance},
	publisher    = {Wiley Online Library},
	volume       = 16,
	number       = 1,
	pages        = {197--211},
	doi          = {10.1111/rego.12358},
	issn         = {1748-5991},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rego.12358},
	urldate      = {2022-04-14},
	note         = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rego.12358},
	date-added   = {2023-09-11 10:51:30 -0400},
	date-modified = {2023-09-11 10:51:30 -0400},
	abstract     = {Arguments in favor of tempering algorithmic decision making with human judgment often appeal to concepts and criteria derived from legal philosophy about the nature of law and legal reasoning, arguing that algorithmic systems cannot satisfy them (but humans can). Such arguments often make implicit appeal to the notion that each case needs to be assessed on its own merits, without comparison to or generalization from previous cases. This article argues that this notion of individual justice can only be meaningfully served through human judgment. It distinguishes individual justice and considers how it relates to other dimensions of justice, namely consistency and fairness / nondiscrimination. Finally, it identifies and discussess two challenges: first, how individual justice can be accommodated alongside other dimensions of justice in the socio-technical contexts of humans-in-the-loop; and second, how inequities in individual justice may result from an uneven application of human judgment in algorithmic contexts.},
	language     = {en},
	keywords     = {\_tablet, algorithmic regulation, data protection, discretion, human-in-the-loop, justice},
	file         = {Binns_2022_Human Judgment in algorithmic loops.pdf:/Users/neilnatarajan/Zotero/storage/8ENLVGM2/Binns_2022_Human Judgment in algorithmic loops.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/A4S8UTEQ/rego.html:text/html}
}
@inproceedings{biran_explanation_2017,
	title        = {Explanation and {Justification} in {Machine} {Learning} : {A} {Survey} {Or}},
	author       = {Biran, Or and Cotton, Courtenay V.},
	year         = 2017
}
@article{bird_social_2010,
	title        = {{SOCIAL} {KNOWING}: {THE} {SOCIAL} {SENSE} {OF} ‘{SCIENTIFIC} {KNOWLEDGE}’: {Social} {Knowing}},
	shorttitle   = {{SOCIAL} {KNOWING}},
	author       = {Bird, Alexander},
	year         = 2010,
	month        = dec,
	journal      = {Philosophical Perspectives},
	volume       = 24,
	number       = 1,
	pages        = {23--56},
	doi          = {10.1111/j.1520-8583.2010.00184.x},
	issn         = 15208583,
	url          = {https://onlinelibrary.wiley.com/doi/10.1111/j.1520-8583.2010.00184.x},
	urldate      = {2021-10-15},
	language     = {en},
	file         = {Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:/Users/neilnatarajan/Zotero/storage/VA66UKK5/Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/UQYJZ2CS/Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:application/pdf}
}
@misc{biyik_batch_2018,
	title        = {Batch {Active} {Preference}-{Based} {Learning} of {Reward} {Functions}},
	author       = {Bıyık, Erdem and Sadigh, Dorsa},
	year         = 2018,
	month        = oct,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1810.04303},
	url          = {http://arxiv.org/abs/1810.04303},
	urldate      = {2022-08-03},
	note         = {arXiv:1810.04303 [cs, stat]},
	abstract     = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/BXV7S9IB/Bıyık and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/65BCXHJD/1810.html:text/html}
}
@article{black_ai-enabled_2020,
	title        = {{AI}-enabled recruiting: {What} is it and how should a manager use it?},
	shorttitle   = {{AI}-enabled recruiting},
	author       = {Black, J. Stewart and van Esch, Patrick},
	year         = 2020,
	month        = mar,
	journal      = {Business Horizons},
	series       = {{ARTIFICIAL} {INTELLIGENCE} {AND} {MACHINE} {LEARNING}},
	volume       = 63,
	number       = 2,
	pages        = {215--226},
	doi          = {10.1016/j.bushor.2019.12.001},
	issn         = {0007-6813},
	url          = {https://www.sciencedirect.com/science/article/pii/S0007681319301612},
	urldate      = {2023-08-01},
	abstract     = {AI-enabled recruiting systems have evolved from nice to talk about to necessary to utilize. In this article, we outline the reasons underlying this development. First, as competitive advantages have shifted from tangible to intangible assets, human capital has transitioned from supporting cast to a starring role. Second, as digitalization has redesigned both the business and social landscapes, digital recruiting of human capital has moved from the periphery to center stage. Third, recent and near-future advances in AI-enabled recruiting have improved recruiting efficiency to the point that managers ignore them or procrastinate their utilization at their own peril. In addition to explaining the forces that have pushed AI-enabled recruiting systems from nice to necessary, we outline the key strategic steps managers need to take in order to capture its main benefits.},
	language     = {en},
	keywords     = {\_tablet, Artificial intelligence, AI-enabled recruiting, Digital recruiting technology, Human resources},
	file         = {Black_van Esch_2020_AI-enabled recruiting.pdf:/Users/neilnatarajan/Zotero/storage/T3BIQUWA/Black_van Esch_2020_AI-enabled recruiting.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/DBM8TZF3/S0007681319301612.html:text/html}
}
@inproceedings{blythe2014research,
	title        = {Research through design fiction: narrative in real and imaginary abstracts},
	author       = {Blythe, Mark},
	year         = 2014,
	booktitle    = {Proceedings of the SIGCHI conference on human factors in computing systems},
	pages        = {703--712}
}
@article{bradbury_action_2003,
	title        = {Action {Research}: {An} {Opportunity} for {Revitalizing} {Research} {Purpose} and {Practices}},
	shorttitle   = {Action {Research}},
	author       = {Bradbury, Hilary and Reason, Peter},
	year         = 2003,
	month        = jun,
	journal      = {Qualitative Social Work},
	volume       = 2,
	number       = 2,
	pages        = {155--175},
	doi          = {10.1177/1473325003002002003},
	issn         = {1473-3250},
	url          = {https://doi.org/10.1177/1473325003002002003},
	urldate      = {2024-07-30},
	note         = {Publisher: SAGE Publications},
	abstract     = {In this overview the authors describe the underlying principles of action research as: (1) grounded in lived experience, (2) developed in partnership, (3) addressing significant problems, (4) working with, rather than simply studying, people, (5) developing new ways of seeing/theorizing the world, and (6) leaving infrastructure in its wake. We refer to the role of social workers as frontline implementers of important social policies and suggest how action research can be used to both implement and also influence the creation of such policies. We offer examples of action research efforts that can be applied to the social worker's practice-scholarship repertoire.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Bradbury_Reason_2003_Action Research.pdf:/Users/neilnatarajan/Zotero/storage/SQ8NC7YK/Bradbury_Reason_2003_Action Research.pdf:application/pdf}
}
@article{bradley_robustness_1978,
	title        = {Robustness?},
	author       = {Bradley, James V.},
	year         = 1978,
	month        = nov,
	journal      = {British Journal of Mathematical and Statistical Psychology},
	volume       = 31,
	number       = 2,
	pages        = {144--152},
	doi          = {10.1111/j.2044-8317.1978.tb00581.x},
	issn         = {0007-1102, 2044-8317},
	url          = {https://bpspsychub.onlinelibrary.wiley.com/doi/10.1111/j.2044-8317.1978.tb00581.x},
	urldate      = {2024-02-19},
	abstract     = {The actual behaviour of the probability of a Type I error under assumption violation is quite complex, depending upon a wide variety of interacting factors. Yet allegations of robustness tend to ignore its highly particularistic nature and neglect to mention important qualifying conditions. The result is often a vast overgeneralization which nevertheless is difficult to refute since a standard quantitative definition of what constitutes robustness does not exist. Yet under any halfway reasonable quantitative definition, many of the most prevalent claims of robustness would be demonstrably false. Therefore robustness is a highly questionable concept.},
	language     = {en}
}
@article{braun_using_2006,
	title        = {Using thematic analysis in psychology},
	author       = {Braun, Virginia and Clarke, Victoria},
	year         = 2006,
	month        = jan,
	journal      = {Qualitative Research in Psychology},
	volume       = 3,
	pages        = {77--101},
	doi          = {10.1191/1478088706qp063oa},
	abstract     = {Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
	file         = {Braun_Clarke_2006_Using thematic analysis in psychology_annotated.pdf:/Users/neilnatarajan/Zotero/storage/ZKGPPLK2/Braun_Clarke_2006_Using thematic analysis in psychology_annotated.pdf:application/pdf;Braun_Clarke_2006_Using thematic analysis in psychology.pdf:/Users/neilnatarajan/Zotero/storage/ZKGPPLK2/Braun_Clarke_2006_Using thematic analysis in psychology.pdf:application/pdf}
}
@article{braun_conceptual_2022,
	title        = {Conceptual and design thinking for thematic analysis},
	author       = {Braun, Virginia and Clarke, Victoria},
	year         = 2022,
	journal      = {Qualitative Psychology},
	volume       = 9,
	number       = 1,
	pages        = {3--26},
	doi          = {10.1037/qup0000196},
	issn         = {2326-3598},
	note         = {Place: US Publisher: Educational Publishing Foundation},
	abstract     = {Thematic analysis (TA) is widely used in qualitative psychology. In using TA, researchers must choose between a diverse range of approaches that can differ considerably in their underlying (but often implicit) conceptualizations of qualitative research, meaningful knowledge production, and key constructs such as themes, as well as analytic procedures. This diversity within the method of TA is typically poorly understood and rarely acknowledged, resulting in the frequent publication of research lacking in design coherence. Furthermore, because TA offers researchers something closer to a method (a transtheoretical tool or technique) rather than a methodology (a theoretically informed framework for research), one with considerable theoretical and design flexibility, researchers need to engage in careful conceptual and design thinking to produce TA research with methodological integrity. In this article, we support researchers in their conceptual and design thinking for TA, and particularly for the reflexive approach we have developed, by guiding them through the conceptual underpinnings of different approaches to TA, and key design considerations. We outline our typology of three main “schools” of TA—coding reliability, codebook, and reflexive—and consider how these differ in their conceptual underpinnings, with a particular focus on the distinct characteristics of our reflexive approach. We discuss key areas of design—research questions, data collection, participant/data item selection strategy and criteria, ethics, and quality standards and practices—and end with guidance on reporting standards for reflexive TA. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	keywords     = {Data Collection, Ethics, Integrity, Methodology, Procedural Knowledge, Qualitative Methods, Reporting Standards, Thematic Analysis},
	file         = {Braun_Clarke_2022_Conceptual and design thinking for thematic analysis_annotated.pdf:/Users/neilnatarajan/Zotero/storage/EXUYUKSY/Braun_Clarke_2022_Conceptual and design thinking for thematic analysis_annotated.pdf:application/pdf;Braun_Clarke_2022_Conceptual and design thinking for thematic analysis.pdf:/Users/neilnatarajan/Zotero/storage/EXUYUKSY/Braun_Clarke_2022_Conceptual and design thinking for thematic analysis.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/R7JRQYN2/2021-45248-001.html:text/html}
}
@article{braun_toward_2023,
	title        = {Toward good practice in thematic analysis: {Avoiding} common problems and be(com)ing a knowing researcher},
	shorttitle   = {Toward good practice in thematic analysis},
	author       = {Braun, Virginia and Clarke, Victoria},
	year         = 2023,
	month        = jan,
	journal      = {International Journal of Transgender Health},
	volume       = 24,
	number       = 1,
	pages        = {1--6},
	doi          = {10.1080/26895269.2022.2129597},
	issn         = {2689-5269},
	url          = {https://doi.org/10.1080/26895269.2022.2129597},
	urldate      = {2023-11-03},
	note         = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/26895269.2022.2129597},
	file         = {Braun_Clarke_2023_Toward good practice in thematic analysis_annotated.pdf:/Users/neilnatarajan/Zotero/storage/YMDUWCTY/Braun_Clarke_2023_Toward good practice in thematic analysis_annotated.pdf:application/pdf;Braun_Clarke_2023_Toward good practice in thematic analysis.pdf:/Users/neilnatarajan/Zotero/storage/YMDUWCTY/Braun_Clarke_2023_Toward good practice in thematic analysis.pdf:application/pdf}
}
@misc{brown_language_2020,
	title        = {Language {Models} are {Few}-{Shot} {Learners}},
	author       = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	month        = jul,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2005.14165},
	url          = {http://arxiv.org/abs/2005.14165},
	urldate      = {2022-08-24},
	note         = {arXiv:2005.14165 [cs]},
	abstract     = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	keywords     = {Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/8ZEKLULT/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6N8ZF8SV/2005.html:text/html}
}
@techreport{brynjolfsson2023generative,
	title        = {Generative AI at work},
	author       = {Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey R},
	year         = 2023,
	institution  = {National Bureau of Economic Research}
}
@inproceedings{Buchenau_Suri_2000,
	title        = {Experience prototyping},
	author       = {Buchenau, Marion and Suri, Jane Fulton},
	year         = 2000,
	month        = aug,
	booktitle    = {Proceedings of the 3rd conference on Designing interactive systems: processes, practices, methods, and techniques},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {DIS ’00},
	pages        = {424–433},
	doi          = {10.1145/347642.347802},
	isbn         = {978-1-58113-219-9},
	url          = {https://dl.acm.org/doi/10.1145/347642.347802},
	abstractnote = {In this paper, we describe “Experience Prototyping” as a form of prototyping that enables design team members, users and clients to gain first-hand appreciation of existing or future conditions through active engagement with prototypes. We use examples from commercial design projects to illustrate the value of such prototypes in three critical design activities: understanding existing experiences, exploring design ideas and in communicating design concepts.},
	collection   = {DIS ’00}
}
@article{budescu2012measure,
	title        = {How to measure diversity when you must.},
	author       = {Budescu, David V and Budescu, Mia},
	year         = 2012,
	journal      = {Psychological methods},
	publisher    = {American Psychological Association},
	volume       = 17,
	number       = 2,
	pages        = 215,
	doi          = {10.1037/a0027129},
	issn         = {1939-1463, 1082-989X},
	url          = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0027129},
	urldate      = {2023-02-14},
	date-added   = {2023-09-11 10:53:14 -0400},
	date-modified = {2023-09-11 10:53:14 -0400},
	abstract     = {Racial/ethnic diversity has become an increasingly important variable in the social sciences. Research from multiple disciplines consistently demonstrates the tremendous impact of ethnic diversity on individuals and organisations. Investigators use a variety of measures, and their choices can affect the conclusions that can be drawn and limit the ability to compare and generalize results across studies effectively. The current article reviews 3 popular approaches to the measurement of diversity: the simplistic majority-minority approach and 2 multiple categories variants, the generalized variance and the lesser used entropy statistic. We discuss the properties of each approach and reject the majority-minority approach. We provide 5 examples using the generalized variance and entropy statistics and illustrate their versatility and flexibility. We urge investigators to adopt these multicategory measures and to use our discussion to determine which measure of diversity is most appropriate given the nature of one's data set and research question.},
	language     = {en}
}
@techreport{bundorf2019humans,
	title        = {How do humans interact with algorithms? Experimental evidence from health insurance},
	author       = {Bundorf, M Kate and Polyakova, Maria and Tai-Seale, Ming},
	year         = 2019,
	institution  = {National Bureau of Economic Research}
}
@misc{burkner_bayesian_2020,
	title        = {Bayesian {Item} {Response} {Modeling} in {R} with brms and {Stan}},
	author       = {Bürkner, Paul-Christian},
	year         = 2020,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/1905.09501},
	urldate      = {2022-09-29},
	note         = {arXiv:1905.09501 [stat]},
	abstract     = {Item Response Theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. We demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and post-processed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
	keywords     = {Statistics - Computation},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/AEDDX34D/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GNA3E52G/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.html:text/html}
}
@article{burkner2021bayesian,
	title        = {Bayesian Item Response Modeling in R with brms and Stan},
	author       = {Bürkner, Paul-Christian},
	year         = 2021,
	journal      = {Journal of Statistical Software},
	volume       = 100,
	number       = 5,
	pages        = {1–54},
	doi          = {10.18637/jss.v100.i05},
	url          = {https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
	abstract     = {&amp;lt;p&amp;gt;Item response theory (IRT) is widely applied in the human sciences to model persons’ responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective pre-specified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.&amp;lt;/p&amp;gt;}
}
@inproceedings{Cabitza_Fregosi_Campagner_Natali_2024,
	title        = {Explanations Considered Harmful: The Impact of Misleading Explanations on Accuracy in Hybrid Human-AI Decision Making},
	author       = {Cabitza, Federico and Fregosi, Caterina and Campagner, Andrea and Natali, Chiara},
	year         = 2024,
	booktitle    = {Explainable Artificial Intelligence},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {255–269},
	doi          = {10.1007/978-3-031-63803-9_14},
	isbn         = {978-3-031-63803-9},
	abstractnote = {Explainable AI (XAI) has the potential to enhance decision-making in human-AI collaborations, yet existing research indicates that explanations can also lead to undue reliance on AI recommendations, a dilemma often referred to as the ‘white box paradox.’ This paradox illustrates how persuasive explanations for incorrect advice might foster inappropriate trust in AI systems. Our study extends beyond the traditional scope of the white box paradox by proposing a framework for examining explanation inadequacy. We specifically investigate how accurate AI advice, when paired with misleading explanations, affects decision-making in logic puzzle tasks. Our findings introduce the concept of the ‘XAI halo effect,’ where participants were influenced by the misleading explanations to the extent that they did not verify the correctness of the advice, despite its accuracy. This effect reveals a nuanced challenge in XAI, where even correct advice can lead to misjudgment if the accompanying explanations are not coherent and contextually relevant. The study highlights the critical need for explanations to be both accurate and relevant, especially in contexts where decision accuracy is paramount. This calls into question the use of explanations in situations where their potential to mislead outweighs their transparency or educational value.},
	editor       = {Longo, Luca and Lapuschkin, Sebastian and Seifert, Christin},
	language     = {en}
}
@book{caldwell_power_nodate,
	title        = {Power {Analysis} with {Superpower}},
	author       = {Caldwell, Aaron R. and Lakens, Daniël and Parlett-Pelleriti, Chelsea M. and Prochilo, Guy and Aust, Frederik},
	year         = 2022,
	url          = {https://aaroncaldwell.us/SuperpowerBook/index.html#preface},
	urldate      = {2022-09-13},
	abstract     = {This is a book describing the capabilities of the Superpower R package.},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/LZ67UGSS/index.html:text/html},
	annote       = {https://aaroncaldwell.us/SuperpowerBook/index.html}
}
@article{calvino2023portrait,
	title        = {A portrait of AI adopters across countries: Firm characteristics, assets’ complementarities and productivity},
	author       = {Calvino, Flavio and Fontanelli, Luca},
	year         = 2023,
	publisher    = {OECD}
}
@misc{camburu_struggles_2020,
	title        = {The {Struggles} of {Feature}-{Based} {Explanations}: {Shapley} {Values} vs. {Minimal} {Sufficient} {Subsets}},
	shorttitle   = {The {Struggles} of {Feature}-{Based} {Explanations}},
	author       = {Camburu, Oana-Maria and Giunchiglia, Eleonora and Foerster, Jakob and Lukasiewicz, Thomas and Blunsom, Phil},
	year         = 2020,
	month        = dec,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2009.11023},
	url          = {http://arxiv.org/abs/2009.11023},
	urldate      = {2022-08-19},
	note         = {arXiv:2009.11023 [cs]},
	abstract     = {For neural models to garner widespread public trust and ensure fairness, we must have human-intelligible explanations for their predictions. Recently, an increasing number of works focus on explaining the predictions of neural models in terms of the relevance of the input features. In this work, we show that feature-based explanations pose problems even for explaining trivial models. We show that, in certain cases, there exist at least two ground-truth feature-based explanations, and that, sometimes, neither of them is enough to provide a complete view of the decision-making process of the model. Moreover, we show that two popular classes of explainers, Shapley explainers and minimal sufficient subsets explainers, target fundamentally different types of ground-truth explanations, despite the apparently implicit assumption that explainers should look for one specific feature-based explanation. These findings bring an additional dimension to consider in both developing and choosing explainers.},
	keywords     = {Computer Science - Computation and Language},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IU7MTK3S/2009.html:text/html;Camburu et al_2020_The Struggles of Feature-Based Explanations_annotated.pdf:/Users/neilnatarajan/Zotero/storage/KQ4TRXBG/Camburu et al_2020_The Struggles of Feature-Based Explanations_annotated.pdf:application/pdf;Camburu et al_2020_The Struggles of Feature-Based Explanations.pdf:/Users/neilnatarajan/Zotero/storage/KQ4TRXBG/Camburu et al_2020_The Struggles of Feature-Based Explanations.pdf:application/pdf}
}
@misc{cappelli_artificial_2019,
	title        = {Artificial {Intelligence} in {Human} {Resources} {Management}: {Challenges} and a {Path} {Forward}},
	shorttitle   = {Artificial {Intelligence} in {Human} {Resources} {Management}},
	author       = {Cappelli, Peter and Tambe, Prasanna and Yakubovich, Valery},
	year         = 2019,
	month        = apr,
	address      = {Rochester, NY},
	doi          = {10.2139/ssrn.3263878},
	url          = {https://papers.ssrn.com/abstract=3263878},
	urldate      = {2023-08-01},
	type         = {{SSRN} {Scholarly} {Paper}},
	abstract     = {We consider the gap between the promise and reality of artificial intelligence in human resource management and suggest how progress might be made. We identify four challenges in using data science techniques for HR tasks: 1) complexity of HR phenomena, 2) constraints imposed by small data sets, 3) accountability questions associated with fairness and other ethical and legal constraints, and 4) possible adverse employee reactions to management decisions via data-based algorithms. We propose practical responses to these challenges and converge on three overlapping principles - causal reasoning, randomization and experiments, and employee contribution—that could be both economically efficient and socially appropriate for using data science in the management of employees.},
	language     = {en},
	keywords     = {big data, artificial intelligence, machine learning, algorithmic management, data science, human resource management},
	file         = {Cappelli et al_2019_Artificial Intelligence in Human Resources Management_annotated.pdf:/Users/neilnatarajan/Zotero/storage/MI46DQCH/Cappelli et al_2019_Artificial Intelligence in Human Resources Management_annotated.pdf:application/pdf;Cappelli et al_2019_Artificial Intelligence in Human Resources Management.pdf:/Users/neilnatarajan/Zotero/storage/MI46DQCH/Cappelli et al_2019_Artificial Intelligence in Human Resources Management.pdf:application/pdf}
}
@book{celebi2022diversity,
	title        = {Diversity Preferences, Affirmative Action and Choice Rules},
	author       = {Celebi, Oguzhan},
	year         = 2022,
	publisher    = {SSRN},
	date-added   = {2023-09-11 21:38:27 -0400},
	date-modified = {2023-09-11 21:38:27 -0400}
}
@article{chadi2018selecting,
	title        = {Selecting successful students? Undergraduate grades as an admission criterion},
	shorttitle   = {Selecting successful students?},
	author       = {Chadi, Adrian and de Pinto, Marco},
	year         = 2018,
	month        = jun,
	journal      = {Applied Economics},
	publisher    = {Taylor \& Francis},
	volume       = 50,
	number       = 28,
	pages        = {3089--3105},
	doi          = {10.1080/00036846.2017.1418072},
	issn         = {0003-6846, 1466-4283},
	url          = {https://www.tandfonline.com/doi/full/10.1080/00036846.2017.1418072},
	urldate      = {2022-04-12},
	date-added   = {2023-09-11 10:53:56 -0400},
	date-modified = {2023-09-11 10:53:56 -0400},
	abstract     = {In Europe's reformed education system, universities may be forced by law to consider undergraduate grade point average (UGPAJ as the primary admission criterion in the selection of graduate students. In this article, we investigate whether UGPA predicts graduate student performance in order to discuss its usefulness as an admission criterion. In our theoretical framework, we show that undergraduate students may choose slower study progress in favour of receiving higher grades and conclude that UGPA is a relatively good (weak) predictor for graduate grade point average (study progress). Having data from a cohort of students whose selection was in clear conflict with the legal requirement, we empirically confirm our theoretical predictions by exploiting a unique opportunity for assessing educationa l policies. Discussion of our findings leads to some important conc lusions concerning the Bologna reforms and the lawmakers' idea of giving some independence to universities, but not too much of it.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Chadi_de Pinto_2018_Selecting successful students.pdf:/Users/neilnatarajan/Zotero/storage/J5G4G9CK/Chadi_de Pinto_2018_Selecting successful students.pdf:application/pdf}
}
@techreport{chari_specious_2021,
	title        = {The {Specious} {Art} of {Single}-{Cell} {Genomics}},
	author       = {Chari, Tara and Banerjee, Joeyta and Pachter, Lior},
	year         = 2021,
	month        = aug,
	doi          = {10.1101/2021.08.25.457696},
	url          = {http://biorxiv.org/lookup/doi/10.1101/2021.08.25.457696},
	urldate      = {2021-09-16},
	type         = {preprint},
	abstract     = {
		Abstract

		Dimensionality reduction is standard practice for filtering noise and identifying relevant dimensions in large-scale data analyses. In biology, single-cell expression studies almost always begin with reduction to two or three dimensions to produce ‘all-in-one’ visuals of the data that are amenable to the human eye, and these are subsequently used for qualitative and quantitative analysis of cell relationships. However, there is little theoretical support for this practice. We examine the theoretical and practical implications of low-dimensional embedding of single-cell data, and find extensive distortions incurred on the global and local properties of biological patterns relative to the high-dimensional, ambient space. In lieu of this, we propose semi-supervised dimension reduction to higher dimension, and show that such targeted reduction guided by the metadata associated with single-cell experiments provides useful latent space representations for hypothesis-driven biological discovery.
	},
	language     = {en},
	institution  = {Genomics},
	file         = {Markup:/Users/neilnatarajan/Zotero/storage/5H7CDVB6/The Specious Art of Single-Cell Genomics.pdf:application/pdf;Submitted Version:/Users/neilnatarajan/Zotero/storage/9J97L62B/Chari et al. - 2021 - The Specious Art of Single-Cell Genomics.pdf:application/pdf}
}
@misc{chen_machine_2023,
	title        = {Machine {Explanations} and {Human} {Understanding}},
	author       = {Chen, Chacha and Feng, Shi and Sharma, Amit and Tan, Chenhao},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2202.04092},
	url          = {http://arxiv.org/abs/2202.04092},
	urldate      = {2023-06-16},
	note         = {arXiv:2202.04092 [cs]},
	abstract     = {Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, but they cannot improve human understanding of task decision boundary or model error. To achieve complementary human-AI performance, we articulate possible ways on how explanations need to work with human intuitions. For instance, human intuitions about the relevance of features (e.g., education is more important than age in predicting a person's income) can be critical in detecting model error. We validate the importance of human intuitions in shaping the outcome of machine explanations with empirical human-subject studies. Overall, our work provides a general framework along with actionable implications for future algorithmic development and empirical experiments of machine explanations.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/U8C5BYB2/Chen et al. - 2023 - Machine Explanations and Human Understanding.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GV6N3KA5/2202.html:text/html}
}
@inproceedings{chen2018investigating,
	title        = {Investigating the impact of gender on rank in resume search engines},
	author       = {Chen, Le and Ma, Ruijun and Hann{\'a}k, Anik{\'o} and Wilson, Christo},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 chi conference on human factors in computing systems},
	pages        = {1--14}
}
@article{chou_counterfactuals_2022,
	title        = {Counterfactuals and causability in explainable artificial intelligence: {Theory}, algorithms, and applications},
	shorttitle   = {Counterfactuals and causability in explainable artificial intelligence},
	author       = {Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
	year         = 2022,
	month        = may,
	journal      = {Information Fusion},
	volume       = 81,
	pages        = {59--83},
	doi          = {10.1016/j.inffus.2021.11.003},
	issn         = {1566-2535},
	url          = {https://www.sciencedirect.com/science/article/pii/S1566253521002281},
	urldate      = {2022-01-26},
	abstract     = {Deep learning models have achieved high performance across different domains, such as medical decision-making, autonomous vehicles, decision support systems, among many others. However, despite this success, the inner mechanisms of these models are opaque because their internal representations are too complex for a human to understand. This opacity makes it hard to understand the how or the why of the predictions of deep learning models. There has been a growing interest in model-agnostic methods that make deep learning models more transparent and explainable to humans. Some researchers recently argued that for a machine to achieve human-level explainability, this machine needs to provide human causally understandable explanations, also known as causability. A specific class of algorithms that have the potential to provide causability are counterfactuals. This paper presents an in-depth systematic review of the diverse existing literature on counterfactuals and causability for explainable artificial intelligence (AI). We performed a Latent Dirichlet topic modelling analysis (LDA) under a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to find the most relevant literature articles. This analysis yielded a novel taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying properties and applications to real-world data. Our research suggests that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker. Furthermore, our findings suggest that the explanations derived from popular algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading to sub-optimal, erroneous, or even biased explanations. Thus, this paper also advances the literature with new directions and challenges on promoting causability in model-agnostic approaches for explainable AI.},
	language     = {en},
	keywords     = {\_tablet, Causability, Causality, Counterfactuals, Deep learning, Explainable AI},
	file         = {Chou et al_2022_Counterfactuals and causability in explainable artificial intelligence.pdf:/Users/neilnatarajan/Zotero/storage/B7Y459EC/Chou et al_2022_Counterfactuals and causability in explainable artificial intelligence.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/Z5ASSTWX/S1566253521002281.html:text/html}
}
@misc{chowdhery_palm_2022,
	title        = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle   = {{PaLM}},
	author       = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	year         = 2022,
	month        = oct,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2204.02311},
	url          = {http://arxiv.org/abs/2204.02311},
	urldate      = {2023-08-31},
	note         = {arXiv:2204.02311 [cs]},
	abstract     = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	keywords     = {Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/DAGVLCV4/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6PVW37WK/2204.html:text/html}
}
@misc{christiano_eliciting_2022,
	title        = {Eliciting latent knowledge},
	author       = {Christiano, Paul},
	year         = 2022,
	month        = feb,
	journal      = {Medium},
	url          = {https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc},
	urldate      = {2022-08-03},
	abstract     = {(This is a repost from December 2021, linking to a google doc.)},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/9FNX6H4L/eliciting-latent-knowledge-f977478608fc.html:text/html}
}
@article{chun_tie_grounded_2019,
	title        = {Grounded theory research: {A} design framework for novice researchers},
	author       = {Chun Tie, Ylona and Birks, Melanie and Francis, Karen},
	year         = 2019,
	month        = jan,
	journal      = {Open Medicine},
	volume       = 7,
	pages        = {1--8}
}
@article{colquitt_justice_2001,
	title        = {Justice at the {Millennium}: {A} {Meta}-{Analytic} {Review} of 25 {Years} of {Organisational} {Justice} {Research}},
	shorttitle   = {Justice at the {Millennium}},
	author       = {Colquitt, Jason and Conlon, Donald and Wesson, Michael and Porter, Christopher and Ng, K.},
	year         = 2001,
	month        = jul,
	journal      = {The Journal of applied psychology},
	volume       = 86,
	pages        = {425--45},
	doi          = {10.1037//0021-9010.86.3.425},
	abstract     = {The field of organisational justice continues to be marked by several important research questions, including the size of relationships among justice dimensions, the relative importance of different justice criteria, and the unique effects of justice dimensions on key outcomes. To address such questions, the authors conducted a meta-analytic review of 183 justice studies. The results suggest that although different justice dimensions are moderately to highly related, they contribute incremental variance explained in fairness perceptions. The results also illustrate the overall and unique relationships among distributive, procedural, interpersonal, and informational justice and several organisational outcomes (e.g., job satisfaction, organisational commitment, evaluation of authority, organisational citizenship behavior, withdrawal, performance). These findings are reviewed in terms of their implications for future research on organisational justice.},
	file         = {Colquitt et al_2001_Justice at the Millennium.pdf:/Users/neilnatarajan/Zotero/storage/KQKMQ8V3/Colquitt et al_2001_Justice at the Millennium.pdf:application/pdf}
}
@article{condon2014international,
	title        = {The international cognitive ability resource: Development and initial validation of a public-domain measure},
	author       = {Condon, David M and Revelle, William},
	year         = 2014,
	journal      = {Intelligence},
	publisher    = {Elsevier},
	volume       = 43,
	pages        = {52--64}
}
@article{connolly_detecting_2017,
	title        = {Detecting peatland drains with {Object} {Based} {Image} {Analysis} and {Geoeye}-1 imagery},
	author       = {Connolly, John and Holden, N.},
	year         = 2017,
	month        = mar,
	journal      = {Carbon Balance and Management},
	volume       = 12,
	doi          = {10.1186/s13021-017-0075-z},
	abstract     = {Background Peatlands play an important role in the global carbon cycle. They provide important ecosystem services including carbon sequestration and storage. Drainage disturbs peatland ecosystem services. Mapping drains is difficult and expensive and their spatial extent is, in many cases, unknown. An object based image analysis (OBIA) was performed on a very high resolution satellite image (Geoeye-1) to extract information about drain location and extent on a blanket peatland in Ireland. Two accuracy assessment methods: Error matrix and the completeness, correctness and quality (CCQ) were used to assess the extracted data across the peatland and at several sub sites. The cost of the OBIA method was compared with manual digitisation and field survey. The drain maps were also used to assess the costs relating to blocking drains vs. a business-as-usual scenario and estimating the impact of each on carbon fluxes at the study site. ResultsThe OBIA method performed well at almost all sites. Almost 500 km of drains were detected within the peatland. In the error matrix method, overall accuracy (OA) of detecting the drains was 94\% and the kappa statistic was 0.66. The OA for all sub-areas, except one, was 95–97\%. The CCQ was 85\%, 85\% and 71\% respectively. The OBIA method was the most cost effective way to map peatland drains and was at least 55\% cheaper than either field survey or manual digitisation, respectively. The extracted drain maps were used constrain the study area CO2 flux which was 19\% smaller than the prescribed Peatland Code value for drained peatlands. Conclusions The OBIA method used in this study showed that it is possible to accurately extract maps of fine scale peatland drains over large areas in a cost effective manner. The development of methods to map the spatial extent of drains is important as they play a critical role in peatland carbon dynamics. The objective of this study was to extract data on the spatial extent of drains on a blanket bog in the west of Ireland. The results show that information on drain extent and location can be extracted from high resolution imagery and mapped with a high degree of accuracy. Under Article 3.4 of the Kyoto Protocol Annex 1 parties can account for greenhouse gas emission by sources and removals by sinks resulting from “wetlands drainage and rewetting”. The ability to map the spatial extent, density and location of peatlands drains means that Annex 1 parties can develop strategies for drain blocking to aid reduction of CO2 emissions, DOC runoff and water discoloration. This paper highlights some uncertainty around using one-size-fits-all emission factors for GHG in drained peatlands and re-wetting scenarios. However, the OBIA method is robust and accurate and could be used to assess the extent of drains in peatlands across the globe aiding the refinement of peatland carbon dynamics .},
	file         = {Connolly_Holden_2017_Detecting peatland drains with Object Based Image Analysis and Geoeye-1 imagery.pdf:/Users/neilnatarajan/Zotero/storage/CNQC7SU6/Connolly_Holden_2017_Detecting peatland drains with Object Based Image Analysis and Geoeye-1 imagery.pdf:application/pdf}
}
@techreport{cowls_ai_2021,
	title        = {The {AI} {Gambit} — {Leveraging} {Artificial} {Intelligence} to {Combat} {Climate} {Change}: {Opportunities}, {Challenges}, and {Recommendations}},
	shorttitle   = {The {AI} {Gambit} — {Leveraging} {Artificial} {Intelligence} to {Combat} {Climate} {Change}},
	author       = {Cowls, Josh and Tsamados, Andreas and Taddeo, Mariarosaria and Floridi, Luciano},
	year         = 2021,
	month        = mar,
	address      = {Rochester, NY},
	number       = {ID 3804983},
	doi          = {10.2139/ssrn.3804983},
	url          = {https://papers.ssrn.com/abstract=3804983},
	urldate      = {2021-10-05},
	type         = {{SSRN} {Scholarly} {Paper}},
	abstract     = {In this article we analyse the role that artificial intelligence (AI) could play, and is playing, to combat global climate change. We identify two crucial opportunities that AI offers in this domain: it can help improve and expand current understanding of climate change, and it can contribute to combatting the climate crisis effectively. However, the development of AI also raises two sets of problems when considering climate change: the possible exacerbation of social and ethical challenges already associated with AI, and the contribution to climate change of the greenhouse gases emitted by training data and computation-intensive AI systems. We assess the carbon footprint of AI research, and the factors that influence AI’s greenhouse gas (GHG) emissions in this domain. We find that the carbon footprint of AI research may be significant and highlight the need for more evidence concerning the trade-off between the GHG emissions generated by AI research and the energy and resource efficiency gains that AI can offer. In light of our analysis, we argue that leveraging the opportunities offered by AI for global climate change whilst limiting its risks is a gambit which requires responsive, evidence-based, and effective governance to become a winning strategy. We conclude by identifying the European Union as being especially well-placed to play a leading role in this policy response and provide 13 recommendations that are designed to identify and harness the opportunities of AI for combatting climate change, while reducing its impact on the environment.},
	language     = {en},
	institution  = {Social Science Research Network},
	keywords     = {Artificial Intelligence, Carbon Footprint, Climate Change, Digital Ethics, Digital Governance, Environment, Sustainability},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/5678LH7E/Cowls et al. - 2021 - The AI Gambit — Leveraging Artificial Intelligence.pdf:application/pdf;Markup:/Users/neilnatarajan/Zotero/storage/NQZSQ28J/Cowls et al. - 2021 - The AI Gambit — Leveraging Artificial Intelligence.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/DPBRWVKN/papers.html:text/html}
}
@misc{cozma_automated_2018,
	title        = {Automated essay scoring with string kernels and word embeddings},
	author       = {Cozma, Mădălina and Butnaru, Andrei M. and Ionescu, Radu Tudor},
	year         = 2018,
	month        = jul,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1804.07954},
	url          = {http://arxiv.org/abs/1804.07954},
	urldate      = {2023-08-08},
	note         = {arXiv:1804.07954 [cs]},
	abstract     = {In this work, we present an approach based on combining string kernels and word embeddings for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply string kernels to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.},
	keywords     = {Computer Science - Computation and Language},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6I5AGN6R/1804.html:text/html;Cozma et al_2018_Automated essay scoring with string kernels and word embeddings_annotated.pdf:/Users/neilnatarajan/Zotero/storage/JTCBTTSV/Cozma et al_2018_Automated essay scoring with string kernels and word embeddings_annotated.pdf:application/pdf;Cozma et al_2018_Automated essay scoring with string kernels and word embeddings.pdf:/Users/neilnatarajan/Zotero/storage/JTCBTTSV/Cozma et al_2018_Automated essay scoring with string kernels and word embeddings.pdf:application/pdf}
}
@misc{GiveMeSomeCredit,
	title        = {Give Me Some Credit},
	author       = {Credit Fusion, Will Cukierski},
	year         = 2011,
	publisher    = {Kaggle},
	url          = {https://kaggle.com/competitions/GiveMeSomeCredit}
}
@article{crites_measuring_1994,
	title        = {Measuring the {Affective} and {Cognitive} {Properties} of {Attitudes}: {Conceptual} and {Methodological} {Issues}},
	shorttitle   = {Measuring the {Affective} and {Cognitive} {Properties} of {Attitudes}},
	author       = {Crites, Stephen L. and Fabrigar, Leandre R. and Petty, Richard E.},
	year         = 1994,
	month        = dec,
	journal      = {Personality and Social Psychology Bulletin},
	publisher    = {SAGE Publications Inc},
	volume       = 20,
	number       = 6,
	pages        = {619--634},
	doi          = {10.1177/0146167294206001},
	issn         = {0146-1672},
	url          = {https://doi.org/10.1177/0146167294206001},
	urldate      = {2022-09-16},
	note         = {Publisher: SAGE Publications Inc},
	abstract     = {Despite renewed interest in the affective and cognitive properties of attitudes, assessment of these constructs is plagued by a number of problems. Some techniques for overcoming these problems are outlined, and scales for assessing the affective and cognitive properties of attitudes are reported. Two studies examine the reliability and validity of these scales. Study 1 assesses the internal consistency and the discriminant and convergent validity of these scales and indicates that the scales are useful for assessing the affective and cognitive properties of attitudes toward a wide range of objects. In Study 2, the ability of the scales to differentiate attitudes that are based primarily on affective versus cognitive information is examined by experimentally creating affective or cognitive attitudes in subjects. Analyses reveal that the scales can differentiate between people whose attitudes are based primarily on either affective or cognitive information.},
	language     = {en},
	file         = {SAGE PDF Full Text:/Users/neilnatarajan/Zotero/storage/ZFA8AAB5/Crites et al. - 1994 - Measuring the Affective and Cognitive Properties o.pdf:application/pdf},
	abstractnote = {Despite renewed interest in the affective and cognitive properties of attitudes, assessment of these constructs is plagued by a number of problems. Some techniques for overcoming these problems are outlined, and scales for assessing the affective and cognitive properties of attitudes are reported. Two studies examine the reliability and validity of these scales. Study 1 assesses the internal consistency and the discriminant and convergent validity of these scales and indicates that the scales are useful for assessing the affective and cognitive properties of attitudes toward a wide range of objects. In Study 2, the ability of the scales to differentiate attitudes that are based primarily on affective versus cognitive information is examined by experimentally creating affective or cognitive attitudes in subjects. Analyses reveal that the scales can differentiate between people whose attitudes are based primarily on either affective or cognitive information.}
}
@article{daubner2017dovetailing,
	title        = {Dovetailing talent management and diversity management: the exclusion-inclusion paradox},
	shorttitle   = {Dovetailing talent management and diversity management},
	author       = {Daubner-Siva, Dagmar and Vinkenburg, Claartje J and Jansen, Paul GW},
	year         = 2017,
	month        = jan,
	journal      = {Journal of Organisational Effectiveness: People and Performance},
	publisher    = {Emerald Publishing Limited},
	volume       = 4,
	number       = 4,
	pages        = {315--331},
	doi          = {10.1108/JOEPP-02-2017-0019},
	issn         = {2051-6614},
	url          = {https://doi.org/10.1108/JOEPP-02-2017-0019},
	urldate      = {2024-07-31},
	note         = {Publisher: Emerald Publishing Limited},
	abstract     = {Purpose The purpose of this paper is to adopt a paradox lens for dovetailing the human resource management sub-domains of talent management (TM) and diversity management (DM), in the attempt to create closer alignment between the two. Design/methodology/approach The authors review paradox theory, TM and DM literatures and formulate a paradox that becomes apparent when considering TM and DM simultaneously. Findings The authors coin this tension as the “exclusion-inclusion paradox,” highlighting that TM and DM reflect contradictory, yet interrelated principles: organisations promote exclusion through a TM architecture that focuses on the identification and development of a few selected employees, while simultaneously, organisations promote inclusion, in the attempt to minimize existing inequalities for traditionally marginalized groups. Practical implications Once uncovered, the exclusion-inclusion paradox enables organisational actors to make choices on whether to respond actively or defensively to the paradox. The authors argue for active responses in order to work through the paradox. Originality/value This is the first paper adopting a paradox lens in order to interweave the DM literature with TM literature in the attempt to explain how DM and TM constitute contradicting yet interrelated principles.},
	keywords     = {Diversity management, Dualities, Paradox theory, Talent management},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/WBTYKE77/html.html:text/html;Submitted Version:/Users/neilnatarajan/Zotero/storage/H4AEDT82/Daubner-Siva et al. - 2017 - Dovetailing talent management and diversity manage.pdf:application/pdf}
}
@article{dawid_individual_2017,
	title        = {On {Individual} {Risk}},
	author       = {Dawid, A. Philip},
	year         = 2017,
	month        = sep,
	journal      = {Synthese},
	volume       = 194,
	number       = 9,
	pages        = {3445--3474},
	doi          = {10.1007/s11229-015-0953-4},
	issn         = {0039-7857, 1573-0964},
	url          = {http://arxiv.org/abs/1406.5540},
	urldate      = {2023-03-20},
	note         = {arXiv:1406.5540 [math, stat]},
	abstract     = {We survey a variety of possible explications of the term "Individual Risk." These in turn are based on a variety of interpretations of "Probability," including Classical, Enumerative, Frequency, Formal, Metaphysical, Personal, Propensity, Chance and Logical conceptions of Probability, which we review and compare. We distinguish between "groupist" and "individualist" understandings of Probability, and explore both "group to individual" (G2i) and "individual to group" (i2G) approaches to characterising Individual Risk. Although in the end that concept remains subtle and elusive, some pragmatic suggestions for progress are made.},
	keywords     = {Mathematics - Statistics Theory, Statistics - Applications, 62A01},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/NEFVMUDE/Dawid - 2017 - On Individual Risk.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/I76KSLY2/1406.html:text/html}
}
@book{DeSouza_Leitão_2009,
	title        = {Semiotic Engineering Methods for Scientific Research in HCI},
	author       = {De Souza, Clarisse Sieckenius and Leitão, Carla Faria},
	year         = 2009,
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {Synthesis Lectures on Human-Centered Informatics},
	doi          = {10.1007/978-3-031-02185-5},
	isbn         = {978-3-031-01057-6},
	url          = {https://link.springer.com/10.1007/978-3-031-02185-5},
	rights       = {https://www.springer.com/tdm},
	collection   = {Synthesis Lectures on Human-Centered Informatics},
	language     = {en}
}
@article{dehouche_plagiarism_2021,
	title        = {Plagiarism in the age of massive {Generative} {Pre}-trained {Transformers} ({GPT}-3)},
	author       = {Dehouche, N},
	year         = 2021,
	month        = mar,
	journal      = {Ethics in Science and Environmental Politics},
	volume       = 21,
	pages        = {17--23},
	doi          = {10.3354/esep00195},
	issn         = {1863-5415, 1611-8014},
	url          = {https://www.int-res.com/abstracts/esep/v21/p17-23/},
	urldate      = {2024-02-20},
	abstract     = {As if 2020 was not a peculiar enough year, its fifth month saw the relatively quiet publication of a preprint describing the most powerful natural language processing (NLP) system to date — GPT-3 (Generative Pre-trained Transformer-3) — created by the Silicon Valley research firm OpenAI. Though the software implementation of GPT-3 is still in its initial beta release phase, and its full capabilities are still unknown as of the time of this writing, it has been shown that this artificial intelligence can comprehend prompts in natural language, on virtually any topic, and generate relevant original text content that is indistinguishable from human writing. Moreover, access to these capabilities, in a limited yet worrisome enough extent, is available to the general public. This paper presents examples of original content generated by the author using GPT-3. These examples illustrate some of the capabilities of GPT-3 in comprehending prompts in natural language and generating convincing content in response. I use these examples to raise specific fundamental questions pertaining to the intellectual property of this content and the potential use of GPT-3 to facilitate plagiarism. The goal is to instigate a sense of urgency, as well as a sense of present tardiness on the part of the academic community in addressing these questions.},
	language     = {en},
	file         = {Dehouche - 2021 - Plagiarism in the age of massive Generative Pre-tr.pdf:/Users/neilnatarajan/Zotero/storage/CYAD6KDV/Dehouche - 2021 - Plagiarism in the age of massive Generative Pre-tr.pdf:application/pdf}
}
@article{deming2017growing,
	title        = {The growing importance of social skills in the labor market},
	author       = {Deming, David J},
	year         = 2017,
	journal      = {The quarterly journal of economics},
	publisher    = {Oxford University Press},
	volume       = 132,
	number       = 4,
	pages        = {1593--1640}
}
@misc{destefano_why_2022,
	title        = {Why {Providing} {Humans} with {Interpretable} {Algorithms} {May}, {Counterintuitively}, {Lead} to {Lower} {Decision}-making {Performance}},
	author       = {DeStefano, Timothy and Kellogg, Katherine and Menietti, Michael and Vendraminelli, Luca},
	year         = 2022,
	month        = oct,
	address      = {Rochester, NY},
	doi          = {10.2139/ssrn.4246077},
	url          = {https://papers.ssrn.com/abstract=4246077},
	urldate      = {2023-01-16},
	type         = {{SSRN} {Scholarly} {Paper}},
	abstract     = {How is algorithmic model interpretability related to human acceptance of algorithmic recommendations and performance on decision-making tasks? We explored these questions in a multi-method field study of a large multinational fashion organisation. We first conducted a quantitative field experiment to compare the use of two models—an interpretable versus an uninterpretable algorithmic model— designed to assist employees with decision making around how many products to send to each of its stores. Contrary to what the literature on interpretable algorithms would lead us to expect, under conditions of high perceived uncertainty, decision makers’ use of an uninterpretable algorithmic model was associated with higher acceptance of algorithmic recommendations and higher task performance than was their use of an interpretable algorithmic model with a similar level of performance. We next investigated this puzzling result using 31 interviews with 14 employees—2 algorithm developers, 2 managers, and 10 decision makers. We advance two concepts that suggest a refinement of theory on interpretable algorithms. First, overconfident troubleshooting—a decision maker rejecting a recommendation coming from an interpretable algorithm, because of their belief that they understand the inner workings of complex processes better than they actually do. Second, social proofing the algorithm—including respected peers in the algorithm development and testing process—may make it more likely that decision makers accept recommendations coming from an uninterpretable algorithm in situations characterized by high perceived uncertainty, because the decision makers may seek to reduce their uncertainty by incorporating the opinions of people with their own knowledge base and experience.},
	language     = {en},
	keywords     = {Artificial Intelligence, AI Adoption, AI and Strategy, Algorithm Aversion, Firm Productivity, Human-in-the-loop Decision Making, Interpretable AI, Machine Learning},
	file         = {Submitted Version:/Users/neilnatarajan/Zotero/storage/JLDXXTQ7/DeStefano et al. - 2022 - Why Providing Humans with Interpretable Algorithms.pdf:application/pdf}
}
@misc{dhaini_detecting_2023,
	title        = {Detecting {ChatGPT}: {A} {Survey} of the {State} of {Detecting} {ChatGPT}-{Generated} {Text}},
	shorttitle   = {Detecting {ChatGPT}},
	author       = {Dhaini, Mahdi and Poelman, Wessel and Erdogan, Ege},
	year         = 2023,
	month        = sep,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2309.07689},
	url          = {http://arxiv.org/abs/2309.07689},
	urldate      = {2023-10-04},
	note         = {arXiv:2309.07689 [cs]},
	abstract     = {While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/T6BJ76M3/2309.html:text/html;Dhaini et al_2023_Detecting ChatGPT_annotated.pdf:/Users/neilnatarajan/Zotero/storage/UXZJY9TZ/Dhaini et al_2023_Detecting ChatGPT_annotated.pdf:application/pdf;Dhaini et al_2023_Detecting ChatGPT.pdf:/Users/neilnatarajan/Zotero/storage/UXZJY9TZ/Dhaini et al_2023_Detecting ChatGPT.pdf:application/pdf}
}
@inproceedings{dhawka2023we,
	title        = {We are the data: Challenges and opportunities for creating demographically diverse anthropographics},
	author       = {Dhawka, Priya and He, Helen Ai and Willett, Wesley},
	year         = 2023,
	booktitle    = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	pages        = {1--14}
}
@inproceedings{dhawka2024better,
	title        = {Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics},
	author       = {Dhawka, Priya and Perera, Lauren and Willett, Wesley},
	year         = 2024,
	booktitle    = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	pages        = {1--14}
}
@inproceedings{dombrowski_social_2016,
	title        = {Social {Justice}-{Oriented} {Interaction} {Design}: {Outlining} {Key} {Design} {Strategies} and {Commitments}},
	shorttitle   = {Social {Justice}-{Oriented} {Interaction} {Design}},
	author       = {Dombrowski, Lynn and Harmon, Ellie and Fox, Sarah},
	year         = 2016,
	month        = jun,
	booktitle    = {Proceedings of the 2016 {ACM} {Conference} on {Designing} {Interactive} {Systems}},
	publisher    = {ACM},
	address      = {Brisbane QLD Australia},
	pages        = {656--671},
	doi          = {10.1145/2901790.2901861},
	isbn         = {978-1-4503-4031-1},
	url          = {https://dl.acm.org/doi/10.1145/2901790.2901861},
	urldate      = {2024-07-30},
	abstract     = {In recent years, many HCI designers have begun pursuing research agendas that address large scale social issues. These systemic or "wicked" problems present challenges for design practice due to their scope, scale, complexity, and political nature. In this paper, we develop a social justice orientation to designing for such challenges. We highlight a breadth of design strategies that target the goals of social justice along six dimensions – transformation, recognition, reciprocity, enablement, distribution, and accountability – and elaborate three commitments necessary to developing a social justice oriented design practice – a commitment to conflict, a commitment to reflexivity, and a commitment to personal ethics and politics. Although there are no easy solutions to systemic social issues, a social justice orientation provides one way to foster an engagement with the thorny political issues that are increasingly acknowledged as crucial to a field that is not just about technological possibility, but also about political responsibility.},
	language     = {en},
	file         = {Dombrowski et al_2016_Social Justice-Oriented Interaction Design_annotated.pdf:/Users/neilnatarajan/Zotero/storage/UWQ6E65N/Dombrowski et al_2016_Social Justice-Oriented Interaction Design_annotated.pdf:application/pdf;Dombrowski et al_2016_Social Justice-Oriented Interaction Design.pdf:/Users/neilnatarajan/Zotero/storage/UWQ6E65N/Dombrowski et al_2016_Social Justice-Oriented Interaction Design.pdf:application/pdf}
}
@article{COPPERSMITH198527,
	title        = {Solving NP-hard problems in ‘almost trees’: Vertex cover},
	author       = {Don Coppersmith and Uzi Vishkin},
	year         = 1985,
	journal      = {Discrete Applied Mathematics},
	volume       = 10,
	number       = 1,
	pages        = {27--45},
	doi          = {https://doi.org/10.1016/0166-218X(85)90057-5},
	issn         = {0166-218X},
	url          = {https://www.sciencedirect.com/science/article/pii/0166218X85900575},
	abstract     = {We present an algorithm which finds a minimum vertex cover in a graph G(V, E) in time O(|V|+(ak)2k3), where for connected graphs G the parameter a is defined as the minimum number of edges that must be added to a tree to produce G, and k is the maximum a over all biconnected components of the graph. The algorithm combines two main approaches for coping with NP-completeness, and thereby achieves better running time than algorithms using only one of these approaches.}
}
@article{doshi-velez_towards_2017,
	title        = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	author       = {Doshi-Velez, Finale and Kim, Been},
	year         = 2017,
	month        = mar,
	journal      = {arXiv:1702.08608 [cs, stat]},
	url          = {http://arxiv.org/abs/1702.08608},
	urldate      = {2022-01-17},
	note         = {arXiv: 1702.08608},
	abstract     = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/RHR7VDNR/1702.html:text/html;Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/BCP6WVRJ/Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:application/pdf}
}
@article{dror2023there,
	title        = {Is there an epistemic advantage to being oppressed?},
	author       = {Dror, Lidal},
	year         = 2023,
	journal      = {No{\^u}s},
	publisher    = {Wiley Online Library},
	volume       = 57,
	number       = 3,
	pages        = {618--640}
}
@misc{dua_uci_2017,
	title        = {{UCI} {Machine} {Learning} {Repository}},
	author       = {Dua, Dheeru and Graff, Casey},
	year         = 2017,
	publisher    = {University of California, Irvine, School of Information and Computer Sciences},
	url          = {http://archive.ics.uci.edu/ml}
}
@misc{dugan_raid_2024,
	title        = {{RAID}: {A} {Shared} {Benchmark} for {Robust} {Evaluation} of {Machine}-{Generated} {Text} {Detectors}},
	shorttitle   = {{RAID}},
	author       = {Dugan, Liam and Hwang, Alyssa and Trhlik, Filip and Ludan, Josh Magnus and Zhu, Andrew and Xu, Hainiu and Ippolito, Daphne and Callison-Burch, Chris},
	year         = 2024,
	month        = jun,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2405.07940},
	url          = {http://arxiv.org/abs/2405.07940},
	urldate      = {2024-08-01},
	note         = {arXiv:2405.07940 [cs]},
	abstract     = {Many commercial and open-source models claim to detect machine-generated text with extremely high accuracy (99\% or more). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging-lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our data along with a leaderboard to encourage future research.},
	keywords     = {Computer Science - Computation and Language, I.2.7},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/TCQ2VFPR/Dugan et al. - 2024 - RAID A Shared Benchmark for Robust Evaluation of .pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/QK8J75US/2405.html:text/html}
}
@article{guilford1967creativity,
	title        = {Creativity: Yesterday, today and tomorrow},
	author       = {Guilford, Joy P},
	year         = 1967,
	journal      = {The Journal of Creative Behavior},
	publisher    = {Wiley Online Library},
	volume       = 1,
	number       = 1,
	pages        = {3--14}
}
@article{dumas_measuring_2020,
	title        = {Measuring {Divergent} {Thinking} {Originality} {With} {Human} {Raters} and {Text}-{Mining} {Models}: {A} {Psychometric} {Comparison} of {Methods}},
	shorttitle   = {Measuring {Divergent} {Thinking} {Originality} {With} {Human} {Raters} and {Text}-{Mining} {Models}},
	author       = {Dumas, Denis and Organisciak, Peter and Doherty, Michael},
	year         = 2020,
	month        = jul,
	journal      = {Psychology of Aesthetics Creativity and the Arts},
	doi          = {10.1037/aca0000319},
	abstract     = {
		Within creativity research, interest and capability in utilizing text-mining models to quantify the Originality of participant responses to Divergent Thinking tasks has risen sharply over the last decade, with many extant studies fruitfully using such methods to uncover substantive patterns among creativity-relevant constructs. However, no systematic psychometric investigation of the reliability and validity of human-rated Originality scores, and scores from various freely available text-mining systems, exists in the literature. Here we conduct such an investigation with the Alternate Uses Task. We demonstrate that, despite their inherent subjectivity, human-rated Originality scores displayed the highest reliability at both the composite and latent factor levels. However, the text-mining system GloVe 840B was highly capable of approximating human-rated scores both in its measurement properties and its correlations to various creativity-related criteria including ideational Fluency, Elaboration, Openness, Intellect, and self-reported Creative Activities. We conclude that, in conjunction with other salient indicators of creative potential, text-mining models (and especially the GloVe 840B system) are capable of supporting reliable and valid inferences about Divergent Thinking.

		An open access system for producing the Originality scores that were psychometrically examined in this paper is available for free at our website: https://openscoring.du.edu/.

		Please use for your research and let us know if you encounter any bugs!
	},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/KRUTG4VE/Dumas et al. - 2020 - Measuring Divergent Thinking Originality With Huma.pdf:application/pdf}
}
@article{dwivedi2021artificial,
	title        = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
	shorttitle   = {Artificial {Intelligence} ({AI})},
	author       = {Dwivedi, Yogesh K and Hughes, Laurie and Ismagilova, Elvira and Aarts, Gert and Coombs, Crispin and Crick, Tom and Duan, Yanqing and Dwivedi, Rohita and Edwards, John and Eirug, Aled and others},
	year         = 2021,
	month        = apr,
	journal      = {International Journal of Information Management},
	publisher    = {Elsevier},
	volume       = 57,
	pages        = 101994,
	doi          = {10.1016/j.ijinfomgt.2019.08.002},
	issn         = {02684012},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S026840121930917X},
	urldate      = {2023-08-01},
	date-added   = {2023-09-12 15:57:11 -0400},
	date-modified = {2023-09-12 15:57:11 -0400},
	abstract     = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportu­ nities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.},
	language     = {en},
	file         = {Dwivedi et al_2021_Artificial Intelligence (AI)_annotated.pdf:/Users/neilnatarajan/Zotero/storage/V9PPGCVI/Dwivedi et al_2021_Artificial Intelligence (AI)_annotated.pdf:application/pdf;Dwivedi et al_2021_Artificial Intelligence (AI).pdf:/Users/neilnatarajan/Zotero/storage/V9PPGCVI/Dwivedi et al_2021_Artificial Intelligence (AI).pdf:application/pdf}
}
@inproceedings{dwork_fairness_2012,
	title        = {Fairness through awareness},
	author       = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year         = 2012,
	month        = jan,
	booktitle    = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{ITCS} '12},
	pages        = {214--226},
	doi          = {10.1145/2090236.2090255},
	isbn         = {978-1-4503-1115-1},
	url          = {https://doi.org/10.1145/2090236.2090255},
	urldate      = {2022-01-27},
	abstract     = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	file         = {Submitted Version:/Users/neilnatarajan/Zotero/storage/ZCLN5LCA/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf}
}
@techreport{dynarski2018closing,
	title        = {Closing the gap: The effect of a targeted, tuition-free promise on college choices of high-achieving, low-income students},
	author       = {Dynarski, Susan and Libassi, CJ and Michelmore, Katherine and Owen, Stephanie},
	year         = 2018,
	institution  = {National Bureau of Economic Research}
}
@article{dzindolet_role_2003,
	title        = {The role of trust in automation reliance},
	author       = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
	year         = 2003,
	month        = jun,
	journal      = {International Journal of Human-Computer Studies},
	series       = {Trust and {Technology}},
	volume       = 58,
	number       = 6,
	pages        = {697--718},
	doi          = {10.1016/S1071-5819(03)00038-7},
	issn         = {1071-5819},
	url          = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
	urldate      = {2022-01-26},
	abstract     = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
	language     = {en},
	keywords     = {\_tablet, Automation reliance, Automation trust, Disuse, Misuse, User Study},
	file         = {Dzindolet et al_2003_The role of trust in automation reliance.pdf:/Users/neilnatarajan/Zotero/storage/BY6ELP6C/Dzindolet et al_2003_The role of trust in automation reliance.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/VYE77HHX/S1071581903000387.html:text/html}
}
@article{elkhatat_evaluating_2023,
	title        = {Evaluating the efficacy of {AI} content detection tools in differentiating between human and {AI}-generated text},
	author       = {Elkhatat, Ahmed M. and Elsaid, Khaled and Almeer, Saeed},
	year         = 2023,
	month        = dec,
	journal      = {International Journal for Educational Integrity},
	volume       = 19,
	number       = 1,
	pages        = {1--16},
	doi          = {10.1007/s40979-023-00140-5},
	issn         = {1833-2595},
	url          = {https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00140-5},
	urldate      = {2024-07-31},
	copyright    = {2023 The Author(s)},
	note         = {Number: 1 Publisher: BioMed Central},
	abstract     = {The proliferation of artificial intelligence (AI)-generated content, particularly from models like ChatGPT, presents potential challenges to academic integrity and raises concerns about plagiarism. This study investigates the capabilities of various AI content detection tools in discerning human and AI-authored content. Fifteen paragraphs each from ChatGPT Models 3.5 and 4 on the topic of cooling towers in the engineering process and five human-witten control responses were generated for evaluation. AI content detection tools developed by OpenAI, Writer, Copyleaks, GPTZero, and CrossPlag were used to evaluate these paragraphs. Findings reveal that the AI detection tools were more accurate in identifying content generated by GPT 3.5 than GPT 4. However, when applied to human-written control responses, the tools exhibited inconsistencies, producing false positives and uncertain classifications. This study underscores the need for further development and refinement of AI content detection tools as AI-generated content becomes more sophisticated and harder to distinguish from human-written text.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/BYLLQC4A/Elkhatat et al. - 2023 - Evaluating the efficacy of AI content detection to.pdf:application/pdf}
}
@misc{experience_diary_nodate,
	title        = {Diary {Studies}: {Understanding} {Long}-{Term} {User} {Behavior} and {Experiences}},
	shorttitle   = {Diary {Studies}},
	author       = {Experience, World Leaders in Research-Based User},
	journal      = {Nielsen Norman Group},
	url          = {https://www.nngroup.com/articles/diary-studies/},
	urldate      = {2023-03-31},
	abstract     = {User logs (diaries) of daily activities as they occur give contextual insights about real-time user behaviors and needs, helping define UX feature requirements.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/H484KAID/diary-studies.html:text/html}
}
@inproceedings{faliagka_application_2012,
	title        = {Application of {Machine} {Learning} {Algorithms} to an online {Recruitment} {System}},
	author       = {Faliagka, Evanthia and Ramantas, Kostas and Tsakalidis, Athanasios and Tzimas, Giannis},
	year         = 2012,
	month        = jan,
	abstract     = {In this work, we present a novel approach for evaluating job applicants in online recruitment systems, leveraging machine learning algorithms to solve the candidate ranking problem. An application of our approach is implemented in the form of a prototype system, whose functionality is showcased and evaluated in a real-world recruitment scenario. The proposed system extracts a set of objective criteria from the applicants' LinkedIn profile, and infers their personality characteristics using linguistic analysis on their blog posts. Our system was found to perform consistently compared to human recruiters; thus, it can be trusted for the automation of applicant ranking and personality mining.},
	keywords     = {\_tablet},
	file         = {Faliagka et al_2012_Application of Machine Learning Algorithms to an online Recruitment System.pdf:/Users/neilnatarajan/Zotero/storage/TGN5TFTH/Faliagka et al_2012_Application of Machine Learning Algorithms to an online Recruitment System.pdf:application/pdf}
}
@article{farber1996learning,
	title        = {Learning and wage dynamics},
	author       = {Farber, Henry S and Gibbons, Robert},
	year         = 1996,
	journal      = {The Quarterly Journal of Economics},
	publisher    = {MIT Press},
	volume       = 111,
	number       = 4,
	pages        = {1007--1047}
}
@inproceedings{Feldman_Harshaw_Karbasi_2017,
	title        = {Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization},
	author       = {Feldman, Moran and Harshaw, Christopher and Karbasi, Amin},
	year         = 2017,
	month        = jun,
	booktitle    = {Proceedings of the 2017 Conference on Learning Theory},
	publisher    = {PMLR},
	pages        = {758–784},
	issn         = {2640-3498},
	url          = {https://proceedings.mlr.press/v65/feldman17b.html},
	abstractnote = {It is known that greedy methods perform well for maximizing textitmonotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity.  In this paper, we show—arguably, surprisingly—that invoking the classical greedy algorithm $O(sqrt{k})$-times leads to the (currently) fastest deterministic algorithm, called RepeatedGreedy, for maximizing a general submodular function subject to $k$-independent system constraints. RepeatedGreedy achieves $(1 + O(1/sqrt{k}))k$ approximation using $O(nrsqrt{k})$ function evaluations (here, $n$ and $r$ denote the size of the ground set and the maximum size of a feasible solution, respectively). We then show that by a careful sampling procedure, we can run the greedy algorithm only textitonce and obtain the (currently) fastest randomized algorithm, called SampleGreedy, for maximizing a submodular function subject to $k$-extendible system constraints (a subclass of $k$-independent system constrains). SampleGreedy achieves $(k + 3)$-approximation with only $O(nr/k)$ function evaluations. Finally, we derive an almost matching lower bound, and show that no polynomial time algorithm can have an approximation ratio smaller than $ k + 1/2 - varepsilon$. To further support our theoretical results, we compare the performance of RepeatedGreedy and SampleGreedy with prior art in a concrete application (movie recommendation). We consistently observe that while SampleGreedy achieves practically the same utility as the best baseline, it performs at least two orders of magnitude faster.},
	language     = {en}
}
@article{flanigan_fair_2021,
	title        = {Fair algorithms for selecting citizens’ assemblies},
	author       = {Flanigan, Bailey and Gölz, Paul and Gupta, Anupam and Hennig, Brett and Procaccia, Ariel D.},
	year         = 2021,
	month        = aug,
	journal      = {Nature},
	volume       = 596,
	number       = 7873,
	pages        = {548--552},
	doi          = {10.1038/s41586-021-03788-6},
	issn         = {1476-4687},
	url          = {https://www.nature.com/articles/s41586-021-03788-6},
	urldate      = {2023-05-17},
	copyright    = {2021 The Author(s)},
	note         = {Number: 7873 Publisher: Nature Publishing Group},
	abstract     = {Globally, there has been a recent surge in ‘citizens’ assemblies’1, which are a form of civic participation in which a panel of randomly selected constituents contributes to questions of policy. The random process for selecting this panel should satisfy two properties. First, it must produce a panel that is representative of the population. Second, in the spirit of democratic equality, individuals would ideally be selected to serve on this panel with equal probability2,3. However, in practice these desiderata are in tension owing to differential participation rates across subpopulations4,5. Here we apply ideas from fair division to develop selection algorithms that satisfy the two desiderata simultaneously to the greatest possible extent: our selection algorithms choose representative panels while selecting individuals with probabilities as close to equal as mathematically possible, for many metrics of ‘closeness to equality’. Our implementation of one such algorithm has already been used to select more than 40 citizens’ assemblies around the world. As we demonstrate using data from ten citizens’ assemblies, adopting our algorithm over a benchmark representing the previous state of the art leads to substantially fairer selection probabilities. By contributing a fairer, more principled and deployable algorithm, our work puts the practice of sortition on firmer foundations. Moreover, our work establishes citizens’ assemblies as a domain in which insights from the field of fair division can lead to high-impact applications.},
	language     = {en},
	keywords     = {\_tablet, Computer science, Economics},
	file         = {Flanigan et al_2021_Fair algorithms for selecting citizens’ assemblies.pdf:/Users/neilnatarajan/Zotero/storage/HJESJVT9/Flanigan et al_2021_Fair algorithms for selecting citizens’ assemblies.pdf:application/pdf}
}
@book{Friedman_2005,
	title        = {The World Is Flat: A Brief History of the Twenty-first Century},
	author       = {Friedman, T.L.},
	year         = 2005,
	publisher    = {Farrar, Straus and Giroux},
	series       = {Business book summary},
	isbn         = {978-0-374-29288-1},
	url          = {https://books.google.com/books?id=g3PbAgAAQBAJ},
	collection   = {Business book summary}
}
@article{Boaz_Hanney_Borst_O’Shea_Kok_2018,
	title        = {How to engage stakeholders in research: design principles to support improvement},
	author       = {Boaz, Annette and Hanney, Stephen and Borst, Robert and O’Shea, Alison and Kok, Maarten},
	year         = 2018,
	month        = jul,
	journal      = {Health Research Policy and Systems},
	volume       = 16,
	number       = 1,
	pages        = 60,
	doi          = {10.1186/s12961-018-0337-6},
	issn         = {1478-4505},
	abstractnote = {Closing the gap between research production and research use is a key challenge for the health research system. Stakeholder engagement is being increasingly promoted across the board by health research funding organisations, and indeed by many researchers themselves, as an important pathway to achieving impact. This opinion piece draws on a study of stakeholder engagement in research and a systematic literature search conducted as part of the study.}
}
@article{fleisher_whats_nodate,
	title        = {What's {Fair} about {Individual} {Fairness}?},
	author       = {Fleisher, Will},
	year         = 2021,
	month        = jul,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	address      = {Rochester, NY},
	number       = 3819799,
	pages        = 12,
	doi          = {10.2139/ssrn.3819799},
	url          = {https://papers.ssrn.com/abstract=3819799},
	urldate      = {2024-04-01},
	abstract     = {One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle I call “similar treatment,” which requires that similar individuals be treated similarly. IF offers a precise account of this deﬁnition using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct deﬁnition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a deﬁnition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a deﬁnition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufﬁcient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to deﬁne fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a deﬁnition of fairness, and instead should be seen as one tool among many for ameliorating algorithmic bias.},
	language     = {en},
	file         = {Fleisher - What's Fair about Individual Fairness.pdf:/Users/neilnatarajan/Zotero/storage/FVPG2475/Fleisher - What's Fair about Individual Fairness.pdf:application/pdf},
	date-added   = {2023-09-11 10:31:46 -0400},
	date-modified = {2023-09-11 10:46:12 -0400},
	type         = {{SSRN} {Scholarly} {Paper}},
	keywords     = {Algorithmic fairness, Ethics of AI, Incommensurable values, Individual fairness},
	abstractnote = {One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle I call “similar treatment,” which requires that similar individuals be treated similarly. IF offers a precise account of this definition using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among many for ameliorating algorithmic bias.}
}
@article{foody_thematic_2004,
	title        = {Thematic {Map} {Comparison}},
	author       = {Foody, Giles M.},
	year         = 2004,
	month        = may,
	journal      = {Photogrammetric Engineering \& Remote Sensing},
	volume       = 70,
	number       = 5,
	pages        = {627--633},
	doi          = {10.14358/PERS.70.5.627},
	abstract     = {The accuracy of thematic maps derived by image classification analyses is often compared in remote sensing studies. This comparison is typically achieved by a basic subjective assessment of the observed difference in accuracy but should be undertaken in a statistically rigorous fashion. One approach for the evaluation of the statistical significance of a difference in map accuracy that has been widely used in remote sensing research is based on the comparison of the kappa coefficient of agreement derived for each map. The conventional approach to the comparison of kappa coefficients assumes that the samples used in their calculation are independent, an assumption that is commonly unsatisfied because the same sample of ground data sites is often used for each map. Alternative methods to evaluate the statistical significance of differences in accuracy are available for both related and independent samples. Approaches for map comparison based on the kappa coefficient and proportion of correctly allocated cases, the two most widely used metrics of thematic map accuracy in remote sensing, are discussed. An example illustrates how classifications based on the same sample of ground data sites may be compared rigorously and highlights the importance of distinguishing between one- and two-sided statistical tests in the comparison of classification accuracy statements.},
	file         = {Foody_2004_Thematic Map Comparison_annotated.pdf:/Users/neilnatarajan/Zotero/storage/H3PIX7WZ/Foody_2004_Thematic Map Comparison_annotated.pdf:application/pdf;Foody_2004_Thematic Map Comparison.pdf:/Users/neilnatarajan/Zotero/storage/H3PIX7WZ/Foody_2004_Thematic Map Comparison.pdf:application/pdf}
}
@article{ford_play_2020,
	title        = {Play {MNIST} {For} {Me}! {User} {Studies} on the {Effects} of {Post}-{Hoc}, {Example}-{Based} {Explanations} \& {Error} {Rates} on {Debugging} a {Deep} {Learning}, {Black}-{Box} {Classifier}},
	author       = {Ford, Courtney and Kenny, Eoin M. and Keane, Mark T.},
	year         = 2020,
	journal      = {ArXiv},
	abstract     = {Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct and as error rates increase above 4\%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. This paper reports two experiments (N=349) on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4\%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.},
	keywords     = {User Study},
	file         = {Ford et al_2020_Play MNIST For Me.pdf:/Users/neilnatarajan/Zotero/storage/VS3NLDMT/Ford et al_2020_Play MNIST For Me.pdf:application/pdf;Ford et al_2020_Play MNIST For Me.pdf:/Users/neilnatarajan/Zotero/storage/VS3NLDMT/false:application/pdf}
}
@article{Friedler_Scheidegger_Venkatasubramanian_2016,
	title        = {On the (im)possibility of fairness},
	author       = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year         = 2016,
	month        = sep,
	publisher    = {arXiv},
	number       = {arXiv:1609.07236},
	doi          = {10.48550/arXiv.1609.07236},
	url          = {http://arxiv.org/abs/1609.07236},
	urldate      = {2024-05-22},
	note         = {arXiv:1609.07236 [cs, stat]},
	abstractnote = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the “observed” space) and outputs (the “decision” space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
	abstract     = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the "observed" space) and outputs (the "decision" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
	keywords     = {Statistics - Machine Learning, Computer Science - Computers and Society},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/FRE8WKVT/Friedler et al. - 2016 - On the (im)possibility of fairness.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/9ZUBXXMK/1609.html:text/html}
}
@article{friedrich_taxonomy_2011,
	title        = {A {Taxonomy} for {Generating} {Explanations} in {Recommender} {Systems}},
	author       = {Friedrich, Gerhard and Zanker, Markus},
	year         = 2011,
	month        = jun,
	journal      = {AI Magazine},
	volume       = 32,
	number       = 3,
	pages        = {90--98},
	doi          = {10.1609/aimag.v32i3.2365},
	issn         = {2371-9621},
	url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2365},
	urldate      = {2022-01-05},
	copyright    = {Copyright (c)},
	note         = {Number: 3},
	abstract     = {In recommender systems, explanations serve as an additional type of information that can help users to better understand the system's output and promote objectives such as trust, confidence in decision making or utility. This article proposes a taxonomy to categorize and review the research in the area of explanations. It provides a unified view on the different recommendation paradigms, allowing similarities and differences to be clearly identified. Finally, the authors present their view on open research issues and opportunities for future work on this topic.},
	language     = {en},
	file         = {Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:/Users/neilnatarajan/Zotero/storage/KNRSFJBW/Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:application/pdf},
	rights       = {Copyright (c)},
	abstractnote = {In recommender systems, explanations serve as an additional type of information that can help users to better understand the system’s output and promote objectives such as trust, confidence in decision making or utility. This article proposes a taxonomy to categorize and review the research in the area of explanations. It provides a unified view on the different recommendation paradigms, allowing similarities and differences to be clearly identified. Finally, the authors present their view on open research issues and opportunities for future work on this topic.}
}
@article{Gatian_1994,
	title        = {Is user satisfaction a valid measure of system effectiveness?},
	author       = {Gatian, Amy W.},
	year         = 1994,
	month        = mar,
	journal      = {Information \& Management},
	volume       = 26,
	number       = 3,
	pages        = {119–131},
	doi          = {10.1016/0378-7206(94)90036-1},
	issn         = {0378-7206},
	abstractnote = {User satisfaction (US) is often used as a surrogate measure of information system effectiveness. If an effective system is defined as one that adds value to the firm, then an effective system must have some positive influence on user behavior (i.e., improve productivity, decision making, etc.). Advocates of US argue that there is theoretical support for linking attitudes (i.e., satisfaction) and behavior in the psychology literature. At the same time, there is evidence of increasing employment of US questionnaires in firms as a measure of system effectiveness. Yet there is surprisingly little information systems research linking user satisfaction with user behavior. In this study, measures of user satisfaction and system affected behavior are taken for an indirect and a direct user group of the same information system in 39 organisations. Results indicate that a relationship does exist between satisfaction and behavior for both user groups.}
}
@article{Gettier_1963,
	title        = {Is Justified True Belief Knowledge?},
	author       = {Gettier, Edmund},
	year         = 1963,
	journal      = {Analysis},
	publisher    = {Analytica},
	volume       = 23,
	number       = 6,
	pages        = {121–123},
	doi          = {10.1093/analys/23.6.121}
}
@inproceedings{ghadiri_socially_2021,
	title        = {Socially {Fair} k-{Means} {Clustering}},
	author       = {Ghadiri, Mehrdad and Samadi, Samira and Vempala, Santosh},
	year         = 2021,
	month        = mar,
	booktitle    = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {ACM},
	address      = {Virtual Event Canada},
	pages        = {438--448},
	doi          = {10.1145/3442188.3445906},
	isbn         = {978-1-4503-8309-7},
	url          = {https://dl.acm.org/doi/10.1145/3442188.3445906},
	urldate      = {2021-09-16},
	language     = {en},
	file         = {Markup:/Users/neilnatarajan/Zotero/storage/MKXAS2NL/Socially Fair k-Means Clustering.pdf:application/pdf;remarkable ver:/Users/neilnatarajan/Zotero/storage/FQ428NM3/Ghadiri et al. - 2021 - Socially Fair k-Means Clustering.pdf:application/pdf;Submitted Version:/Users/neilnatarajan/Zotero/storage/S89UG6FD/Ghadiri et al. - 2021 - Socially Fair k-Means Clustering.pdf:application/pdf}
}
@article{gillet_diversity_2011,
	title        = {Diversity selection algorithms},
	author       = {Gillet, Valerie J.},
	year         = 2011,
	journal      = {WIREs Computational Molecular Science},
	publisher    = {Wiley Online Library},
	volume       = 1,
	number       = 4,
	pages        = {580--589},
	doi          = {10.1002/wcms.33},
	issn         = {1759-0884},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.33},
	urldate      = {2023-02-14},
	note         = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcms.33},
	abstract     = {Molecular diversity has been an important topic in chemoinformatics for the last two decades following the introduction of high-throughput screening and combinatorial chemistry techniques. This article reviews the main algorithms that have been developed for assessing the diversity of a set of compounds and for selecting a diverse subset of compounds from a larger library. Particular focus is given to recent trends including the use of scaffolds as a way of assessing molecular diversity and the importance now given to optimizing multiple properties simultaneously in an attempt to reduce late stage attrition during the drug development stage of drug discovery. © 2011 John Wiley \& Sons, Ltd. WIREs Comput Mol Sci 2011 1 580-589 DOI: 10.1002/wcms.33 This article is categorized under: Computer and Information Science {\textgreater} Chemoinformatics},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/8UG78XGU/Gillet - 2011 - Diversity selection algorithms.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/BQZF3I82/wcms.html:text/html},
	date-added   = {2023-09-11 10:58:01 -0400},
	date-modified = {2023-09-11 10:58:01 -0400}
}
@article{gilpin_chaos_nodate,
	title        = {Chaos as an interpretable benchmark for...},
	author       = {Gilpin},
	file         = {Gilpin - 2021 - Chaos as an interpretable benchmark for forecastin.pdf:/Users/neilnatarajan/Zotero/storage/N5S487Q2/Gilpin - 2021 - Chaos as an interpretable benchmark for forecastin.pdf:application/pdf}
}
@misc{Goodhart,
	title        = {Too diverse?},
	author       = {Goodhart, David},
	year         = 2004,
	url          = {https://www.sneps.net/t/images/Articles/Goodhart%20-%20too%20diverse.pdf},
	abstractnote = {Is Britain becoming too diverse to sustain the mutual obligations behind a good society and the welfare state?},
	language     = {en}
}
@inproceedings{gordon_disagreement_2021,
	title        = {The {Disagreement} {Deconvolution}: {Bringing} {Machine} {Learning} {Performance} {Metrics} {In} {Line} {With} {Reality}},
	shorttitle   = {The {Disagreement} {Deconvolution}},
	author       = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
	year         = 2021,
	month        = may,
	booktitle    = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {ACM},
	address      = {Yokohama Japan},
	pages        = {1--14},
	doi          = {10.1145/3411764.3445423},
	isbn         = {978-1-4503-8096-6},
	url          = {https://dl.acm.org/doi/10.1145/3411764.3445423},
	urldate      = {2021-10-29},
	language     = {en},
	file         = {Gordon et al_2021_The Disagreement Deconvolution.pdf:/Users/neilnatarajan/Zotero/storage/MYR9QZ5A/Gordon et al_2021_The Disagreement Deconvolution.pdf:application/pdf}
}
@misc{gozalo-brizuela_survey_2023,
	title        = {A survey of {Generative} {AI} {Applications}},
	author       = {Gozalo-Brizuela, Roberto and Garrido-Merchán, Eduardo C.},
	year         = 2023,
	month        = jun,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2306.02781},
	url          = {http://arxiv.org/abs/2306.02781},
	urldate      = {2023-08-31},
	note         = {arXiv:2306.02781 [cs]},
	abstract     = {Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/T6AULMZK/Gozalo-Brizuela and Garrido-Merchán - 2023 - A survey of Generative AI Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/JMFDMJWK/2306.html:text/html}
}
@misc{gptzero_gptzero_2023,
	title        = {{GPTZero} {\textbar} {Technology}},
	author       = {GPTZero},
	year         = 2023,
	month        = aug,
	journal      = {GPTZero},
	url          = {https://gptzero.me/},
	urldate      = {2023-08-31},
	abstract     = {The World's \#1 AI Content Detector with over 1 Million Users},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/DYUXWK7T/technology.html:text/html}
}
@article{green_principles_2019,
	title        = {The {Principles} and {Limits} of {Algorithm}-in-the-{Loop} {Decision} {Making}},
	author       = {Green, Ben and Chen, Yiling},
	year         = 2019,
	month        = nov,
	journal      = {Proceedings of the ACM on Human-Computer Interaction},
	volume       = 3,
	number       = {CSCW},
	pages        = {1--24},
	doi          = {10.1145/3359152},
	issn         = {2573-0142},
	url          = {https://dl.acm.org/doi/10.1145/3359152},
	urldate      = {2023-01-25},
	abstract     = {The rise of machine learning has fundamentally altered decision making: rather than being made solely by people, many important decisions are now made through an "algorithm-in-the-loop'' process where machine learning models inform people. Yet insufficient research has considered how the interactions between people and models actually influence human decisions. Society lacks both clear normative principles regarding how people should collaborate with algorithms as well as robust empirical evidence about how people do collaborate with algorithms. Given research suggesting that people struggle to interpret machine learning models and to incorporate them into their decisions---sometimes leading these models to produce unexpected outcomes---it is essential to consider how different ways of presenting models and structuring human-algorithm interactions affect the quality and type of decisions made. This paper contributes to such research in two ways. First, we posited three principles as essential to ethical and responsible algorithm-in-the-loop decision making. Second, through a controlled experimental study on Amazon Mechanical Turk, we evaluated whether people satisfy these principles when making predictions with the aid of a risk assessment. We studied human predictions in two contexts (pretrial release and financial lending) and under several conditions for risk assessment presentation and structure. Although these conditions did influence participant behaviors and in some cases improved performance, only one desideratum was consistently satisfied. Under all conditions, our study participants 1) were unable to effectively evaluate the accuracy of their own or the risk assessment's predictions, 2) did not calibrate their reliance on the risk assessment based on the risk assessment's performance, and 3) exhibited bias in their interactions with the risk assessment. These results highlight the urgent need to expand our analyses of algorithmic decision making aids beyond evaluating the models themselves to investigating the full sociotechnical contexts in which people and algorithms interact.},
	language     = {en},
	file         = {Green and Chen - 2019 - The Principles and Limits of Algorithm-in-the-Loop.pdf:/Users/neilnatarajan/Zotero/storage/UPPYMVGI/Green and Chen - 2019 - The Principles and Limits of Algorithm-in-the-Loop.pdf:application/pdf},
	abstractnote = {The rise of machine learning has fundamentally altered decision making: rather than being made solely by people, many important decisions are now made through an "algorithm-in-the-loop’’ process where machine learning models inform people. Yet insufficient research has considered how the interactions between people and models actually influence human decisions. Society lacks both clear normative principles regarding how people should collaborate with algorithms as well as robust empirical evidence about how people do collaborate with algorithms. Given research suggesting that people struggle to interpret machine learning models and to incorporate them into their decisions---sometimes leading these models to produce unexpected outcomes---it is essential to consider how different ways of presenting models and structuring human-algorithm interactions affect the quality and type of decisions made. This paper contributes to such research in two ways. First, we posited three principles as essential to ethical and responsible algorithm-in-the-loop decision making. Second, through a controlled experimental study on Amazon Mechanical Turk, we evaluated whether people satisfy these principles when making predictions with the aid of a risk assessment. We studied human predictions in two contexts (pretrial release and financial lending) and under several conditions for risk assessment presentation and structure. Although these conditions did influence participant behaviors and in some cases improved performance, only one desideratum was consistently satisfied. Under all conditions, our study participants 1) were unable to effectively evaluate the accuracy of their own or the risk assessment’s predictions, 2) did not calibrate their reliance on the risk assessment based on the risk assessment’s performance, and 3) exhibited bias in their interactions with the risk assessment. These results highlight the urgent need to expand our analyses of algorithmic decision making aids beyond evaluating the models themselves to investigating the full sociotechnical contexts in which people and algorithms interact.}
}
@inbook{Grice_1975,
	title        = {Logic and Conversation},
	author       = {Grice, H. P.},
	year         = 1975,
	booktitle    = {The Logic of Grammar},
	pages        = {64–75},
	editor       = {Davidson, Donald and Harman, Gilbert},
	file         = {Grice-Logic-libre.pdf:/Users/neilnatarajan/Zotero/storage/5HFPA2VY/Grice-Logic-libre.pdf:application/pdf;Grice-Logic.pdf:/Users/neilnatarajan/Zotero/storage/BA56RKZ6/Grice-Logic.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/MR5EYUNJ/GRILAC-6.html:text/html}
}
@article{Griffiths_Johnson_Hartley_2007,
	title        = {User satisfaction as a measure of system performance},
	author       = {Griffiths, Jillian R. and Johnson, Frances and Hartley, Richard J.},
	year         = 2007,
	month        = sep,
	journal      = {Journal of Librarianship and Information Science},
	publisher    = {SAGE Publications Ltd},
	volume       = 39,
	number       = 3,
	pages        = {142–152},
	doi          = {10.1177/0961000607080417},
	issn         = {0961-0006},
	abstractnote = {It is evident from previous research that user satisfaction is a multidimensional, subjective variable which can be affected by many factors other than performance of the system or searcher. This article draws on information retrieval and information systems literature in an attempt to understand what user satisfaction is, how it is measured, what factors affect it, and why findings on user satisfaction have been so varied and contradictory. It concludes with recommendations for future investigation of the use of user satisfaction as a measure of system performance.},
	language     = {en}
}
@article{grimon2022impact,
	title        = {The Impact of Algorithmic Tools on Child Protection: Evidence from a Randomized Controlled Trial},
	author       = {Grimon, Marie-Pascale and Mills, Christopher},
	year         = 2022,
	journal      = {Job market paper}
}
@article{Grinsztajn-et-al,
	title        = {Why do tree-based models still outperform deep learning on tabular data?},
	author       = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
	year         = 2022,
	month        = jul,
	journal      = {arXiv preprint arXiv:2207.08815},
	publisher    = {arXiv},
	number       = {arXiv:2207.08815},
	doi          = {10.48550/arXiv.2207.08815},
	url          = {http://arxiv.org/abs/2207.08815},
	urldate      = {2024-03-13},
	note         = {arXiv:2207.08815 [cs, stat]},
	abstract     = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\${\textbackslash}sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/SF242E84/Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/VNWIJG59/2207.html:text/html},
	abstractnote = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.}
}
@article{halpern_causes_2005,
	title        = {Causes and {Explanations}: {A} {Structural}-{Model} {Approach}, {Part} {I}: {Causes}},
	shorttitle   = {Causes and {Explanations}},
	author       = {Halpern, Joseph Y. and Pearl, Judea},
	year         = 2005,
	month        = nov,
	journal      = {arXiv:cs/0011012},
	url          = {http://arxiv.org/abs/cs/0011012},
	urldate      = {2021-11-25},
	note         = {arXiv: cs/0011012},
	abstract     = {We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.},
	keywords     = {Computer Science - Artificial Intelligence, I.2.4},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/YBSIIHGN/0011012.html:text/html;Halpern_Pearl_2005_Causes and Explanations.pdf:/Users/neilnatarajan/Zotero/storage/STQSIK6S/Halpern_Pearl_2005_Causes and Explanations.pdf:application/pdf}
}
@article{halpern_causes_2005-1,
	title        = {Causes and {Explanations}: {A} {Structural}-{Model} {Approach}. {Part} {II}: {Explanations}},
	shorttitle   = {Causes and {Explanations}},
	author       = {Halpern, Joseph Y. and Pearl, Judea},
	year         = 2005,
	month        = nov,
	journal      = {arXiv:cs/0208034},
	url          = {http://arxiv.org/abs/cs/0208034},
	urldate      = {2021-11-25},
	note         = {arXiv: cs/0208034},
	abstract     = {We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.},
	keywords     = {Computer Science - Artificial Intelligence, I.2.4},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6GUZ9GVH/0208034.html:text/html;Halpern_Pearl_2005_Causes and Explanations.pdf:/Users/neilnatarajan/Zotero/storage/WJKCTBZL/Halpern_Pearl_2005_Causes and Explanations.pdf:application/pdf}
}
@book{hardin_trust_2002,
	title        = {Trust and trustworthiness},
	author       = {Hardin, Russell},
	year         = 2002,
	publisher    = {Russell Sage Foundation},
	address      = {New York, NY, US},
	series       = {Trust and trustworthiness},
	pages        = {xxi, 234},
	isbn         = {978-0-87154-342-4},
	note         = {Pages: xxi, 234},
	abstractnote = {In this book the author addresses the the standard theories of trust and articulates his own new idea: that much of what we call trust can be best described as “encapsulated interest.” Research into the roles of trust in society has offered a broad range of often conflicting theories. Some theorists maintain that trust is a social virtue that cannot be reduced to strategic self-interest; others claim that trusting another person is ultimately a rational calculation based on information about that person and his or her incentives and motivations. The author argues that we place our trust in persons whom we believe to have strong reasons to act in our best interests. He claims that we are correct when we assume that the main incentive of those whom we trust is to maintain a relationship with us--whether it be for reasons of economic benefit or for love and friendship. The author articulates his theory by using examples from a broad array of personal and social relationships, paying particular attention to explanations of the development of trusting relationships. He also examines trustworthiness and seeks to understand why people may behave in ways that violate their own self-interest in order to honor commitments they have made to others. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	collection   = {Trust and trustworthiness},
	abstract     = {In this book the author addresses the the standard theories of trust and articulates his own new idea: that much of what we call trust can be best described as "encapsulated interest." Research into the roles of trust in society has offered a broad range of often conflicting theories. Some theorists maintain that trust is a social virtue that cannot be reduced to strategic self-interest; others claim that trusting another person is ultimately a rational calculation based on information about that person and his or her incentives and motivations. The author argues that we place our trust in persons whom we believe to have strong reasons to act in our best interests. He claims that we are correct when we assume that the main incentive of those whom we trust is to maintain a relationship with us--whether it be for reasons of economic benefit or for love and friendship. The author articulates his theory by using examples from a broad array of personal and social relationships, paying particular attention to explanations of the development of trusting relationships. He also examines trustworthiness and seeks to understand why people may behave in ways that violate their own self-interest in order to honor commitments they have made to others. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	keywords     = {Interpersonal Interaction, Social Interaction, Trust (Social Behavior)},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/GGU3Q78H/2002-02461-000.html:text/html}
}
@book{harding2004feminist,
	title        = {The feminist standpoint theory reader: Intellectual and political controversies},
	author       = {Harding, Sandra G},
	year         = 2004,
	publisher    = {Psychology Press}
}
@article{Hayes_2011,
	title        = {The relationship of action research to human-computer interaction},
	author       = {Hayes, Gillian R.},
	year         = 2011,
	month        = aug,
	journal      = {ACM Trans. Comput.-Hum. Interact.},
	volume       = 18,
	number       = 3,
	pages        = {15:1--15:20},
	doi          = {10.1145/1993060.1993065},
	issn         = {1073-0516},
	abstractnote = {Alongside the growing interest within HCI, and arguably computing more generally, in conducting research that has substantial societal benefits, there is a need for new ways to think about and to articulate the challenges of these engaged research projects as well as their results. Action Research (AR) is a class of methods and approaches for conducting democratic and collaborative research with community partners. AR has evolved over the last several decades and offers HCI researchers theoretical lenses, methodological approaches, and pragmatic guidance for conducting socially relevant, collaborative, and engaged research. In this article, I describe the historical context and origins of AR, the scientifically rigorous practice of conducting and evaluating AR projects, and the ways in which AR might meaningfully be applied to HCI research.}
}
@article{highhouse2002assessing,
	title        = {Assessing the candidate as a whole: A historical and critical analysis of individual psychological assessment for personnel decision making},
	author       = {Highhouse, Scott},
	year         = 2002,
	month        = jun,
	journal      = {Personnel Psychology},
	publisher    = {Wiley Online Library},
	volume       = 55,
	number       = 2,
	pages        = {363--396},
	doi          = {10.1111/j.1744-6570.2002.tb00114.x},
	note         = {MAG ID: 2063757465},
	date-added   = {2023-09-11 10:58:50 -0400},
	date-modified = {2023-09-11 10:58:50 -0400},
	abstract     = {Although individual assessment is a thriving area of professional practice in industry, it receives little, if any, attention from textbooks on industrial psychology or personnel management. This article is an attempt to establish individual assessment's place in the history of personnel selection, and to examine why the practice has survived despite receiving little attention in research and graduate training. It is argued that the clinical, holistic approach that has characterized individual-assessment practice has survived primarily because the “elementalistic” testing approach, focusing on traits and abilities, has often been dismissed as inadequate for addressing the complexities of the executive profile. Moreover, public displeasure with standard paper-and-pencil testing in the 1960s and 1970s made the holistic approach to assessment an attractive, alternative. The article contrasts individual assessment practice with the current state of knowledge on psychological assessment and personnel decision making. Like psychotherapy in the 1950s, individual psychological assessment appears to have achieved the status of functional autonomy within psychology.}
}
@article{highhouse2008stubborn,
	title        = {Stubborn reliance on intuition and subjectivity in employee selection},
	author       = {Highhouse, Scott},
	year         = 2008,
	month        = sep,
	journal      = {Industrial and Organisational Psychology},
	publisher    = {Cambridge University Press},
	volume       = 1,
	number       = 3,
	pages        = {333--342},
	doi          = {10.1111/j.1754-9434.2008.00058.x},
	note         = {MAG ID: 2171321543},
	date-added   = {2023-09-11 11:00:23 -0400},
	date-modified = {2023-09-11 11:00:23 -0400},
	abstract     = {The focus of this article is on implicit beliefs that inhibit adoption of selection decision aids (e.g., paper-and-pencil tests, structured interviews, mechanical combination of predictors). Understanding these beliefs is just as important as understanding organisational constraints to the adoption of selection technologies and may be more useful for informing the design of successful interventions. One of these is the implicit belief that it is theoretically possible to achieve near-perfect precision in predicting performance on the job. That is, people have an inherent resistance to analytical approaches to selection because they fail to view selection as probabilistic and subject to error. Another is the implicit belief that prediction of human behavior is improved through experience. This myth of expertise results in an overreliance on intuition and a reluctance to undermine one’s own credibility by using a selection decision aid.}
}
@book{hildebrandt_law_nodate,
	title        = {Law for computer scientists and other folk},
	author       = {Hildebrandt},
	file         = {_.pdf:/Users/neilnatarajan/Zotero/storage/P6LUM9QL/_.pdf:application/pdf}
}
@inproceedings{himmelsbach2019we,
	title        = {Do we care about diversity in human computer interaction: A comprehensive content analysis on diversity dimensions in research},
	author       = {Himmelsbach, Julia and Schwarz, Stephanie and Gerdenitsch, Cornelia and Wais-Zechmann, Beatrix and Bobeth, Jan and Tscheligi, Manfred},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 CHI conference on human factors in computing systems},
	pages        = {1--16}
}
@article{hirschman_dequantifying_2016,
	title        = {Dequantifying diversity: affirmative action and admissions at the {University} of {Michigan}},
	shorttitle   = {Dequantifying diversity},
	author       = {Hirschman, Daniel and Berrey, Ellen and Rose-Greenland, Fiona},
	year         = 2016,
	month        = jun,
	journal      = {Theory and Society},
	volume       = 45,
	number       = 3,
	pages        = {265--301},
	doi          = {10.1007/s11186-016-9270-2},
	issn         = {1573-7853},
	url          = {https://doi.org/10.1007/s11186-016-9270-2},
	urldate      = {2024-07-10},
	abstract     = {To explore the limits of quantification as a form of rationalization, we examine a rare case of dequantification: race-based affirmative action in undergraduate admissions at the University of Michigan. Michigan adopted a policy of holistically reviewing undergraduate applications in 2003, after the US Supreme Court ruled unconstitutional its points-based admissions policy. Using archival and ethnographic data, we trace the adoption, evolution, and undoing of Michigan’s quantified system of admissions decision-making between 1964 and 2004. In a context in which opponents of the system had legal avenues to engage a powerful outside authority, we argue that three internal features of the University’s quantified admissions policy contributed to its demise: its transparency, the instability of the categories it quantified, and the existence of qualitative alternatives. Our analysis challenges the presumed durability and inevitability of quantification by identifying its vulnerabilities and suggests that quantification should be understood as a matter of degree rather than a simple binary.},
	language     = {en},
	keywords     = {\_tablet, Gratz, Grutter, Organisational routines, Quantification, Race, Rationalization},
	file         = {Hirschman et al_2016_Dequantifying diversity.pdf:/Users/neilnatarajan/Zotero/storage/F7KCPUGK/Hirschman et al_2016_Dequantifying diversity.pdf:application/pdf}
}
@misc{hofmann_dialect_2024,
	title        = {Dialect prejudice predicts {AI} decisions about people's character, employability, and criminality},
	author       = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	year         = 2024,
	month        = mar,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2403.00742},
	urldate      = {2024-03-05},
	note         = {arXiv:2403.00742 [cs]},
	abstract     = {Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Q8MVFGGD/2403.html:text/html;Full Text PDF:/Users/neilnatarajan/Zotero/storage/EC4WU2SY/Hofmann et al. - 2024 - Dialect prejudice predicts AI decisions about peop.pdf:application/pdf}
}
@article{horodyski_applicants_2023,
	title        = {Applicants' perception of artificial intelligence in the recruitment process},
	author       = {Horodyski, Piotr},
	year         = 2023,
	month        = aug,
	journal      = {Computers in Human Behavior Reports},
	volume       = 11,
	pages        = 100303,
	doi          = {10.1016/j.chbr.2023.100303},
	issn         = {2451-9588},
	url          = {https://www.sciencedirect.com/science/article/pii/S2451958823000362},
	urldate      = {2023-08-01},
	abstract     = {The proliferation of Artificial intelligence (AI) technologies impacting entire business sectors is also transforming the field of human resources and recruitment. AI-based recruitment tools are changing the way recruitment processes are conducted. However, the perception of AI technology from the candidate's perspective has received limited coverage in the literature. Since little is known about how applicants experience AI-enabled recruitment, this paper explores their experiences and perceptions in hiring processes. The results of this study show that applicants perceive AI technology positively in hiring processes and see it as useful and easy to use. In terms of advantages, reduced response time was recognized as the most significant benefit. The lack of nuance in human judgment, low accuracy and reliability, and immature technology were identified as the biggest drawbacks of AI in recruitment.},
	language     = {en},
	keywords     = {\_tablet, Artificial intelligence, Recruitment, AI, Human resources, Applicants' perception, TAM},
	file         = {Horodyski_2023_Applicants' perception of artificial intelligence in the recruitment process.pdf:/Users/neilnatarajan/Zotero/storage/IX4GAUST/Horodyski_2023_Applicants' perception of artificial intelligence in the recruitment process.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/XSGHI3VG/S2451958823000362.html:text/html}
}
@article{hsieh2019allocation,
	title        = {The allocation of talent and us economic growth},
	author       = {Hsieh, Chang-Tai and Hurst, Erik and Jones, Charles I and Klenow, Peter J},
	year         = 2019,
	journal      = {Econometrica},
	publisher    = {Wiley Online Library},
	volume       = 87,
	number       = 5,
	pages        = {1439--1474},
	date-added   = {2023-09-12 15:16:21 -0400},
	date-modified = {2023-09-12 15:16:21 -0400}
}
@article{hu_challenges_2023,
	title        = {Challenges for enforcing editorial policies on {AI}-generated papers},
	author       = {Hu, Guangwei},
	year         = 2023,
	month        = feb,
	journal      = {Accountability in Research},
	volume       = {0},
	number       = {0},
	pages        = {1--3},
	doi          = {10.1080/08989621.2023.2184262},
	issn         = {0898-9621},
	url          = {https://doi.org/10.1080/08989621.2023.2184262},
	urldate      = {2023-04-06},
	note         = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/08989621.2023.2184262},
	abstract     = {ChatGPT, a chatbot released by OpenAI in November 2022, has rocked academia with its capacity to generate papers “good enough” for academic journals. Major journals such as Nature and professional societies such as the World Association of Medical Editors have moved fast to issue policies to ban or curb AI-written papers. Amid the flurry of policy initiatives, one important challenge seems to be overlooked: AI-generated papers are not easily discernible to the human eye, and we lack the right tools to implement the policies. Without such tools, the well-intentioned policies are likely to remain on paper.},
	pmid         = 36840450,
	keywords     = {ChatGPT; editorial policies; AI-generated papers; challenges},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/46N2DI8B/Hu - 2023 - Challenges for enforcing editorial policies on AI-.pdf:application/pdf}
}
@article{yu_huang_reflection_2023,
	title        = {Reflection on whether {Chat} {GPT} should be banned by academia from the perspective of education and teaching},
	author       = {Huang, Yu},
	year         = 2023,
	month        = jun,
	volume       = 14,
	doi          = {10.3389/fpsyg.2023.1181712},
	note         = {MAG ID: 4379010216},
	abstract     = {OPINION article Front. Psychol., 01 June 2023Sec. Educational Psychology Volume 14 - 2023 {\textbar} https://doi.org/10.3389/fpsyg.2023.1181712},
	pmid         = 37325766,
	keywords     = {\_tablet},
	file         = {Yu Huang_2023_Reflection on whether Chat GPT should be banned by academia from the.pdf:/Users/neilnatarajan/Zotero/storage/QFBWGS9B/Yu Huang_2023_Reflection on whether Chat GPT should be banned by academia from the.pdf:application/pdf}
}
@inproceedings{hupont2021diverse,
	title        = {How diverse is the ACII community? Analysing gender, geographical and business diversity of Affective Computing research},
	author       = {Hupont, Isabelle and Tolan, Song{\"u}l and Freire, Ana and Porcaro, Lorenzo and Estevez, Sara and G{\'o}mez, Emilia},
	year         = 2021,
	booktitle    = {2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)},
	pages        = {1--8},
	organisation = {IEEE}
}
@article{huppenkothen2020entrofy,
	title        = {Entrofy your cohort: A transparent method for diverse cohort selection},
	shorttitle   = {Entrofy your cohort},
	author       = {Huppenkothen, Daniela and McFee, Brian and Nor{\'e}n, Laura},
	year         = 2020,
	month        = jul,
	journal      = {Plos one},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = 15,
	number       = 7,
	pages        = {e0231939},
	doi          = {10.1371/journal.pone.0231939},
	issn         = {1932-6203},
	url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7384611/},
	urldate      = {2022-04-12},
	date-added   = {2023-09-11 11:29:58 -0400},
	date-modified = {2023-09-11 11:29:58 -0400},
	abstract     = {Selecting a cohort from a set of candidates is a common task within and beyond academia. Admitting students, awarding grants, and choosing speakers for a conference are situations where human biases may affect the selection of any particular candidate, and, thereby the composition of the final cohort. In this paper, we propose a new algorithm, entrofy, designed to be part of a human-in-the-loop decision making strategy aimed at making cohort selection as just, transparent, and accountable as possible. We suggest embedding entrofy in a two-step selection procedure. During a merit review, the committee selects all applicants, submissions, or other entities that meet their merit-based criteria. This often yields a cohort larger than the admissible number. In the second stage, the target cohort can be chosen from this meritorious pool via a new algorithm and software tool called entrofy. entrofy optimizes differences across an assignable set of categories selected by the human committee. Criteria could include academic discipline, home country, experience with certain technologies, or other quantifiable characteristics. The entrofy algorithm then yields the approximation of pre-defined target proportions for each category by solving the tie-breaking problem with provable performance guarantees. We show how entrofy selects cohorts according to pre-determined characteristics in simulated sets of applications and demonstrate its use in a case study of Astro Hack Week. This two stage candidate and cohort selection process allows human judgment and debate to guide the assessment of candidates’ merit in step 1. Then the human committee defines relevant diversity criteria which will be used as computational parameters in entrofy. Once the parameters are defined, the set of candidates who meet the minimum threshold for merit are passed through the entrofy cohort selection procedure in step 2 which yields a cohort of a composition as close as possible to the computational parameters defined by the committee. This process has the benefit of separating the meritorious assessment of candidates from certain elements of their diversity and from some considerations around cohort composition. It also increases the transparency and auditability of the process, which enables, but does not guarantee, fairness. Splitting merit and diversity considerations into their own assessment stages makes it easier to explain why a given candidate was selected or rejected, though it does not eliminate the possibility of objectionable bias.},
	pmid         = 32716929,
	pmcid        = {PMC7384611},
	file         = {Full Text:/Users/neilnatarajan/Zotero/storage/8XZ33SGI/Huppenkothen et al. - 2020 - Entrofy your cohort A transparent method for dive.pdf:application/pdf}
}
@inproceedings{hwang2024sound,
	title        = {The Sound of Support: Gendered Voice Agent as Support to Minority Teammates in Gender-Imbalanced Team},
	author       = {Hwang, Angel Hsing-Chi and Won, Andrea Stevenson},
	year         = 2024,
	booktitle    = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	pages        = {1--22}
}
@article{imai2023experimental,
	title        = {Experimental evaluation of algorithm-assisted human decision-making: Application to pretrial public safety assessment},
	author       = {Imai, Kosuke and Jiang, Zhichao and Greiner, D James and Halen, Ryan and Shin, Sooahn},
	year         = 2023,
	journal      = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	publisher    = {Oxford University Press US},
	volume       = 186,
	number       = 2,
	pages        = {167--189}
}
@article{ippolito_human_2019,
	title        = {Human and {Automatic} {Detection} of {Generated} {Text}.},
	author       = {Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
	year         = 2019,
	month        = nov,
	journal      = {arXiv: Computation and Language},
	note         = {MAG ID: 2988675894 S2ID: dee33aa8d7f8ff4458a65379ebce8bd8860d0100},
	abstract     = {With the advent of generative models with a billion parameters or more, it is now possible to automatically generate vast amounts of human-sounding text. This raises questions into just how human-like is the machine-generated text, and how long does a text excerpt need to be for both humans and automatic discriminators to be able reliably detect that it was machine-generated. In this paper, we conduct a thorough investigation of how choices such as sampling strategy and text excerpt length can impact the performance of automatic detection methods as well as human raters. We find that the sampling strategies which result in more human-like text according to human raters create distributional differences from human-written text that make detection easy for automatic discriminators.}
}
@misc{irving_ai_2018,
	title        = {{AI} safety via debate},
	author       = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	year         = 2018,
	month        = oct,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1805.00899},
	url          = {http://arxiv.org/abs/1805.00899},
	urldate      = {2022-08-03},
	note         = {arXiv:1805.00899 [cs, stat]},
	abstract     = {To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/SE6MAM82/Irving et al. - 2018 - AI safety via debate.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/BMFVFFGN/1805.html:text/html}
}
@article{islam_technology_2022,
	title        = {Technology {Adoption} and {Human} {Resource} {Management} {Practices}: {The} {Use} of {Artificial} {Intelligence} for {Recruitment} in {Bangladesh}},
	shorttitle   = {Technology {Adoption} and {Human} {Resource} {Management} {Practices}},
	author       = {Islam, Muhaiminul and Mamun, Abdullah Al and Afrin, Samina and Ali Quaosar, G. M. Azmal and Uddin, Md. Aftab},
	year         = 2022,
	month        = dec,
	journal      = {South Asian Journal of Human Resources Management},
	volume       = 9,
	number       = 2,
	pages        = {324--349},
	doi          = {10.1177/23220937221122329},
	issn         = {2322-0937},
	url          = {https://doi.org/10.1177/23220937221122329},
	urldate      = {2023-08-01},
	note         = {Publisher: SAGE Publications India},
	abstract     = {Artificial intelligence (AI) is now considered indispensable in undertaking operational activities, especially in the area of human resource analytics. However, in practice, the rate of the adoption of such modern algorithms in organisations is still in its early stages. Consequently, the primary objective of this study is to identify the main antecedents of the adoption of AI-based technologies in recruitment, using the lens of the unified theory of acceptance and use of technology (UTAUT) model, alongside perceived credibility and moderating variables, in the context of an emerging nation in South Asia, namely Bangladesh. Data were collected from 283 human resource professionals employed in different manufacturing and service firms in Bangladesh through the administration of a questionnaire, which was analysed by applying PLS-SEM. The outcomes of the study show that all the direct hypothesised relationships were found to be significant, apart from the extended variable of perceived credibility. However, no moderating effect of gender or firm size was found in any of the hypothesised propositions. Finally, policy implications and recommendations for future researchers are proposed.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Islam et al_2022_Technology Adoption and Human Resource Management Practices.pdf:/Users/neilnatarajan/Zotero/storage/L9ZMJLHD/Islam et al_2022_Technology Adoption and Human Resource Management Practices.pdf:application/pdf}
}
@book{jacko_human_2012,
	title        = {Human {Computer} {Interaction} {Handbook}: {Fundamentals}, {Evolving} {Technologies}, and {Emerging} {Applications}, {Third} {Edition}},
	shorttitle   = {Human {Computer} {Interaction} {Handbook}},
	author       = {Jacko, Julie A.},
	year         = 2012,
	publisher    = {Taylor \& Francis Group},
	address      = {Baton Rouge, UNITED STATES},
	isbn         = {978-1-4398-2944-8},
	url          = {http://ebookcentral.proquest.com/lib/oxford/detail.action?docID=911990},
	urldate      = {2023-03-13},
	keywords     = {etc., Human-computer interaction -- Handbooks, manuals},
	file         = {ProQuest Ebook Snapshot:/Users/neilnatarajan/Zotero/storage/FCEZFK2S/reader.html:text/html}
}
@article{jacobs_how_2021,
	title        = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
	shorttitle   = {How machine-learning recommendations influence clinician treatment selections},
	author       = {Jacobs, Maia and Pradier, Melanie F. and McCoy, Thomas H. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
	year         = 2021,
	month        = feb,
	journal      = {Translational Psychiatry},
	volume       = 11,
	number       = 1,
	pages        = {1--9},
	doi          = {10.1038/s41398-021-01224-x},
	issn         = {2158-3188},
	url          = {https://www.nature.com/articles/s41398-021-01224-x},
	urldate      = {2022-01-26},
	copyright    = {2021 The Author(s)},
	note         = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Number: 1 Primary\_atype: Research Publisher: Nature Publishing Group Subject\_term: Depression;Scientific community Subject\_term\_id: depression;scientific-community},
	abstract     = {Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians’ treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning (ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians’ treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.},
	language     = {en},
	keywords     = {Depression, Scientific community, \_tablet, User Study},
	file         = {Jacobs et al_2021_How machine-learning recommendations influence clinician treatment selections.pdf:/Users/neilnatarajan/Zotero/storage/6EBQT5RA/Jacobs et al_2021_How machine-learning recommendations influence clinician treatment selections.pdf:application/pdf;Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf:/Users/neilnatarajan/Zotero/storage/MVJRDDP8/Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/T2M6TSIZ/s41398-021-01224-x.html:text/html}
}
@inproceedings{jacovi_formalizing_2021,
	title        = {Formalizing {Trust} in {Artificial} {Intelligence}: {Prerequisites}, {Causes} and {Goals} of {Human} {Trust} in {AI}},
	shorttitle   = {Formalizing {Trust} in {Artificial} {Intelligence}},
	author       = {Jacovi, Alon and Marasović, Ana and Miller, Tim and Goldberg, Yoav},
	year         = 2021,
	month        = mar,
	booktitle    = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAccT} '21},
	pages        = {624--635},
	doi          = {10.1145/3442188.3445923},
	isbn         = {978-1-4503-8309-7},
	url          = {https://doi.org/10.1145/3442188.3445923},
	urldate      = {2021-10-15},
	abstract     = {Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.},
	keywords     = {artificial intelligence, contractual trust, distrust, formalization, sociology, trust, trustworthy, warranted trust},
	file         = {Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/R729TD7X/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf;Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/IGT2LYYQ/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf}
}
@book{jaggar1983feminist,
	title        = {Feminist politics and human nature},
	author       = {Jaggar, Alison M},
	year         = 1983,
	publisher    = {Rowman \& Littlefield}
}
@article{jarrahi_artificial_2018,
	title        = {Artificial intelligence and the future of work: {Human}-{AI} symbiosis in organisational decision making},
	shorttitle   = {Artificial intelligence and the future of work},
	author       = {Jarrahi, Mohammad Hossein},
	year         = 2018,
	month        = jul,
	journal      = {Business Horizons},
	volume       = 61,
	number       = 4,
	pages        = {577--586},
	doi          = {10.1016/j.bushor.2018.03.007},
	issn         = {0007-6813},
	url          = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
	urldate      = {2023-08-01},
	abstract     = {Artificial intelligence (AI) has penetrated many organisational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organisational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organisational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.},
	language     = {en},
	keywords     = {\_tablet, Artificial intelligence, Analytical and intuitive decision making, Human augmentation, Human-machine symbiosis, Organisational decision making},
	file         = {Jarrahi_2018_Artificial intelligence and the future of work.pdf:/Users/neilnatarajan/Zotero/storage/YVCFE9FC/Jarrahi_2018_Artificial intelligence and the future of work.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/DUANG6K9/S0007681318300387.html:text/html}
}
@misc{javed_svarah_2023,
	title        = {Svarah: {Evaluating} {English} {ASR} {Systems} on {Indian} {Accents}},
	shorttitle   = {Svarah},
	author       = {Javed, Tahir and Joshi, Sakshi and Nagarajan, Vignesh and Sundaresan, Sai and Nawale, Janki and Raman, Abhigyan and Bhogale, Kaushal and Kumar, Pratyush and Khapra, Mitesh M.},
	year         = 2023,
	month        = may,
	journal      = {arXiv.org},
	url          = {https://arxiv.org/abs/2305.15760v1},
	urldate      = {2024-01-12},
	abstract     = {India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents. Svarah as well as all our code will be publicly available.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/74V763ZP/Javed et al. - 2023 - Svarah Evaluating English ASR Systems on Indian A.pdf:application/pdf}
}
@article{jones_renewable_2021,
	title        = {Renewable electricity deals investigated by {UK} government},
	author       = {Jones, Rupert},
	year         = 2021,
	month        = aug,
	journal      = {The Guardian},
	issn         = {0261-3077},
	url          = {https://www.theguardian.com/business/2021/aug/15/renewable-electricity-deals-investigated-by-uk-government},
	urldate      = {2021-10-15},
	chapter      = {Business},
	abstract     = {Plan is to tighten rules to stop energy firms exaggerating environmental benefits of green tariffs},
	language     = {en-GB},
	keywords     = {Environment, Business, Consumer affairs, Energy, Energy industry, Money, Renewable energy, UK news},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/2QHG7DXF/renewable-electricity-deals-investigated-by-uk-government.html:text/html}
}
@article{CHICAOLMO2020102560,
	title        = {Assessing Colombia's policy of socio-economic stratification: An intra-city study of self-reported quality of life},
	author       = {Jorge Chica-Olmo and Angeles Sánchez and Fabio H. Sepúlveda-Murillo},
	year         = 2020,
	journal      = {Cities},
	volume       = 97,
	pages        = 102560,
	doi          = {https://doi.org/10.1016/j.cities.2019.102560},
	issn         = {0264-2751},
	url          = {https://www.sciencedirect.com/science/article/pii/S0264275119312995},
	keywords     = {Public policies, Self-reported quality of life, Economic resources, Social resources, Multilevel models, Medellin},
	abstract     = {This paper aims to analyse whether the Colombian government's classification of households into socio-economic strata accurately reflects the level of quality of life of households in Medellin. To this end, logit multilevel models and a novel graphic analysis were used. Our findings suggest that the stratification criteria currently used in Medellin should be revised. The socio-economic strata, which only considers the characteristics of the dwellings, streets and surrounding areas, explain approximately one tenth of the variability in self-reported quality of life. When other economic and non-economic factors are considered, a different strata ranking for guiding public policies than that used in Medellin was estimated. Neighbourhood safety and personal safety are key drivers of quality of life, whereas higher consumption expenditure is most effective in the most disadvantaged strata.}
}
@book{kahneman2011thinking,
	title        = {Thinking, fast and slow},
	author       = {Kahneman, Daniel},
	year         = 2011,
	month        = jan,
	publisher    = {macmillan},
	note         = {MAG ID: 2752099845},
	date-added   = {2023-09-11 11:01:04 -0400},
	date-modified = {2023-09-11 11:01:04 -0400},
	abstract     = {Daniel Kahneman, recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology challenging the rational model of judgment and decision making, is one of the world's most important thinkers. His ideas have had a profound impact on many fields - including business, medicine, and politics - but until now, he has never brought together his many years of research in one book. In "Thinking, Fast and Slow", Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think and make choices. One system is fast, intuitive, and emotional; the other is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities - and also the faults and biases - of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behaviour. The importance of properly framing risks, the effects of cognitive biases on how we view others, the dangers of prediction, the right ways to develop skills, the pros and cons of fear and optimism, the difference between our experience and memory of events, the real components of happiness - each of these can be understood only by knowing how the two systems work together to shape our judgments and decisions. Drawing on a lifetime's experimental experience, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our professional and our personal lives-and how we can use different techniques to guard against the mental glitches that often get us into trouble. "Thinking, Fast and Slow" will transform the way you take decisions and experience the world.}
}
@inproceedings{karimi_algorithmic_2021,
	title        = {Algorithmic {Recourse}: from {Counterfactual} {Explanations} to {Interventions}},
	shorttitle   = {Algorithmic {Recourse}},
	author       = {Karimi, Amir-Hossein and Schölkopf, Bernhard and Valera, Isabel},
	year         = 2021,
	month        = mar,
	booktitle    = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAccT} '21},
	pages        = {353--362},
	doi          = {10.1145/3442188.3445899},
	isbn         = {978-1-4503-8309-7},
	url          = {https://doi.org/10.1145/3442188.3445899},
	urldate      = {2021-10-15},
	abstract     = {As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.},
	keywords     = {algorithmic recourse, causal inference, consequential recommendations, contrastive explanations, counterfactual explanations, explainable artificial intelligence, minimal interventions},
	file         = {Karimi et al_2021_Algorithmic Recourse.pdf:/Users/neilnatarajan/Zotero/storage/CQFZDLXK/Karimi et al_2021_Algorithmic Recourse.pdf:application/pdf},
	abstractnote = {As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman’s terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.},
	collection   = {FAccT ’21}
}
@techreport{karlan_hoping_2012,
	title        = {Hoping to {Win}, {Expected} to {Lose}: {Theory} and {Lessons} on {Microenterprise} {Development}},
	shorttitle   = {Hoping to {Win}, {Expected} to {Lose}},
	author       = {Karlan, Dean S. and Knight, Ryan and Udry, Christopher},
	year         = 2012,
	month        = nov,
	address      = {Rochester, NY},
	number       = {ID 2226588},
	doi          = {10.2139/ssrn.2226588},
	url          = {https://papers.ssrn.com/abstract=2226588},
	urldate      = {2021-10-28},
	type         = {{SSRN} {Scholarly} {Paper}},
	abstract     = {We show how financial and managerial constraints impede experimentation and thus limit learning about the profitability of investments. Imperfect information about one’s own type, but willingness to experiment to learn one’s type, leads to short-run negative expected returns to investments, with some outliers succeeding. We find in an experiment that entrepreneurs invest randomized grants of cash and adopt advice from randomized grants of consulting services, but both lead to lower profits on average. In the long run, they revert back to their prior scale of operations. In a meta-analysis, results from 19 other experiments find mixed support for this theory.},
	language     = {en},
	institution  = {Social Science Research Network},
	keywords     = {business training, consulting, credit constraints, entrepreneurship, managerial capital},
	file         = {Karlan et al_2012_Hoping to Win, Expected to Lose.pdf:/Users/neilnatarajan/Zotero/storage/JNU35XYZ/Karlan et al_2012_Hoping to Win, Expected to Lose.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/IZSAG65G/papers.html:text/html}
}
@article{kaur_where_nodate,
	title        = {Where are the {Humans} in {Human}-{AI} {Interaction}: {The} {Missing} {Human}-{Centered} {Perspective} on {Interpretability} {Tools} for {Machine} {Learning}},
	author       = {Kaur, Harmanpreet},
	file         = {Kaur_Where are the Humans in Human-AI Interaction_annotated.pdf:/Users/neilnatarajan/Zotero/storage/PR9FX5LC/Kaur_Where are the Humans in Human-AI Interaction_annotated.pdf:application/pdf;Kaur_Where are the Humans in Human-AI Interaction.pdf:/Users/neilnatarajan/Zotero/storage/PR9FX5LC/Kaur_Where are the Humans in Human-AI Interaction.pdf:application/pdf}
}
@article{kim_relation_2018,
	title        = {Relation between attitudinal trust and behavioral trust: {An} exploratory study},
	shorttitle   = {Relation between attitudinal trust and behavioral trust},
	author       = {Kim, Bora},
	year         = 2018,
	month        = sep,
	journal      = {Current Research in Social Psychology},
	volume       = 26,
	pages        = {39--54},
	abstract     = {Previous studies have reported the low predictability of attitudinal trust measures for behavioral trust outcomes. This study argues that there has been a mismatch of the trust construct by using social trust attitude measures to predict materialistic trust behavioral outcomes. Through exploratory pilot experiments, distributional preference measures were found to be related to the materialistic trust decision in the game but not to the social trust decision in the scenario. These findings can shed light on the validity issue of trust measures in future research. https://uiowa.edu/crisp/prior-issues-covered},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/PE9Q9N3V/Kim - 2018 - Relation between attitudinal trust and behavioral .pdf:application/pdf},
	abstractnote = {Previous studies have reported the low predictability of attitudinal trust measures for behavioral trust outcomes. This study argues that there has been a mismatch of the trust construct by using social trust attitude measures to predict materialistic trust behavioral outcomes. Through exploratory pilot experiments, distributional preference measures were found to be related to the materialistic trust decision in the game but not to the social trust decision in the scenario. These findings can shed light on the validity issue of trust measures in future research. https://uiowa.edu/crisp/prior-issues-covered}
}
@article{king_automation_2009,
	title        = {The {Automation} of {Science}},
	author       = {King, Ross D. and Rowland, Jem and Oliver, Stephen G. and Young, Michael and Aubrey, Wayne and Byrne, Emma and Liakata, Maria and Markham, Magdalena and Pir, Pinar and Soldatova, Larisa N. and Sparkes, Andrew and Whelan, Kenneth E. and Clare, Amanda},
	year         = 2009,
	month        = apr,
	journal      = {Science},
	volume       = 324,
	number       = 5923,
	pages        = {85--89},
	doi          = {10.1126/science.1165620},
	url          = {https://www.science.org/doi/10.1126/science.1165620},
	urldate      = {2021-10-15},
	note         = {Publisher: American Association for the Advancement of Science},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/UZU2KRQS/King et al. - 2009 - The Automation of Science.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/IFI7FHCS/King et al. - 2009 - The Automation of Science.pdf:application/pdf}
}
@article{king_functional_2004,
	title        = {Functional genomic hypothesis generation and experimentation by a robot scientist},
	author       = {King, Ross D. and Whelan, Kenneth E. and Jones, Ffion M. and Reiser, Philip G. K. and Bryant, Christopher H. and Muggleton, Stephen H. and Kell, Douglas B. and Oliver, Stephen G.},
	year         = 2004,
	month        = jan,
	journal      = {Nature},
	volume       = 427,
	number       = 6971,
	pages        = {247--252},
	doi          = {10.1038/nature02236},
	issn         = {1476-4687},
	url          = {https://www.nature.com/articles/nature02236},
	urldate      = {2021-10-15},
	copyright    = {2004 Macmillan Magazines Ltd.},
	note         = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Number: 6971 Primary\_atype: Research Publisher: Nature Publishing Group},
	abstract     = {The question of whether it is possible to automate the scientific process is of both great theoretical interest1,2 and increasing practical importance because, in many scientific areas, data are being generated much faster than they can be effectively analysed. We describe a physically implemented robotic system that applies techniques from artificial intelligence3,4,5,6,7,8 to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. Here we apply the system to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments9. We built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway. In biological experiments that automatically reconstruct parts of this model, we show that an intelligent experiment selection strategy is competitive with human performance and significantly outperforms, with a cost decrease of 3-fold and 100-fold (respectively), both cheapest and random-experiment selection.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/6VNL26IW/King et al. - 2004 - Functional genomic hypothesis generation and exper.pdf:application/pdf;King et al. (2004) Supplementary Material.pdf:/Users/neilnatarajan/Zotero/storage/DDXG2PDV/King et al. (2004) Supplementary Material.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/UUNNIQMK/King et al. - 2004 - Functional genomic hypothesis generation and exper.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/4D2NUMUZ/nature02236.html:text/html}
}
@misc{kirchner_new_2023,
	title        = {New {AI} classifier for indicating {AI}-written text},
	author       = {Kirchner, Jan Hendrik and Ahmad, Lama and Aaronson, Scott and Leike, Jan},
	year         = 2023,
	month        = jan,
	url          = {https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text},
	urldate      = {2023-08-31},
	abstract     = {We’re launching a classifier trained to distinguish between AI-written and human-written text.},
	language     = {en-US},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/Q3MPEXFC/new-ai-classifier-for-indicating-ai-written-text.html:text/html}
}
@article{kleinberg2018human,
	title        = {Human decisions and machine predictions},
	author       = {Kleinberg, Jon and Lakkaraju, Himabindu and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
	year         = 2018,
	journal      = {The quarterly journal of economics},
	publisher    = {Oxford University Press},
	volume       = 133,
	number       = 1,
	pages        = {237--293}
}
@article{kleinberg2015prediction,
	title        = {Prediction policy problems},
	author       = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad},
	year         = 2015,
	journal      = {American Economic Review},
	publisher    = {American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203},
	volume       = 105,
	number       = 5,
	pages        = {491--495}
}
@inproceedings{kleinberg2018algorithmic,
	title        = {Algorithmic fairness},
	author       = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Rambachan, Ashesh},
	year         = 2018,
	booktitle    = {Aea papers and proceedings},
	volume       = 108,
	pages        = {22--27},
	date-added   = {2023-09-12 19:09:21 -0400},
	date-modified = {2023-09-12 19:09:21 -0400},
	organisation = {American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}
@book{Knapp_Zeratzky_Kowitz_2016,
	title        = {Sprint: How to solve big problems and test new ideas in just five days.},
	author       = {Knapp, Jake and Zeratzky, John and Kowitz, Braden},
	year         = 2016,
	month        = mar,
	publisher    = {Simon and Schuster}
}
@article{kobis_artificial_2021,
	title        = {Artificial intelligence versus {Maya} {Angelou}: {Experimental} evidence that people cannot differentiate {AI}-generated from human-written poetry},
	shorttitle   = {Artificial intelligence versus {Maya} {Angelou}},
	author       = {Köbis, Nils and Mossink, Luca D.},
	year         = 2021,
	month        = jan,
	journal      = {Computers in Human Behavior},
	volume       = 114,
	pages        = 106553,
	doi          = {10.1016/j.chb.2020.106553},
	issn         = {0747-5632},
	url          = {https://www.sciencedirect.com/science/article/pii/S0747563220303034},
	urldate      = {2023-04-06},
	abstract     = {The release of openly available, robust natural language generation algorithms (NLG) has spurred much public attention and debate. One reason lies in the algorithms' purported ability to generate humanlike text across various domains. Empirical evidence using incentivized tasks to assess whether people (a) can distinguish and (b) prefer algorithm-generated versus human-written text is lacking. We conducted two experiments assessing behavioral reactions to the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal = 830). Using the identical starting lines of human poems, GPT-2 produced samples of poems. From these samples, either a random poem was chosen (Human-out-of-theloop) or the best one was selected (Human-in-the-loop) and in turn matched with a human-written poem. In a new incentivized version of the Turing Test, participants failed to reliably detect the algorithmicallygenerated poems in the Human-in-the-loop treatment, yet succeeded in the Human-out-of-the-loop treatment. Further, people reveal a slight aversion to algorithm-generated poetry, independent on whether participants were informed about the algorithmic origin of the poem (Transparency) or not (Opacity). We discuss what these results convey about the performance of NLG algorithms to produce human-like text and propose methodologies to study such learning algorithms in human-agent experimental settings.},
	language     = {en},
	keywords     = {Computational creativity, Creativity, Machine behavior, Natural language generation, Test, Turing},
	file         = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/ZUVRGDDI/Köbis and Mossink - 2021 - Artificial intelligence versus Maya Angelou Exper.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ALUIBRXG/S0747563220303034.html:text/html}
}
@inproceedings{kohavi_scaling_1996,
	title        = {Scaling up the accuracy of {Naive}-{Bayes} classifiers: a decision-tree hybrid},
	shorttitle   = {Scaling up the accuracy of {Naive}-{Bayes} classifiers},
	author       = {Kohavi, Ron},
	year         = 1996,
	month        = aug,
	booktitle    = {Proceedings of the {Second} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher    = {AAAI Press},
	address      = {Portland, Oregon},
	series       = {{KDD}'96},
	pages        = {202--207},
	urldate      = {2022-09-12},
	abstract     = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested.},
	abstractnote = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested.},
	collection   = {KDD’96}
}
@article{koivunen_pitfalls_2023,
	title        = {Pitfalls and {Tensions} in {Digitalizing} {Talent} {Acquisition}: {An} {Analysis} of {HRM} {Professionals}’ {Considerations} {Related} to {Digital} {Ethics}},
	shorttitle   = {Pitfalls and {Tensions} in {Digitalizing} {Talent} {Acquisition}},
	author       = {Koivunen, Sami and Sahlgren, Otto and Ala-Luopa, Saara and Olsson, Thomas},
	year         = 2023,
	month        = mar,
	journal      = {Interacting with Computers},
	pages        = {iwad018},
	doi          = {10.1093/iwc/iwad018},
	issn         = {1873-7951},
	url          = {https://doi.org/10.1093/iwc/iwad018},
	urldate      = {2023-03-18},
	abstract     = {The practices of organisational talent acquisition are rapidly transforming as a result of the proliferation of information systems that support decision-making, ranging from applicant tracking systems to recruitment chatbots. As part of human resource management (HRM), talent acquisition covers recruitment and team-assembly activities and is allegedly in dire need for digital aid. We analyze the pitfalls and tensions of digitalization in this area through a lens that builds on the interdisciplinary literature related to digital ethics. Using three relevant landmark papers, we analyzed qualitative data from 47 interviews of HRM professionals in Finland, including team-assembly facilitators and recruitment experts. The analysis highlights 14 potential tensions and pitfalls, such as the tension between requesting detailed data versus respecting privacy and the pitfall of unequal treatment across application channels. We identify that the values of autonomy, fairness and utility are often especially at risk of being compromised. We discuss the tendency of the binary considerations related to human and automated decision making, and the reasons for the incompatibility between current digital systems and organisations’ needs for talent acquisition.},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/KYB6DGCA/Koivunen et al. - 2023 - Pitfalls and Tensions in Digitalizing Talent Acqui.pdf:application/pdf}
}
@article{korb_introduction_2004,
	title        = {Introduction: {Machine} {Learning} as {Philosophy} of {Science}},
	shorttitle   = {Introduction},
	author       = {Korb, Kevin B.},
	year         = 2004,
	month        = nov,
	journal      = {Minds and Machines},
	volume       = 14,
	number       = 4,
	pages        = {433--440},
	doi          = {10.1023/B:MIND.0000045986.90956.7f},
	issn         = {1572-8641},
	url          = {https://doi.org/10.1023/B:MIND.0000045986.90956.7f},
	urldate      = {2021-10-15},
	abstract     = {I consider three aspects in which machine learning and philosophy of science can illuminate each other: methodology, inductive simplicity and theoretical terms. I examine the relations between the two subjects and conclude by claiming these relations to be very close.},
	language     = {en},
	file         = {Korb_2004_Introduction_annotated.pdf:/Users/neilnatarajan/Zotero/storage/E7RF8LYZ/Korb_2004_Introduction_annotated.pdf:application/pdf;Korb_2004_Introduction.pdf:/Users/neilnatarajan/Zotero/storage/E7RF8LYZ/Korb_2004_Introduction.pdf:application/pdf}
}
@article{krause2014submodular,
	title        = {Submodular function maximization.},
	author       = {Krause, Andreas and Golovin, Daniel},
	year         = 2014,
	journal      = {Tractability},
	volume       = 3,
	number       = {71-104},
	pages        = 3,
	date-added   = {2023-09-11 10:52:21 -0400},
	date-modified = {2023-09-11 10:52:21 -0400}
}
@incollection{bordeaux_submodular_2014,
	title        = {Submodular {Function} {Maximization}},
	author       = {Krause, Andreas and Golovin, Daniel},
	year         = 2014,
	month        = feb,
	booktitle    = {Tractability},
	publisher    = {Cambridge University Press},
	pages        = {71--104},
	doi          = {10.1017/CBO9781139177801.004},
	url          = {https://www.cambridge.org/core/product/identifier/CBO9781139177801A031/type/book_part},
	urldate      = {2023-02-15},
	edition      = 1,
	language     = {en},
	editor       = {Bordeaux, Lucas and Hamadi, Youssef and Kohli, Pushmeet}
}
@article{krishna_disagreement_2022,
	title        = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}: {A} {Practitioner}'s {Perspective}},
	shorttitle   = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}},
	author       = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	year         = 2022,
	month        = feb,
	url          = {https://arxiv.org/abs/2202.01602v2},
	urldate      = {2022-02-08},
	abstract     = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
	language     = {en},
	file         = {Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/UUVTJGUF/Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:application/pdf;Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/UUVTJGUF/false:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/WP7DRH5K/2202.html:text/html}
}
@article{kruger_problem_2022,
	title        = {The problem with trust: on the discursive commodification of trust in {AI}},
	shorttitle   = {The problem with trust},
	author       = {Krüger, Steffen and Wilson, Christopher},
	year         = 2022,
	month        = feb,
	journal      = {AI \& SOCIETY},
	doi          = {10.1007/s00146-022-01401-6},
	issn         = {1435-5655},
	url          = {https://doi.org/10.1007/s00146-022-01401-6},
	urldate      = {2022-03-01},
	abstract     = {This commentary draws critical attention to the ongoing commodification of trust in policy and scholarly discourses of artificial intelligence (AI) and society. Based on an assessment of publications discussing the implementation of AI in governmental and private services, our findings indicate that this discursive trend towards commodification is driven by the need for a trusting population of service users to harvest data at scale and leads to the discursive construction of trust as an essential good on a par with data as raw material. This discursive commodification is marked by a decreasing emphasis on trust understood as the expected reliability of a trusted agent, and increased emphasis on instrumental and extractive framings of trust as a resource. This tendency, we argue, does an ultimate disservice to developers, users, and systems alike, insofar as it obscures the subtle mechanisms through which trust in AI systems might be built, making it less likely that it will be.},
	language     = {en},
	file         = {Springer Full Text PDF:/Users/neilnatarajan/Zotero/storage/MT7KIMAP/Krüger and Wilson - 2022 - The problem with trust on the discursive commodif.pdf:application/pdf}
}
@article{kumar_problems_2020,
	title        = {Problems with {Shapley}-value-based explanations as feature importance measures},
	author       = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
	year         = 2020,
	month        = jun,
	journal      = {arXiv:2002.11097 [cs, stat]},
	url          = {http://arxiv.org/abs/2002.11097},
	urldate      = {2022-01-05},
	note         = {arXiv: 2002.11097},
	abstract     = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/HYXCE9H2/2002.html:text/html;Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:/Users/neilnatarajan/Zotero/storage/I4WGRVEL/Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:application/pdf}
}
@article{kuncel2013mechanical,
	title        = {Mechanical versus clinical data combination in selection and admissions decisions: a meta-analysis.},
	author       = {Kuncel, Nathan R and Klieger, David M and Connelly, Brian S and Ones, Deniz S},
	year         = 2013,
	month        = sep,
	journal      = {Journal of applied psychology},
	publisher    = {American Psychological Association},
	volume       = 98,
	number       = 6,
	pages        = 1060,
	doi          = {10.1037/a0034156},
	note         = {MAG ID: 1989277811},
	date-added   = {2023-09-11 11:16:23 -0400},
	date-modified = {2023-09-11 11:16:23 -0400},
	abstract     = {In employee selection and academic admission decisions, holistic (clinical) data combination methods continue to be relied upon and preferred by practitioners in our field. This meta-analysis examined and compared the relative predictive power of mechanical methods versus holistic methods in predicting multiple work (advancement, supervisory ratings of performance, and training performance) and academic (grade point average) criteria. There was consistent and substantial loss of validity when data were combined holistically--even by experts who are knowledgeable about the jobs and organisations in question--across multiple criteria in work and academic settings. In predicting job performance, the difference between the validity of mechanical and holistic data combination methods translated into an improvement in prediction of more than 50\%. Implications for evidence-based practice are discussed.},
	pmid         = 24041118
}
@inproceedings{lai_human_2019,
	title        = {On {Human} {Predictions} with {Explanations} and {Predictions} of {Machine} {Learning} {Models}: {A} {Case} {Study} on {Deception} {Detection}},
	shorttitle   = {On {Human} {Predictions} with {Explanations} and {Predictions} of {Machine} {Learning} {Models}},
	author       = {Lai, Vivian and Tan, Chenhao},
	year         = 2019,
	month        = jan,
	booktitle    = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAT}* '19},
	pages        = {29--38},
	doi          = {10.1145/3287560.3287590},
	isbn         = {978-1-4503-6125-5},
	url          = {https://doi.org/10.1145/3287560.3287590},
	urldate      = {2022-01-26},
	abstract     = {Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels ({\textgreater}20\% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.},
	keywords     = {explanations, \_tablet, human agency, human performance, predictions},
	file         = {Lai_Tan_2019_On Human Predictions with Explanations and Predictions of Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/9UAJYVZJ/Lai_Tan_2019_On Human Predictions with Explanations and Predictions of Machine Learning.pdf:application/pdf}
}
@article{lambrecht2019algorithmic,
	title        = {Algorithmic bias? An empirical study of apparent gender-based discrimination in the display of STEM career ads},
	author       = {Lambrecht, Anja and Tucker, Catherine},
	year         = 2019,
	journal      = {Management science},
	publisher    = {INFORMS},
	volume       = 65,
	number       = 7,
	pages        = {2966--2981}
}
@article{lannelongue_green_2021,
	title        = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},
	shorttitle   = {Green {Algorithms}},
	author       = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
	year         = 2021,
	journal      = {Advanced Science},
	volume       = 8,
	number       = 12,
	pages        = 2100707,
	doi          = {10.1002/advs.202100707},
	issn         = {2198-3844},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202100707},
	urldate      = {2022-01-04},
	note         = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202100707},
	abstract     = {Climate change is profoundly affecting nearly all aspects of life on earth, including human societies, economies, and health. Various human activities are responsible for significant greenhouse gas (GHG) emissions, including data centers and other sources of large-scale computation. Although many important scientific milestones are achieved thanks to the development of high-performance computing, the resultant environmental impact is underappreciated. In this work, a methodological framework to estimate the carbon footprint of any computational task in a standardized and reliable way is presented and metrics to contextualize GHG emissions are defined. A freely available online tool, Green Algorithms (www.green-algorithms.org) is developed, which enables a user to estimate and report the carbon footprint of their computation. The tool easily integrates with computational processes as it requires minimal information and does not interfere with existing code, while also accounting for a broad range of hardware configurations. Finally, the GHG emissions of algorithms used for particle physics simulations, weather forecasts, and natural language processing are quantified. Taken together, this study develops a simple generalizable framework and freely available tool to quantify the carbon footprint of nearly any computation. Combined with recommendations to minimize unnecessary CO2 emissions, the authors hope to raise awareness and facilitate greener computation.},
	language     = {en},
	keywords     = {climate change, computational research, green computing},
	file         = {Lannelongue et al_2021_Green Algorithms.pdf:/Users/neilnatarajan/Zotero/storage/NALV4V6I/Lannelongue et al_2021_Green Algorithms.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/VRHJZI85/advs.html:text/html}
}
@misc{mattu_how_nodate,
	title        = {How {We} {Analyzed} the {COMPAS} {Recidivism} {Algorithm}},
	author       = {Larson, Jeff and Mattu, Surya and Kirchner, Lauren and Angwin, Julia},
	journal      = {ProPublica},
	url          = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
	urldate      = {2022-11-09},
	abstract     = {ProPublica is an independent, non-profit newsroom that produces investigative journalism in the public interest.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/5U5BTTY7/how-we-analyzed-the-compas-recidivism-algorithm.html:text/html}
}
@inproceedings{Lashkari_Cheng_2023,
	title        = {“Finding the Magic Sauce”: Exploring Perspectives of Recruiters and Job Seekers on Recruitment Bias and Automated Tools},
	author       = {Lashkari, Mitra and Cheng, Jinghui},
	year         = 2023,
	month        = apr,
	booktitle    = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI ’23},
	pages        = {1–16},
	doi          = {10.1145/3544548.3581548},
	isbn         = {978-1-4503-9421-5},
	url          = {https://dl.acm.org/doi/10.1145/3544548.3581548},
	abstractnote = {Automated recruitment tools are proliferating. While having the promise of improving efficiency, various risks, including bias, challenges the potential of these tools. An in-depth understanding of the perceived risk factors and needs from the perspective of both recruiters and job seekers is needed. We address this through an interview study in the high-tech industry to compare and contrast the concerns of these two roles. We found that the importance of clarifying position requirements and assessing candidates as “whole individuals” are commonly discussed by both recruiters and job seekers. In contrast, while recruiters tended to be more aware of cognitive bias and desired more tool support during interviews, job seekers voiced more desire towards a healthy candidate-company relationship. Additionally, both roles considered the uncertainty of the current technology capability and reduced human contact as concerns for using automated tools. Based on these results, we provided design implications for automated recruitment tools and related decision-support technologies.},
	collection   = {CHI ’23}
}
@article{laville_ai_2021,
	title        = {{AI} reveals 1,000 'dark discharges' of untreated sewage in {England}},
	author       = {Laville, Sandra},
	year         = 2021,
	month        = mar,
	journal      = {The Guardian},
	issn         = {0261-3077},
	url          = {https://www.theguardian.com/environment/2021/mar/12/ai-reveals-1000-dark-discharges-of-untreated-sewage-in-england},
	urldate      = {2021-10-15},
	chapter      = {Environment},
	abstract     = {Paper says machine learning could prove crucial tool in efforts to improve quality of country’s rivers},
	language     = {en-GB},
	keywords     = {Environment, UK news, Artificial intelligence (AI), Pollution, Rivers, Technology},
	file         = {Laville - 2021 - AI reveals 1,000 'dark discharges' of untreated se.pdf:/Users/neilnatarajan/Zotero/storage/VMGW3S4J/Laville - 2021 - AI reveals 1,000 'dark discharges' of untreated se.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/VPFLRMAS/ai-reveals-1000-dark-discharges-of-untreated-sewage-in-england.html:text/html}
}
@incollection{lazar_chapter_2017,
	title        = {Chapter 9 - {Ethnography}},
	author       = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	year         = 2017,
	month        = jan,
	booktitle    = {Research {Methods} in {Human} {Computer} {Interaction} ({Second} {Edition})},
	publisher    = {Morgan Kaufmann},
	address      = {Boston},
	pages        = {229--261},
	doi          = {10.1016/B978-0-12-805390-4.00009-1},
	isbn         = {978-0-12-805390-4},
	url          = {https://www.sciencedirect.com/science/article/pii/B9780128053904000091},
	urldate      = {2023-03-13},
	abstract     = {In-depth, longitudinal observation of complex contexts is often necessary for understanding systems requirements or the impact of systems in use. Ethnography refers to the use of in-depth observation, and often participation, of a human group, culture, or context, with the goal of developing a rich description of activities, interactions, beliefs, roles, and goals. Ethnographic research in human-computer interaction (HCI) is particularly useful for understanding environment where stakeholders interact to complete complex tasks involving the need for coordination and exchange of information. This chapter discusses the selection of sites, identification of research roles, practical issues for ethnographic studies, and processes involved in collecting and analyzing ethnographic data. Examples of ethnographic HCI studies in home, work, education, and other settings are provided.},
	language     = {en},
	editor       = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	keywords     = {Ethnography, Informants, Observation, Participation, Triangulation},
	file         = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/B88RKS6Z/Lazar et al. - 2017 - Chapter 9 - Ethnography.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ECYJCGI4/research-methods-in-human-computer-interaction.html:text/html}
}
@article{lee_trust_2004,
	title        = {Trust in {Automation}: {Designing} for {Appropriate} {Reliance}},
	shorttitle   = {Trust in {Automation}},
	author       = {Lee, John D. and See, Katrina A.},
	year         = 2004,
	month        = mar,
	journal      = {Human Factors},
	publisher    = {SAGE Publications Inc},
	volume       = 46,
	number       = 1,
	pages        = {50--80},
	doi          = {10.1518/hfes.46.1.50_30392},
	issn         = {0018-7208},
	url          = {https://journals.sagepub.com/doi/abs/10.1518/hfes.46.1.50_30392},
	urldate      = {2022-09-16},
	note         = {Publisher: SAGE Publications Inc},
	abstract     = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organisational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
	file         = {SAGE PDF Full Text:/Users/neilnatarajan/Zotero/storage/3BIIJWJQ/Lee and See - 2004 - Trust in Automation Designing for Appropriate Rel.pdf:application/pdf},
	abstractnote = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organisational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.}
}
@misc{leike_scalable_2018,
	title        = {Scalable agent alignment via reward modeling: a research direction},
	shorttitle   = {Scalable agent alignment via reward modeling},
	author       = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	year         = 2018,
	month        = nov,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1811.07871},
	url          = {http://arxiv.org/abs/1811.07871},
	urldate      = {2022-08-03},
	note         = {arXiv:1811.07871 [cs, stat]},
	abstract     = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/NXSBF35A/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/N3AD4J7F/1811.html:text/html}
}
@book{lentin_Multiculturalism_2011,
	title        = {The Crises of Multiculturalism: Racism in a Neoliberal age},
	author       = {Lentin, Alana and Titley, Gavan},
	year         = 2011,
	month        = {01},
	doi          = {10.5040/9781350223035},
	isbn         = {978-1-84813-580-2}
}
@inproceedings{Leung_Zhang_Jibuti_Zhao_Klein_Pierce_Robert_Zhu_2020,
	title        = {Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases},
	author       = {Leung, Weiwen and Zhang, Zheng and Jibuti, Daviti and Zhao, Jinhao and Klein, Maximilian and Pierce, Casey and Robert, Lionel and Zhu, Haiyi},
	year         = 2020,
	month        = apr,
	booktitle    = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI ’20},
	pages        = {1–11},
	doi          = {10.1145/3313831.3376874},
	isbn         = {978-1-4503-6708-0},
	url          = {https://dl.acm.org/doi/10.1145/3313831.3376874},
	abstractnote = {We conduct a study of hiring bias on a simulation platform where we ask Amazon MTurk participants to make hiring decisions for a mathematically intensive task. Our findings suggest hiring biases against Black workers and less attractive workers, and preferences towards Asian workers, female workers and more attractive workers. We also show that certain UI designs, including provision of candidates’ information at the individual level and reducing the number of choices, can significantly reduce discrimination. However, provision of candidate’s information at the subgroup level can increase discrimination. The results have practical implications for designing better online freelance marketplaces.},
	collection   = {CHI ’20}
}
@techreport{li2020hiring,
	title        = {Hiring as Exploration},
	author       = {Li, Danielle and Raymond, Lindsey R and Bergman, Peter},
	year         = 2020,
	institution  = {National Bureau of Economic Research}
}
@inproceedings{li2021algorithmic,
	title        = {Algorithmic hiring in practice: Recruiter and HR Professional's perspectives on AI use in hiring},
	author       = {Li, Lan and Lassiter, Tina and Oh, Joohee and Lee, Min Kyung},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {166--176}
}
@article{liang_gpt_2023,
	title        = {{GPT} detectors are biased against non-native {English} writers},
	author       = {Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
	year         = 2023,
	month        = jul,
	journal      = {Patterns},
	volume       = 4,
	number       = 7,
	pages        = 100779,
	doi          = {10.1016/j.patter.2023.100779},
	issn         = {2666-3899},
	url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10382961/},
	urldate      = {2023-09-20},
	abstract     = {GPT detectors frequently misclassify non-native English writing as AI generated, raising concerns about fairness and robustness. Addressing the biases in these detectors is crucial to prevent the marginalization of non-native English speakers in evaluative and educational settings and to create a more equitable digital landscape.},
	pmid         = 37521038,
	pmcid        = {PMC10382961},
	file         = {PubMed Central Full Text PDF:/Users/neilnatarajan/Zotero/storage/FVZUIK6N/Liang et al. - 2023 - GPT detectors are biased against non-native Englis.pdf:application/pdf}
}
@misc{liao_human-centered_2022,
	title        = {Human-{Centered} {Explainable} {AI} ({XAI}): {From} {Algorithms} to {User} {Experiences}},
	shorttitle   = {Human-{Centered} {Explainable} {AI} ({XAI})},
	author       = {Liao, Q. Vera and Varshney, Kush R.},
	year         = 2022,
	month        = apr,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2110.10790},
	urldate      = {2022-08-18},
	note         = {arXiv:2110.10790 [cs]},
	abstract     = {In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question "what are human-centered approaches doing for XAI" and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, \_tablet},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Q3NRRGHW/2110.html:text/html;Liao_Varshney_2022_Human-Centered Explainable AI (XAI).pdf:/Users/neilnatarajan/Zotero/storage/VAJV5NEW/Liao_Varshney_2022_Human-Centered Explainable AI (XAI).pdf:application/pdf}
}
@article{CHU2022220,
	title        = {Design, modeling, and control of morphing aircraft: A review},
	author       = {Lingling CHU and Qi LI and Feng GU and Xintian DU and Yuqing HE and Yangchen DENG},
	year         = 2022,
	journal      = {Chinese Journal of Aeronautics},
	volume       = 35,
	number       = 5,
	pages        = {220--246},
	doi          = {https://doi.org/10.1016/j.cja.2021.09.013},
	issn         = {1000-9361},
	url          = {https://www.sciencedirect.com/science/article/pii/S1000936121003496},
	keywords     = {Aircraft design, Dynamic modeling, Fault tolerance, Flight control, Intelligent material, Morphing aircraft, Unmanned Aerial Vehicles (UAV)},
	abstract     = {A morphing aircraft can adapt its configuration to suit different types of tasks, which is also an important requirement of Unmanned Aerial Vehicles (UAV). The successful development of an unmanned morphing aircraft involves three steps that determine its ability and intelligent: configuration design, dynamic modeling and flight control. This study conducts a comprehensive survey of morphing aircraft. First, the methods to design the configuration of a morphing aircraft are presented and analyzed. Then, the nonlinear dynamic characteristics and aerodynamic interference caused by a morphing wing are described. Subsequently, the dynamic modeling and flight control methods for solving the flight control problems are summarized with respect to these features. Finally, the general as well as special challenges ahead of the development of intelligent morphing aircraft are discussed. The findings can provide a theoretical and technical reference for designing future morphing aircraft or morphing-wing UAVs.}
}
@inproceedings{linxen2021weird,
	title        = {How weird is CHI?},
	author       = {Linxen, Sebastian and Sturm, Christian and Br{\"u}hlmann, Florian and Cassau, Vincent and Opwis, Klaus and Reinecke, Katharina},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 chi conference on human factors in computing systems},
	pages        = {1--14}
}
@article{Lipton,
	title        = {The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
	author       = {Lipton, Zachary C},
	year         = 2018,
	journal      = {Queue},
	publisher    = {ACM New York, NY, USA},
	volume       = 16,
	number       = 3,
	pages        = {31--57}
}
@misc{liu_deid-gpt_2023,
	title        = {{DeID}-{GPT}: {Zero}-shot {Medical} {Text} {De}-{Identification} by {GPT}-4},
	shorttitle   = {{DeID}-{GPT}},
	author       = {Liu, Zhengliang and Yu, Xiaowei and Zhang, Lu and Wu, Zihao and Cao, Chao and Dai, Haixing and Zhao, Lin and Liu, Wei and Shen, Dinggang and Li, Quanzheng and Liu, Tianming and Zhu, Dajiang and Li, Xiang},
	year         = 2023,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2303.11032},
	url          = {http://arxiv.org/abs/2303.11032},
	urldate      = {2023-04-05},
	note         = {arXiv:2303.11032 [cs]},
	abstract     = {The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework ("DeID-GPT") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.},
	keywords     = {Computer Science - Computers and Society, Computer Science - Computation and Language, \_tablet},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/KSFDDDXM/2303.html:text/html;Liu et al_2023_DeID-GPT.pdf:/Users/neilnatarajan/Zotero/storage/292WR3K5/Liu et al_2023_DeID-GPT.pdf:application/pdf}
}
@inproceedings{lu_organizing_2023,
	title        = {Organizing {Community}-based {Events} in {Participatory} {Action} {Research}: {Lessons} {Learned} from a {Photovoice} {Exhibition}},
	shorttitle   = {Organizing {Community}-based {Events} in {Participatory} {Action} {Research}},
	author       = {Lu, Alex Jiahong and Sannon, Shruti and Brewer, Savana and Jackson, Kisha N and Green, Jaye and Reeder, Daivon and Wafer, Camaria and Dillahunt, Tawanna R},
	year         = 2023,
	month        = apr,
	booktitle    = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} {EA} '23},
	pages        = {1--8},
	doi          = {10.1145/3544549.3573846},
	isbn         = {978-1-4503-9422-2},
	url          = {https://dl.acm.org/doi/10.1145/3544549.3573846},
	urldate      = {2024-07-30},
	abstract     = {Participatory action research (PAR) approaches center community members’ lived experiences and can spur positive change around pressing challenges faced by communities. Even though PAR and similar approaches have been increasingly adopted in HCI research that focuses on social justice and community empowerment, public-facing events that are based on this research and center community members’ voices are less common. This case study sheds light on how to initiate and organize events that build on existing PAR efforts, and what practical challenges might exist in this process. Building on a photovoice research project, we—a collaborative team of university researchers and staff members of a community organisation in Eastside Detroit—co-organized a community-based public-facing exhibition that featured community members’ photographic narratives of personal and communal safety and surveillance. In this case study, we reflect on the challenges we experienced in planning and holding the exhibition. We contribute a set of practical guidelines to help researchers facilitate community-based events when conducting participatory action research in HCI.},
	file         = {Lu et al_2023_Organizing Community-based Events in Participatory Action Research_annotated.pdf:/Users/neilnatarajan/Zotero/storage/R9YE2JS7/Lu et al_2023_Organizing Community-based Events in Participatory Action Research_annotated.pdf:application/pdf;Lu et al_2023_Organizing Community-based Events in Participatory Action Research.pdf:/Users/neilnatarajan/Zotero/storage/R9YE2JS7/Lu et al_2023_Organizing Community-based Events in Participatory Action Research.pdf:application/pdf}
}
@article{lucas_causal_nodate,
	title        = {Causal {Inference}},
	author       = {Lucas, Keane and Huang, Biwei and Stelmakh, Ivan},
	url          = {https://blog.ml.cmu.edu/2020/08/31/7-causality/}
}
@article{luccioni_using_nodate,
	title        = {Using {Natural} {Language} {Processing} to {Analyze} {Financial} {Climate} {Disclosures}},
	author       = {Luccioni, Alexandra and Palacios, Hector},
	pages        = 3,
	abstract     = {According to U.S. ﬁnancial legislation, companies traded on the stock market are obliged to regularly disclose risks and uncertainties that are likely to affect their operations or ﬁnancial position. Since 2010, these disclosures must also include climate-related risk projections. These disclosures therefore present a large quantity of textual information on which we can apply NLP techniques in order to pinpoint the companies that divulge their climate risks and those that do not, the types of vulnerabilities that are disclosed, and to follow the evolution of these risks over time.},
	language     = {en},
	file         = {Luccioni_Palacios_Using Natural Language Processing to Analyze Financial Climate Disclosures.pdf:/Users/neilnatarajan/Zotero/storage/2Y6ZYC8M/Luccioni_Palacios_Using Natural Language Processing to Analyze Financial Climate Disclosures.pdf:application/pdf}
}
@misc{lum_closer_2021,
	title        = {Closer than they appear: {A} {Bayesian} perspective on individual-level heterogeneity in risk assessment},
	shorttitle   = {Closer than they appear},
	author       = {Lum, Kristian and Dunson, David B. and Johndrow, James},
	year         = 2021,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2102.01135},
	urldate      = {2023-03-18},
	note         = {arXiv:2102.01135 [stat]},
	abstract     = {Risk assessment instruments are used across the criminal justice system to estimate the probability of some future behavior given covariates. The estimated probabilities are then used in making decisions at the individual level. In the past, there has been controversy about whether the probabilities derived from group-level calculations can meaningfully be applied to individuals. Using Bayesian hierarchical models applied to a large longitudinal dataset from the court system in the state of Kentucky, we analyze variation in individual-level probabilities of failing to appear for court and the extent to which it is captured by covariates. We ﬁnd that individuals within the same risk group vary widely in their probability of the outcome. In practice, this means that allocating individuals to risk groups based on standard approaches to risk assessment, in large part, results in creating distinctions among individuals who are not meaningfully different in terms of their likelihood of the outcome. This is because uncertainty about the probability that any particular individual will fail to appear is large relative to the difference in average probabilities among any reasonable set of risk groups.},
	language     = {en},
	keywords     = {Statistics - Applications},
	file         = {Lum et al. - 2021 - Closer than they appear A Bayesian perspective on.pdf:/Users/neilnatarajan/Zotero/storage/9Q8JR2KP/Lum et al. - 2021 - Closer than they appear A Bayesian perspective on.pdf:application/pdf}
}
@article{lundberg_unified_2017,
	title        = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	author       = {Lundberg, Scott and Lee, Su-In},
	year         = 2017,
	month        = nov,
	journal      = {arXiv:1705.07874 [cs, stat]},
	url          = {http://arxiv.org/abs/1705.07874},
	urldate      = {2022-01-06},
	note         = {arXiv: 1705.07874},
	abstract     = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LS6VRCJC/1705.html:text/html;Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/neilnatarajan/Zotero/storage/AIQWBTDJ/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:application/pdf},
	annote       = {Comment: To appear in NIPS 2017}
}
@article{lundberg_consistent_2018,
	title        = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	author       = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1802.03888},
	url          = {http://arxiv.org/abs/1802.03888},
	note         = {\_eprint: 1802.03888}
}
@article{mai_user_nodate,
	title        = {User {Mental} {Models} of {Cryptocurrency} {Systems} - {A} {Grounded} {Theory} {Approach}},
	author       = {Mai, Alexandra and Pfeffer, Katharina},
	abstract     = {Frequent reports of monetary loss, fraud, and user-caused security incidents in the context of cryptocurrencies emphasize the need for human-centered research in this domain. We contribute the ﬁrst qualitative user study (N = 29) on user mental models of cryptocurrency systems and the associated threat landscape. Using Grounded Theory, we reveal misconceptions affecting users’ security and privacy. Our results suggest that current cryptocurrency tools (e.g., wallets and exchanges) are not capable of counteracting threats caused by these misconceptions. Hence, users frequently fail to securely manage their private keys or assume to be anonymous when they are not. Based on our ﬁndings, we contribute actionable advice, grounded in the mental models of users, to improve the usability and secure usage of cryptocurrency systems.},
	language     = {en},
	file         = {Mai and Pfeffer - User Mental Models of Cryptocurrency Systems - A G.pdf:/Users/neilnatarajan/Zotero/storage/EUYFJKPY/Mai and Pfeffer - User Mental Models of Cryptocurrency Systems - A G.pdf:application/pdf}
}
@article{malik_hierarchy_2020,
	title        = {A {Hierarchy} of {Limitations} in {Machine} {Learning}},
	author       = {Malik, Momin M.},
	year         = 2020,
	month        = feb,
	journal      = {arXiv:2002.05193 [cs, econ, math, stat]},
	url          = {http://arxiv.org/abs/2002.05193},
	urldate      = {2021-11-25},
	note         = {arXiv: 2002.05193},
	abstract     = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society, Economics - Econometrics, G.3, I.6.4, J.4, Mathematics - Statistics Theory},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/L9M4H87C/2002.html:text/html;Malik_2020_A Hierarchy of Limitations in Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/E8MDBZ9L/Malik_2020_A Hierarchy of Limitations in Machine Learning.pdf:application/pdf}
}
@article{mandrekar_receiver_2010,
	title        = {Receiver {Operating} {Characteristic} {Curve} in {Diagnostic} {Test} {Assessment}},
	author       = {Mandrekar, Jayawant N.},
	year         = 2010,
	month        = sep,
	journal      = {Journal of Thoracic Oncology},
	volume       = 5,
	number       = 9,
	pages        = {1315--1316},
	doi          = {10.1097/JTO.0b013e3181ec173d},
	issn         = {1556-0864},
	url          = {https://www.sciencedirect.com/science/article/pii/S1556086415306043},
	urldate      = {2024-02-19},
	abstract     = {The performance of a diagnostic test in the case of a binary predictor can be evaluated using the measures of sensitivity and specificity. However, in many instances, we encounter predictors that are measured on a continuous or ordinal scale. In such cases, it is desirable to assess performance of a diagnostic test over the range of possible cutpoints for the predictor variable. This is achieved by a receiver operating characteristic (ROC) curve that includes all the possible decision thresholds from a diagnostic test result. In this brief report, we discuss the salient features of the ROC curve, as well as discuss and interpret the area under the ROC curve, and its utility in comparing two different tests or predictor variables of interest.},
	keywords     = {AUC, ROC, Sensitivity, Specificity},
	file         = {Full Text:/Users/neilnatarajan/Zotero/storage/F6QXFX5R/Mandrekar - 2010 - Receiver Operating Characteristic Curve in Diagnos.pdf:application/pdf}
}
@misc{marcus_next_2020,
	title        = {The {Next} {Decade} in {AI}: {Four} {Steps} {Towards} {Robust} {Artificial} {Intelligence}},
	shorttitle   = {The {Next} {Decade} in {AI}},
	author       = {Marcus, Gary},
	year         = 2020,
	month        = feb,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2002.06177},
	url          = {http://arxiv.org/abs/2002.06177},
	urldate      = {2022-11-16},
	note         = {arXiv:2002.06177 [cs]},
	abstract     = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.6, \_tablet, I.2},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IQYEFV7G/2002.html:text/html;Marcus_2020_The Next Decade in AI.pdf:/Users/neilnatarajan/Zotero/storage/ZTIP4HAA/Marcus_2020_The Next Decade in AI.pdf:application/pdf}
}
@article{markus_role_2021,
	title        = {The role of explainability in creating trustworthy artificial intelligence for health care: {A} comprehensive survey of the terminology, design choices, and evaluation strategies},
	shorttitle   = {The role of explainability in creating trustworthy artificial intelligence for health care},
	author       = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
	year         = 2021,
	month        = jan,
	journal      = {Journal of Biomedical Informatics},
	volume       = 113,
	pages        = 103655,
	doi          = {10.1016/j.jbi.2020.103655},
	issn         = {1532-0464},
	url          = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
	urldate      = {2022-01-26},
	abstract     = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
	language     = {en},
	keywords     = {\_tablet, Explainable artificial intelligence, Explainable modelling, Interpretability, Post-hoc explanation, Trustworthy artificial intelligence},
	file         = {Markus et al_2021_The role of explainability in creating trustworthy artificial intelligence for.pdf:/Users/neilnatarajan/Zotero/storage/89XMUAIS/Markus et al_2021_The role of explainability in creating trustworthy artificial intelligence for.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/B2DJP4BI/S1532046420302835.html:text/html}
}
@article{mccradden_when_2021,
	title        = {When is accuracy off-target?},
	author       = {McCradden, Melissa D.},
	year         = 2021,
	month        = jun,
	journal      = {Translational Psychiatry},
	volume       = 11,
	number       = 1,
	pages        = 369,
	doi          = {10.1038/s41398-021-01479-4},
	issn         = {2158-3188},
	url          = {http://www.nature.com/articles/s41398-021-01479-4},
	urldate      = {2021-09-16},
	language     = {en},
	file         = {Markup:/Users/neilnatarajan/Zotero/storage/P7MBUHQC/When is accuracy off-target.pdf:application/pdf;McCradden_2021_When is accuracy off-target.pdf:/Users/neilnatarajan/Zotero/storage/KISMSTP2/McCradden_2021_When is accuracy off-target.pdf:application/pdf}
}
@book{Mertler_2019,
	title        = {The Wiley handbook of action research in education},
	author       = {Mertler, Craig A},
	year         = 2019,
	publisher    = {John Wiley and Sons},
	address      = {Hoboken, NJ},
	series       = {Wiley handbooks in education},
	isbn         = {978-1-119-39996-4},
	callnumber   = {LB1028.24 .W54 2019},
	collection   = {Wiley handbooks in education},
	language     = {en}
}
@article{merton1942note,
	title        = {A note on science and democracy},
	author       = {Merton, Robert K},
	year         = 1942,
	journal      = {Journal of legal and political sociology},
	publisher    = {East Hampton},
	volume       = 1,
	number       = {1-2},
	pages        = {115--126}
}
@article{MikePerkins_JasperRoe_2023,
	title        = {Decoding Academic Integrity Policies: A Corpus Linguistics Investigation of AI and Other Technological Threats},
	author       = {Mike Perkins and Jasper Roe},
	year         = 2023,
	month        = jul,
	doi          = {10.1057/s41307-023-00323-2},
	note         = {DOI: 10.1057/s41307-023-00323-2 MAG ID: 4384560074},
	abstractnote = {This study presents a corpus analysis of academic integrity policies from Higher Education Institutions (HEIs) worldwide, exploring how they address the issues posed by technological threats, such as Automated Paraphrasing Tools and generative-artificial intelligence tools, such as ChatGPT. The analysis of 142 policies conducted in November and December 2022, and May 2023 reveals a gap regarding the mention of AI and associated technologies in the available academic integrity policies. Despite the growing prevalence of these tools in the 6-month period since the release of ChatGPT, no HEIs had produced revised academic integrity policies. Content analysis of 53 guidance documents produced by HEIs suggests an overall positive focus of Gen AI tools, yet advises caution. This study suggests a modification to Bretag et al.’s (Int J Educ Integr 7, 2011) exemplary academic integrity model, introducing “Technological Explicitness” — emphasizing the need to include explicit guidelines about new technologies in academic integrity policies. These results underscore the urgent need for HEIs to revise their academic integrity policies, considering the evolving landscape of AI and its implications for academic integrity. This paper argues for a multifaceted approach to deal with the issues of integrating technology, education, policy reform, and assessment restructuring to navigate these challenges while upholding academic integrity.},
	abstract     = {This study presents a corpus analysis of academic integrity policies from Higher Education Institutions (HEIs) worldwide, exploring how they address the issues posed by technological threats, such as Automated Paraphrasing Tools and generative-artificial intelligence tools, such as ChatGPT. The analysis of 142 policies conducted in November and December 2022, and May 2023 reveals a gap regarding the mention of AI and associated technologies in the available academic integrity policies. Despite the growing prevalence of these tools in the 6-month period since the release of ChatGPT, no HEIs had produced revised academic integrity policies. Content analysis of 53 guidance documents produced by HEIs suggests an overall positive focus of Gen AI tools, yet advises caution. This study suggests a modification to Bretag et al.’s (Int J Educ Integr 7, 2011) exemplary academic integrity model, introducing “Technological Explicitness” — emphasizing the need to include explicit guidelines about new technologies in academic integrity policies. These results underscore the urgent need for HEIs to revise their academic integrity policies, considering the evolving landscape of AI and its implications for academic integrity. This paper argues for a multifaceted approach to deal with the issues of integrating technology, education, policy reform, and assessment restructuring to navigate these challenges while upholding academic integrity.},
	keywords     = {\_tablet},
	file         = {Mike Perkins_Jasper Roe_2023_Decoding Academic Integrity Policies.pdf:/Users/neilnatarajan/Zotero/storage/WQTLG8HZ/Mike Perkins_Jasper Roe_2023_Decoding Academic Integrity Policies.pdf:application/pdf}
}
@book{mill1998liberty,
	title        = {On liberty and other essays},
	author       = {Mill, John Stuart},
	year         = 1998,
	publisher    = {Oxford University Press, USA}
}
@article{miller_explanation_2017,
	title        = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	shorttitle   = {Explanation in {Artificial} {Intelligence}},
	author       = {Miller, Tim},
	year         = 2017,
	month        = jun,
	journal      = {CoRR},
	volume       = {abs/1706.07269},
	url          = {https://arxiv.org/abs/1706.07269v3},
	urldate      = {2021-11-08},
	note         = {\_eprint: 1706.07269},
	abstract     = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/ATHVMUH4/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/L7F658C4/1706.html:text/html}
}
@misc{miller_explainable_2023,
	title        = {Explainable {AI} is {Dead}, {Long} {Live} {Explainable} {AI}! {Hypothesis}-driven decision support},
	author       = {Miller, Tim},
	year         = 2023,
	month        = mar,
	publisher    = {arXiv},
	number       = {arXiv:2302.12389},
	doi          = {10.48550/arXiv.2302.12389},
	url          = {http://arxiv.org/abs/2302.12389},
	urldate      = {2023-04-05},
	note         = {arXiv:2302.12389 [cs]},
	abstract     = {In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/FDVSR8XQ/2302.html:text/html;Miller_2023_Explainable AI is Dead, Long Live Explainable AI_annotated.pdf:/Users/neilnatarajan/Zotero/storage/5GLPJWA2/Miller_2023_Explainable AI is Dead, Long Live Explainable AI_annotated.pdf:application/pdf;Miller_2023_Explainable AI is Dead, Long Live Explainable AI.pdf:/Users/neilnatarajan/Zotero/storage/5GLPJWA2/Miller_2023_Explainable AI is Dead, Long Live Explainable AI.pdf:application/pdf},
	abstractnote = {In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.}
}
@book{mills2015blackness,
	title        = {Blackness visible: Essays on philosophy and race},
	author       = {Mills, Charles W},
	year         = 2015,
	publisher    = {Cornell University Press}
}
@article{minasny_digital_2019,
	title        = {Digital mapping of peatlands – {A} critical review},
	author       = {Minasny, Budiman and Berglund, Örjan and Connolly, John and Hedley, Carolyn and de Vries, Folkert and Gimona, Alessandro and Kempen, Bas and Kidd, Darren and Lilja, Harry and Malone, Brendan and McBratney, Alex and Roudier, Pierre and O'Rourke, Sharon and {Rudiyanto} and Padarian, José and Poggio, Laura and ten Caten, Alexandre and Thompson, Daniel and Tuve, Clint and Widyatmanti, Wirastuti},
	year         = 2019,
	month        = sep,
	journal      = {Earth-Science Reviews},
	volume       = 196,
	pages        = 102870,
	doi          = {10.1016/j.earscirev.2019.05.014},
	issn         = {00128252},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S001282521830360X},
	urldate      = {2021-10-15},
	abstract     = {Peatlands offer a series of ecosystem services including carbon storage, biomass production, and climate regulation. Climate change and rapid land use change are degrading peatlands, liberating their stored carbon (C) into the atmosphere. To conserve peatlands and help in realising the Paris Agreement, we need to understand their extent, status, and C stocks. However, current peatland knowledge is vague—estimates of global peatland extent ranges from 1 to 4.6 million km2, and C stock estimates vary between 113 and 612 Pg (or billion tonne C). This uncertainty mostly stems from the coarse spatial scale of global soil maps. In addition, most global peatland estimates are based on rough country inventories and reports that use outdated data. This review shows that digital mapping using field observations combined with remotely-sensed images and statistical models is an avenue to more accurately map peatlands and decrease this knowledge gap. We describe peat mapping experiences from 12 countries or regions and review 90 recent studies on peatland mapping. We found that interest in mapping peat information derived from satellite imageries and other digital mapping technologies is growing. Many studies have delineated peat extent using land cover from remote sensing, ecology, and environmental field studies, but rarely perform validation, and calculating the uncertainty of prediction is rare. This paper then reviews various proximal and remote sensing techniques that can be used to map peatlands. These include geophysical measurements (electromagnetic induction, resistivity measurement, and gamma radiometrics), radar sensing (SRTM, SAR), and optical images (Visible and Infrared). Peatland is better mapped when using more than one covariate, such as optical and radar products using nonlinear machine learning algorithms. The proliferation of satellite data available in an open-access format, availability of machine learning algorithms in an open-source computing environment, and high-performance computing facilities could enhance the way peatlands are mapped. Digital soil mapping allows us to map peat in a cost-effective, objective, and accurate manner. Securing peatlands for the future, and abating their contribution to atmospheric C levels, means digitally mapping them now.},
	language     = {en},
	file         = {Minasny et al_2019_Digital mapping of peatlands – A critical review.pdf:/Users/neilnatarajan/Zotero/storage/EGARA839/Minasny et al_2019_Digital mapping of peatlands – A critical review.pdf:application/pdf}
}
@article{minkin2023diversity,
	title        = {Diversity, Equity and Inclusion in the Workplace},
	author       = {Minkin, Rachel},
	year         = 2023,
	publisher    = {Pew Research Center},
	date-added   = {2023-09-11 21:39:01 -0400},
	date-modified = {2023-09-11 21:39:01 -0400}
}
@misc{mitchell_detectgpt_2023,
	title        = {{DetectGPT}: {Zero}-{Shot} {Machine}-{Generated} {Text} {Detection} using {Probability} {Curvature}},
	shorttitle   = {{DetectGPT}},
	author       = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
	year         = 2023,
	month        = jan,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2301.11305},
	url          = {http://arxiv.org/abs/2301.11305},
	urldate      = {2023-04-06},
	note         = {arXiv:2301.11305 [cs]},
	abstract     = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/5FSXCBTN/Mitchell et al. - 2023 - DetectGPT Zero-Shot Machine-Generated Text Detect.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GFYNS2GZ/2301.html:text/html}
}
@inproceedings{mohseni_trust_nodate,
	title        = {Trust {Evolution} {Over} {Time} in {Explainable} {AI} for {Fake} {News} {Detection}},
	shorttitle   = {Trust Evolution Over Time in Explainable AI for Fake News Detection},
	author       = {Mohseni, Sina and Yang, Fan and Pentyala, Shiva and Du, Mengnan and Liu, Yi and Lupfer, Nic and Hu, Xia and Ji, Shuiwang and Ragan, Eric D},
	year         = 2020,
	publisher    = {Association for Computing Machinery},
	series       = {CHI 2020},
	pages        = 4,
	abstract     = {The need for interpretable and accountable intelligent systems is strong as artiﬁcial intelligence (AI) becomes more prevalent in human life. We study the effects of interpretability on user’s trust in an AI assistant tool designed for fake news detection. In our study, we expose participants to different types of AI and Explainable AI (XAI) assistants, measure their perceived accuracy of algorithm, and cluster user trust changes over time into ﬁve types of trust evolution. We present quantitative results and analysis from human-subject studies and discuss our ﬁndings regarding how model explanations affect on user trust evolution over time.},
	language     = {en},
	file         = {Mohseni et al. - Trust Evolution Over Time in Explainable AI for Fa.pdf:/Users/neilnatarajan/Zotero/storage/SIN9YLNL/Mohseni et al. - Trust Evolution Over Time in Explainable AI for Fa.pdf:application/pdf},
	abstractnote = {The need for interpretable and accountable intelligent systems is strong as artiﬁcial intelligence (AI) becomes more prevalent in human life. We study the effects of interpretability on user’s trust in an AI assistant tool designed for fake news detection. In our study, we expose participants to different types of AI and Explainable AI (XAI) assistants, measure their perceived accuracy of algorithm, and cluster user trust changes over time into ﬁve types of trust evolution. We present quantitative results and analysis from human-subject studies and discuss our ﬁndings regarding how model explanations affect on user trust evolution over time.}
}
@book{molnar_interpretable_2019,
	title        = {Interpretable {Machine} {Learning}: {A} {Guide} for {Making} {Black} {Box} {Models} {Explainable}},
	author       = {Molnar, Christoph},
	year         = 2019,
	annote       = {https://christophm.github.io/interpretable-ml-book/}
}
@article{monbiot_britains_2021,
	title        = {Britain’s rivers are suffocating to death},
	author       = {Monbiot, George},
	year         = 2021,
	month        = jul,
	journal      = {The Guardian},
	issn         = {0261-3077},
	url          = {https://www.theguardian.com/commentisfree/2021/jul/21/britains-rivers-suffocating-industrial-farm-waste},
	urldate      = {2021-10-15},
	chapter      = {Opinion},
	abstract     = {Water that should be crystal clear has become a green-brown slop of microscopic algae because of industrial farm waste, says Guardian columnist George Monbiot},
	language     = {en-GB},
	keywords     = {Environment, UK news, Pollution, Rivers, Animals, Biodiversity, Conservation, Farming, Plants, Wildlife},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/G5YMKKR4/britains-rivers-suffocating-industrial-farm-waste.html:text/html}
}
@article{morata-ramirez_construct_2013,
	title        = {Construct {Validity} of {Likert} {Scales} through {Confirmatory} {Factor} {Analysis}: {A} {Simulation} {Study} {Comparing} {Different} {Methods} of {Estimation} {Based} on {Pearson} and {Polychoric} {Correlations}},
	shorttitle   = {Construct {Validity} of {Likert} {Scales} through {Confirmatory} {Factor} {Analysis}},
	author       = {Morata-Ramírez, María de los Ángeles and Holgado-Tello, Francisco Pablo},
	year         = 2013,
	month        = jan,
	journal      = {International Journal of Social Science Studies},
	volume       = 1,
	number       = 1,
	pages        = {54--61},
	doi          = {10.11114/ijsss.v1i1.27},
	issn         = {2324-8041},
	url          = {https://redfame.com/journal/index.php/ijsss/article/view/27},
	urldate      = {2022-09-15},
	copyright    = {Copyright (c)},
	note         = {Number: 1},
	abstract     = {The widespread use of Pearson correlations and, by extension, the Maximum Likelihood estimation method, does not take into account the measurement properties of Likert scales observed variables when carrying out a construct validity process through Confirmatory Factor Analysis (CFA). This simulation study compares four estimation methods (Maximum Likelihood –ML-, Robust Maximum Likelihood –RML-, Robust Unweighted Least Squares –, RULS) according to two of the assumptions CFA is supposed to fulfil: multivariate normality and, especially, the continuous measurement nature of both latent and observed variables. Goodness of fit is diagnosed by X2 Likelihood Ratio Test and RMSEA indices. Results suggest ULS and RULS are preferable as polychoric correlations help to overcome grouping and transformation errors produced when using Pearson correlations for ordinal observed variables. Data measurement scale consideration enhances the ability of hypothesized models to reproduce accurately construct variables relationships.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/UVDDF45R/Morata-Ramírez and Holgado-Tello - 2013 - Construct Validity of Likert Scales through Confir.pdf:application/pdf},
	rights       = {Copyright (c)},
	abstractnote = {The widespread use of Pearson correlations and, by extension, the Maximum Likelihood estimation method, does not take into account the measurement properties of Likert scales observed variables when carrying out a construct validity process through Confirmatory Factor Analysis (CFA). This simulation study compares four estimation methods (Maximum Likelihood –ML-, Robust Maximum Likelihood –RML-, Robust Unweighted Least Squares –, RULS) according to two of the assumptions CFA is supposed to fulfil: multivariate normality and, especially, the continuous measurement nature of both latent and observed variables. Goodness of fit is diagnosed by X2 Likelihood Ratio Test and RMSEA indices. Results suggest ULS and RULS are preferable as polychoric correlations help to overcome grouping and transformation errors produced when using Pearson correlations for ordinal observed variables. Data measurement scale consideration enhances the ability of hypothesized models to reproduce accurately construct variables relationships.}
}
@book{morris1984origins,
	title        = {The origins of the civil rights movement},
	author       = {Morris, Aldon D},
	year         = 1984,
	publisher    = {Simon and Schuster}
}
@article{mothilal_explaining_2019,
	title        = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	author       = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1905.07697},
	url          = {http://arxiv.org/abs/1905.07697},
	note         = {\_eprint: 1905.07697}
}
@book{mullainathan2019machine,
	title        = {A machine learning approach to low-value health care: wasted tests, missed heart attacks and mis-predictions},
	author       = {Mullainathan, Sendhil and Obermeyer, Ziad},
	year         = 2019,
	publisher    = {National Bureau of Economic Research}
}
@inproceedings{muller_learning_2019,
	title        = {Learning from {Team} and {Group} {Diversity}: {Nurturing} and {Benefiting} from our {Heterogeneity}},
	shorttitle   = {Learning from {Team} and {Group} {Diversity}},
	author       = {Muller, Michael and Fussell, Susan R. and Gao, Ge and Hinds, Pamela J. and Oliveira, Nigini and Reinecke, Katharina and Robert, Lionel and Siangliulue, Kanya (Pao) and Wulf, Volker and Yuan, Chien-Wen},
	year         = 2019,
	month        = nov,
	booktitle    = {Companion {Publication} of the 2019 {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CSCW} '19 {Companion}},
	pages        = {498--505},
	doi          = {10.1145/3311957.3359440},
	isbn         = {978-1-4503-6692-2},
	url          = {https://doi.org/10.1145/3311957.3359440},
	urldate      = {2024-07-23},
	abstract     = {By 2019, diversity is an established fact in most workplaces, teams, and work-groups, presenting both old and new challenges to CSCW in terms of team structure and technological supports for increasingly diverse teams. The research literature on diversity and teams has examined many definitions and attributes of diversity, and has described different types of teams, tasks, and measures, with contrasting and even contradictory results. Diversity becomes a strength in some studies, and a burden in others. The literature is similarly complex regarding individual and organisational approaches to realize those strengths, or to mitigate those burdens. In this workshop, we collectively take stock of these complex findings; we consider the several theoretical and methodological efforts to organize these findings; and we propose new research directions to address the "diversity of diversity studies.",0},
	file         = {Submitted Version:/Users/neilnatarajan/Zotero/storage/3R3VC2BX/Muller et al. - 2019 - Learning from Team and Group Diversity Nurturing .pdf:application/pdf}
}
@article{murdoch_definitions_2019,
	title        = {Definitions, methods, and applications in interpretable machine learning},
	author       = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	year         = 2019,
	month        = oct,
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = 116,
	number       = 44,
	pages        = {22071--22080},
	doi          = {10.1073/pnas.1900654116},
	issn         = {0027-8424, 1091-6490},
	url          = {https://pnas.org/doi/full/10.1073/pnas.1900654116},
	urldate      = {2022-04-14},
	abstract     = {Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. , Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Murdoch et al_2019_Definitions, methods, and applications in interpretable machine learning.pdf:/Users/neilnatarajan/Zotero/storage/56TDM4D9/Murdoch et al_2019_Definitions, methods, and applications in interpretable machine learning.pdf:application/pdf}
}
@inproceedings{natarajan_trust_2023,
	title        = {Trust {Explanations} to {Do} {What} {They} {Say}},
	author       = {Natarajan, Neil and Binns, Reuben and Zhao, Jun and Shadbolt, Nigel},
	year         = 2022,
	booktitle    = {Human-Centered AI Workshop at NeurIPS 2022},
	url          = {https://openreview.net/pdf?id=mzsPCefDaY5},
	abstract     = {How much are we to trust a decision made by an AI algorithm? Trusting an algorithm without cause may lead to abuse, and mistrusting it may similarly lead to disuse. Trust in an AI is only desirable if it is warranted; thus, calibrating trust is critical to ensuring appropriate use. In the name of calibrating trust appropriately, AI developers should provide contracts specifying use cases in which an algorithm can and cannot be trusted. Automated explanation of AI outputs is often touted as a method by which trust can be built in the algorithm. However, automated explanations arise from algorithms themselves, so trust in these explanations is similarly only desirable if it is warranted. Developers of algorithms explaining AI outputs (xAI algorithms) should provide similar contracts, which should specify use cases in which an explanation can and cannot be trusted.},
	language     = {en}
}
@inproceedings{natarajan_detecting_2024,
	title        = {Detecting Generative AI Usage in Application Essays},
	author       = {Natarajan, Neil and Hanno, Elías Sánchez and Gittelson, Logan},
	year         = 2024,
	booktitle    = {Generative AI and HCI workshop at CHI 2024},
	url          = {https://generativeaiandhci.github.io/papers/2024/genaichi2024_9.pdf},
	language     = {en}
}
@book{Ziegler_2008,
	title        = {Legacy: Cecil Rhodes, the Rhodes Trust and Rhodes Scholarships},
	author       = {Ziegler, P.},
	year         = 2008,
	publisher    = {Yale University Press},
	isbn         = {978-0-300-11835-3},
	url          = {https://books.google.com/books?id=FuOeAAAAMAAJ}
}
@inbook{batyavalue,
	title        = {Value Sensitive Design and Information Systems},
	author       = {Friedman, Batya and Kahn, Peter and Borning, Alan and Zhang, Ping and Galletta, Dennis},
	year         = 2006,
	month        = {01},
	journal      = {The Handbook of Information and Computer Ethics},
	doi          = {10.1007/978-94-007-7844-3_4},
	isbn         = {978-94-007-7843-6}
}
@inproceedings{Dumbalska_Bhatti_Ali_Summerfield_2023,
	title        = {How do humans learn concepts and strategies?},
	author       = {Dumbalska, Tsvetomira and Bhatti, Areej and Ali, Ibi and Summerfield, Christopher},
	year         = 2023,
	booktitle    = {2023 Conference on Cognitive Computational Neuroscience},
	publisher    = {Cognitive Computational Neuroscience},
	address      = {Oxford, UK},
	doi          = {10.32470/CCN.2023.1520-0},
	url          = {https://2023.ccneuro.org/view_paper.php?PaperNum=1520}
}
@article{Pasquale_2006,
	title        = {Rankings, Reductionism, and Responsibility},
	author       = {Pasquale, Frank},
	year         = 2006,
	month        = jan,
	journal      = {Faculty Scholarship},
	url          = {https://digitalcommons.law.umaryland.edu/fac_pubs/1351}
}
@inbook{Latzer_Hollnbuchner_Just_Saurwein_2014,
	title        = {The Economics of Algorithmic Selection on the Internet},
	author       = {Latzer, Michael and Hollnbuchner, Katharina and Just, Natascha and Saurwein, Florian},
	year         = 2014,
	month        = oct,
	address      = {Rochester, NY},
	number       = 2710399,
	doi          = {10.5167/uzh-100400},
	url          = {https://papers.ssrn.com/abstract=2710399},
	abstractnote = {The Internet is increasingly permeating daily life with an essential and structuring bundle of innovations: Internet-based applications that operate on algorithmic selection. Automated algorithmic selection is embedded in a variety of services and applied for numerous purposes. Algorithms in search engines or news Aggregators select information in general and news in particular; in recommender systems, they suggest music and video entertainment or influence one’s choice of products. They may also suggest friends, partners, and travel routes. Moreover, they are used to automatically produce news articles and messages in social media, to calculate scoring of content and people, and to observe behavior and interests as well as to predict and shape future needs and actions. Although their modes of operation differ in detail, all these applications share a common defining functionality: They automatically select information elements and assign relevance to them.},
	type         = {SSRN Scholarly Paper},
	language     = {en}
}
@phdthesis{elijahthesis,
	title        = {Defensible Explanations for Algorithmic Decsions about Writing in Education},
	author       = {Mayfield, Elijah},
	year         = 2020,
	school       = {Carnegie Mellon University}
}
@article{Citron_2008,
	title        = {Technological Due Process},
	author       = {Citron, Danielle Keats},
	year         = 2008,
	month        = jan,
	journal      = {Washington University Law Review},
	volume       = 85,
	number       = 6,
	pages        = {1249–1313},
	issn         = {2166-7993 (Print) </p><p>ISSN: 2166-8000 (Online)}
}
@article{Kaashoek2024Impact,
	title        = {The {Impact} of {Generative} {AI} on {Labor} {Market} {Matching}},
	author       = {Kaashoek, Justin and Raghavan, Manish and Horton, John J.},
	year         = 2024,
	journal      = {An MIT Exploration of Generative AI},
	publisher    = {MIT},
	note         = {https://mit-genai.pubpub.org/pub/4t8pqt06}
}
@book{Olsaretti_2018,
	title        = {The Oxford Handbook of Distributive Justice},
	author       = {Olsaretti, Serena},
	year         = 2018,
	month        = may,
	publisher    = {Oxford University Press},
	doi          = {10.1093/oxfordhb/9780199645121.001.0001},
	isbn         = {978-0-19-964512-1},
	url          = {https://doi.org/10.1093/oxfordhb/9780199645121.001.0001},
	abstractnote = {This volume collects previously unpublished work on distributive justice written by leading political philosophers. Its aim is to provide a wide-ranging overview of the central contemporary debates—some more familiar than others—concerning distributive justice. The volume opens with an introduction and its thirty-two chapters are divided into four parts. The chapters of Part I introduce readers to the main contemporary approaches to distributive justice; those of Part II discuss the relation between distributive justice and some other social virtues; and those of Part III address some central foundational issues that affect our thinking about justice. Finally, the chapters of Part IV examine the implications of distributive justice for some key aspects of social life.}
}
@misc{lahoti2019ifairlearningindividuallyfair,
	title        = {iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making},
	author       = {Preethi Lahoti and Krishna P. Gummadi and Gerhard Weikum},
	year         = 2019,
	url          = {https://arxiv.org/abs/1806.01059},
	eprint       = {1806.01059},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{pmlr-v28-zemel13,
	title        = {Learning Fair Representations},
	author       = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
	year         = 2013,
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	volume       = 28,
	number       = 3,
	pages        = {325--333},
	url          = {https://proceedings.mlr.press/v28/zemel13.html},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/zemel13.pdf},
	abstract     = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}
@article{10.1111/j.1467-954X.2007.00740.x,
	title        = {Scorecards as devices for consumer credit: the case of Fair, Isaac \& Company Incorporated},
	author       = {Poon, Martha},
	year         = 2007,
	journal      = {The Sociological Review},
	volume       = 55,
	number       = {s2},
	pages        = {284--306},
	doi          = {https://doi.org/10.1111/j.1467-954X.2007.00740.x},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-954X.2007.00740.x},
	eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-954X.2007.00740.x}
}
@article{10.1145/242485.242493,
	title        = {Value-sensitive design},
	author       = {Friedman, Batya},
	year         = 1996,
	month        = dec,
	journal      = {Interactions},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 6,
	pages        = {16–23},
	doi          = {10.1145/242485.242493},
	issn         = {1072-5520},
	url          = {https://doi.org/10.1145/242485.242493},
	issue_date   = {Nov./Dec. 1996},
	numpages     = 8
}
@inproceedings{10.1145/3351095.3372867,
	title        = {Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organisational reputation},
	author       = {Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L\"{u}nich, Marco},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	location     = {Barcelona, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {FAT* '20},
	pages        = {122–130},
	doi          = {10.1145/3351095.3372867},
	isbn         = 9781450369367,
	url          = {https://doi.org/10.1145/3351095.3372867},
	abstract     = {Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organisational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organisational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.},
	numpages     = 9,
	keywords     = {algorithmic decision making, artificial intelligence, distributive fairness, exit, higher education systems, procedural fairness, reputation, voice}
}
@article{Goodman_Flaxman_2017,
	title        = {European Union regulations on algorithmic decision-making and a “right to explanation”},
	author       = {Goodman, Bryce and Flaxman, Seth},
	year         = 2017,
	month        = sep,
	journal      = {AI Magazine},
	volume       = 38,
	number       = 3,
	pages        = {50–57},
	doi          = {10.1609/aimag.v38i3.2741},
	issn         = {0738-4602, 2371-9621},
	note         = {arXiv:1606.08813 [cs, stat]},
	abstractnote = {We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.}
}
@inproceedings{pmlr-v80-kearns18a,
	title        = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	author       = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	year         = 2018,
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {2564--2572},
	url          = {https://proceedings.mlr.press/v80/kearns18a.html},
	editor       = {Dy, Jennifer and Krause, Andreas},
	pdf          = {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
	abstract     = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}
@article{Peng_Nushi_Kıcıman_Inkpen_Suri_Kamar_2019,
	title        = {What You See Is What You Get? The Impact of Representation Criteria on Human Bias in Hiring},
	author       = {Peng, Andi and Nushi, Besmira and Kıcıman, Emre and Inkpen, Kori and Suri, Siddharth and Kamar, Ece},
	year         = 2019,
	journal      = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	volume       = 7,
	number       = 1,
	pages        = {125--134},
	doi          = {10.1609/hcomp.v7i1.5281},
	url          = {https://ojs.aaai.org/index.php/HCOMP/article/view/5281},
	abstractnote = {&lt;p&gt;Although systematic biases in decision-making are widely documented, the ways in which they emerge from different sources is less understood. We present a controlled experimental platform to study gender bias in hiring by decoupling the effect of world distribution (the gender breakdown of candidates in a specific profession) from bias in human decision-making. We explore the effectiveness of &lt;em&gt;representation criteria&lt;/em&gt;, fixed proportional display of candidates, as an intervention strategy for mitigation of gender bias by conducting experiments measuring human decision-makers’ rankings for who they would recommend as potential hires. Experiments across professions with varying gender proportions show that balancing gender representation in candidate slates can correct biases for some professions where the world distribution is skewed, although doing so has no impact on other professions where human persistent preferences are at play. We show that the gender of the decision-maker, complexity of the decision-making task and over- and under-representation of genders in the candidate slate can all impact the final decision. By decoupling sources of bias, we can better isolate strategies for bias mitigation in human-in-the-loop systems.&lt;/p&gt;}
}
@inproceedings{Leake2001ArtiicialI,
	title        = {Artiicial Intelligence},
	author       = {David B. Leake},
	year         = 2001,
	url          = {https://api.semanticscholar.org/CorpusID:18409035}
}
@inproceedings{wang2008you,
	title        = {What Do You Mean by" AI"?},
	author       = {Wang, Pei},
	year         = 2008,
	booktitle    = {AGI},
	volume       = 171,
	pages        = {362--373}
}
@article{du2020ai,
	title        = {What is AI? Applications of artificial intelligence to dermatology},
	author       = {Du-Harpur, X and Watt, FM and Luscombe, NM and Lynch, MD},
	year         = 2020,
	journal      = {British Journal of Dermatology},
	publisher    = {Blackwell Publishing Ltd Oxford, UK},
	volume       = 183,
	number       = 3,
	pages        = {423--430}
}
@misc{natarajan_binns_2022,
	title        = {Misleading AI Explanations},
	author       = {Natarajan, Neil and Binns, Reuben},
	year         = 2022,
	month        = aug,
	publisher    = {OSF},
	doi          = {10.17605/OSF.IO/MQ86P},
	url          = {osf.io/mq86p}
}
@article{nemhauser_analysis_1978,
	title        = {An analysis of approximations for maximizing submodular set functions—{I}},
	author       = {Nemhauser, G. L. and Wolsey, L. A. and Fisher, M. L.},
	year         = 1978,
	month        = dec,
	journal      = {Mathematical Programming},
	volume       = 14,
	number       = 1,
	pages        = {265--294},
	doi          = {10.1007/BF01588971},
	issn         = {0025-5610, 1436-4646},
	url          = {http://link.springer.com/10.1007/BF01588971},
	urldate      = {2023-02-15},
	abstract     = {LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)≥z(S⋃T)+z(S⋂T) for allS, T inN. Such a function is called submodular. We consider the problem maxS⊂N\{a(S):{\textbar}S{\textbar}≤K,z(S) submodular\}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a “greedy” heuristic always produces a solution whose value is at least 1 −[(K − 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e − 1)/e, where e is the base of the natural logarithm.},
	language     = {en}
}
@article{nemhauser1978analysis,
	title        = {An analysis of approximations for maximizing submodular set functions---I},
	author       = {Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
	year         = 1978,
	journal      = {Mathematical programming},
	publisher    = {Springer},
	volume       = 14,
	pages        = {265--294},
	date-added   = {2023-09-11 11:18:10 -0400},
	date-modified = {2023-09-11 11:18:10 -0400}
}
@article{newman2009recruitment,
	title        = {Recruitment efforts to reduce adverse impact: Targeted recruiting for personality, cognitive ability, and diversity.},
	author       = {Newman, Daniel A and Lyon, Julie S},
	year         = 2009,
	journal      = {Journal of applied psychology},
	publisher    = {American Psychological Association},
	volume       = 94,
	number       = 2,
	pages        = 298
}
@inproceedings{niculescu-mizil_predicting_2005,
	title        = {Predicting good probabilities with supervised learning},
	author       = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year         = 2005,
	month        = aug,
	booktitle    = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{ICML} '05},
	pages        = {625--632},
	doi          = {10.1145/1102351.1102430},
	isbn         = {978-1-59593-180-1},
	url          = {https://doi.org/10.1145/1102351.1102430},
	urldate      = {2024-02-19},
	abstract     = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.}
}
@article{nkomo2019diversity,
	title        = {Diversity at a critical juncture: New theories for a complex phenomenon},
	author       = {Nkomo, Stella M and Bell, Myrtle P and Roberts, Laura Morgan and Joshi, Aparna and Thatcher, Sherry MB},
	year         = 2019,
	journal      = {Academy of Management Review},
	publisher    = {Academy of Management Briarcliff Manor, NY},
	volume       = 44,
	number       = 3,
	pages        = {498--517}
}
@book{nkwake_credibility_2015,
	title        = {Credibility, {Validity}, and {Assumptions} in {Program} {Evaluation} {Methodology}},
	author       = {Nkwake, Apollo and Mayne, John},
	year         = 2015,
	month        = jan,
	publisher    = {Springer},
	doi          = {10.1007/978-3-319-19021-1},
	isbn         = {978-3-319-19020-4},
	note         = {Journal Abbreviation: Credibility, Validity, and Assumptions in Program Evaluation Methodology Pages: 166 Publication Title: Credibility, Validity, and Assumptions in Program Evaluation Methodology},
	abstract     = {This book focuses on assumptions underlyingmethods choice in program evaluation. Credible program evaluation extends beyond the accuracy of research designs toinclude arguments justifying the appropriateness of methods. An important part of this justification is explaining the assumptions made about the validity of methods. This book provides a framework for understanding methodological assumptions, identifying the decisions made at each stage of the evaluation process, the major forms of validity affected by those decisions, and the preconditions for andassumptions about those validities.Though the selection of appropriate research methodology is not a new topic within social development research, previouspublications suggest only advantages and disadvantagesof using various methods and when to use them. Thisbook goes beyond other publications to analyze the assumptions underlying actual methodological choicesinevaluation studies and how these eventually influence evaluation quality.The analysis offered issupported by a collation of assumptions collected from acase study of 34 evaluations. Due to its in-depth analysis, strong theoretical basis, andpractice examples, Credibility, Validity and Assumptions is amust-have resource for researchers, students, university professors and practitioners in program evaluation.Importantly, it provides tools for the application ofappropriate research methods in program evaluation},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/AISL65I8/Nkwake and Mayne - 2015 - Credibility, Validity, and Assumptions in Program .pdf:application/pdf},
	date-added   = {2023-09-11 11:19:07 -0400},
	date-modified = {2023-09-11 11:19:07 -0400}
}
@techreport{noray2023systemic,
	title        = {Communication and Systemic Disadvantage: Evidence from the Rise in Social Skills},
	author       = {Noray, Kadeem and Noray, Savannah},
	year         = 2023,
	journal      = {Working Paper},
	institution  = {Harvard University}
}
@article{nourani_effects_2019,
	title        = {The {Effects} of {Meaningful} and {Meaningless} {Explanations} on {Trust} and {Perceived} {System} {Accuracy} in {Intelligent} {Systems}},
	author       = {Nourani, Mahsan and Kabir, Samia and Mohseni, Sina and Ragan, Eric D.},
	year         = 2019,
	month        = oct,
	journal      = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	volume       = 7,
	pages        = {97--105},
	doi          = {10.1609/hcomp.v7i1.5284},
	issn         = {2769-1349},
	url          = {https://ojs.aaai.org/index.php/HCOMP/article/view/5284},
	urldate      = {2023-12-05},
	copyright    = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	abstract     = {Machine learning and artificial intelligence algorithms can assist human decision making and analysis tasks. While such technology shows promise, willingness to use and rely on intelligent systems may depend on whether people can trust and understand them. To address this issue, researchers have explored the use of explainable interfaces that attempt to help explain why or how a system produced the output for a given input. However, the effects of meaningful and meaningless explanations (determined by their alignment with human logic) are not properly understood, especially with users who are non-experts in data science. Additionally, we wanted to explore how explanation inclusion and level of meaningfulness would affect the user’s perception of accuracy. We designed a controlled experiment using an image classification scenario with local explanations to evaluate and better understand these issues. Our results show that whether explanations are human-meaningful can significantly affect perception of a system’s accuracy independent of the actual accuracy observed from system usage. Participants significantly underestimated the system’s accuracy when it provided weak, less human-meaningful explanations. Therefore, for intelligent systems with explainable interfaces, this research demonstrates that users are less likely to accurately judge the accuracy of algorithms that do not operate based on human-understandable rationale.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/QZXQRVFQ/Nourani et al. - 2019 - The Effects of Meaningful and Meaningless Explanat.pdf:application/pdf}
}
@article{noy2023experimental,
	title        = {Experimental evidence on the productivity effects of generative artificial intelligence},
	author       = {Noy, Shakked and Zhang, Whitney},
	year         = 2023,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 381,
	number       = 6654,
	pages        = {187--192}
}
@article{nyrup_water_2020,
	title        = {‘{Of} {Water} {Drops} and {Atomic} {Nuclei}: {Analogies} and {Pursuit} {Worthiness} in {Science}’},
	shorttitle   = {‘{Of} {Water} {Drops} and {Atomic} {Nuclei}},
	author       = {Nyrup, Rune},
	year         = 2020,
	month        = sep,
	journal      = {The British Journal for the Philosophy of Science},
	volume       = 71,
	number       = 3,
	pages        = {881--903},
	doi          = {10.1093/bjps/axy036},
	issn         = {0007-0882},
	url          = {https://www.journals.uchicago.edu/doi/full/10.1093/bjps/axy036},
	urldate      = {2021-10-15},
	note         = {Publisher: The University of Chicago Press},
	abstract     = {
		This article highlights a use of analogies in science that so far has received relatively little systematic discussion: providing reasons for pursuing a model or theory. Using the development of the liquid drop model as a test case, I critically assess two extant pursuit worthiness accounts: (i) that analogies justify pursuit by supporting plausibility arguments and (ii) that analogies can serve as a guide to potential theoretical unification. Neither of these fit the liquid drop model case. Instead, I develop an alternative account, based on the idea that analogies facilitate the transfer of a well-understood modelling strategy to a new domain.

		1.  Introduction

		2.  Case Study: The Development of the Liquid Drop Model

		3.  Plausibility Accounts

		3.1.  Bartha on plausibility and analogical inference

		3.2.  Plausibility and the drop analogy

		4.  Analogies as a Guide to Unification

		5.  Generative Accounts

		5.1.  Analogy-based modelling strategies

		5.2.  Did analogies play a merely generative role?

		6.  A New Pursuit Worthiness Account of Analogies

		6.1.  Transferring understanding-with through analogies

		6.2.  Understanding-with and the liquid drop model

		7.  Conclusion
	},
	file         = {Nyrup_2020_‘Of Water Drops and Atomic Nuclei.pdf:/Users/neilnatarajan/Zotero/storage/JT36FMWN/Nyrup_2020_‘Of Water Drops and Atomic Nuclei.pdf:application/pdf}
}
@misc{openai_gpt-4_2023,
	title        = {{GPT}-4 {Technical} {Report}},
	author       = {OpenAI},
	year         = 2023,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2303.08774},
	url          = {http://arxiv.org/abs/2303.08774},
	urldate      = {2023-08-31},
	note         = {arXiv:2303.08774 [cs]},
	abstract     = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/P7VNGEEB/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LCHF5QMU/2303.html:text/html}
}
@article{oprea_what_2020,
	title        = {What {Makes} a {Rule} {Complex}?},
	author       = {Oprea, Ryan},
	year         = 2020,
	month        = dec,
	journal      = {American Economic Review},
	volume       = 110,
	number       = 12,
	pages        = {3913--3951},
	doi          = {10.1257/aer.20191717},
	issn         = {0002-8282},
	url          = {https://www.aeaweb.org/articles?id=10.1257/aer.20191717},
	urldate      = {2024-02-28},
	abstract     = {We study the complexity of rules by paying experimental subjects to implement a series of algorithms and then eliciting their willingness-to-pay to avoid implementing them again in the future. The design allows us to examine hypotheses from the theoretical "automata" literature about the characteristics of rules that generate complexity costs. We find substantial aversion to complexity and a number of regularities in the characteristics of rules that make them complex and costly for subjects. Experience with a rule, the way a rule is represented, and the context in which a rule is implemented (mentally versus physically) also influence complexity.},
	language     = {en},
	keywords     = {Belief, Communication, Evolutionary Games, Information and Knowledge, Learning, Repeated Games, Consumer Economics: Theory, Consumer Economics: Empirical Analysis, Search, Stochastic and Dynamic Games, Unawareness, Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making},
	file         = {Oprea - 2020 - What Makes a Rule Complex.pdf:/Users/neilnatarajan/Zotero/storage/JPT7VD4U/Oprea - 2020 - What Makes a Rule Complex.pdf:application/pdf}
}
@article{organisciak_beyond_2023,
	title        = {Beyond semantic distance: {Automated} scoring of divergent thinking greatly improves with large language models},
	shorttitle   = {Beyond semantic distance},
	author       = {Organisciak, Peter and Acar, Selcuk and Dumas, Denis and Berthiaume, Kelly},
	year         = 2023,
	month        = sep,
	journal      = {Thinking Skills and Creativity},
	volume       = 49,
	pages        = 101356,
	doi          = {10.1016/j.tsc.2023.101356},
	issn         = {1871-1871},
	url          = {https://www.sciencedirect.com/science/article/pii/S1871187123001256},
	urldate      = {2024-03-04},
	abstract     = {Automated scoring for divergent thinking (DT) seeks to overcome a key obstacle to creativity measurement: the effort, cost, and reliability of scoring open-ended tests. For a common test of DT, the Alternate Uses Task (AUT), the primary automated approach casts the problem as a semantic distance between a prompt and the resulting idea in a text model. This work presents an alternative approach that greatly surpasses the performance of the best existing semantic distance approaches. Our system, Ocsai, fine-tunes deep neural network-based large-language models (LLMs) on human-judged responses. Trained and evaluated against one of the largest collections of human-judged AUT responses, with 27 thousand responses collected from nine past studies, our fine-tuned large-language-models achieved up to r = 0.81 correlation with human raters, greatly surpassing current systems (r = 0.12–0.26). Further, learning transfers well to new test items and the approach is still robust with small numbers of training labels. We also compare prompt-based zero-shot and few-shot approaches, using GPT-3, ChatGPT, and GPT-4. This work also suggests a limit to the underlying assumptions of the semantic distance model, showing that a purely semantic approach that uses the stronger language representation of LLMs, while still improving on existing systems, does not achieve comparable improvements to our fine-tuned system. The increase in performance can support stronger applications and interventions in DT and opens the space of automated DT scoring to new areas for improving and understanding this branch of methods.},
	keywords     = {Alternate uses test, Automated scoring, Divergent thinking, Large-language models},
	file         = {Organisciak et al. - 2023 - Beyond semantic distance Automated scoring of div.pdf:/Users/neilnatarajan/Zotero/storage/WRV5NCHQ/Organisciak et al. - 2023 - Beyond semantic distance Automated scoring of div.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/PI6BZSZE/S1871187123001256.html:text/html}
}
@article{ostergaard_does_2011,
	title        = {Does a different view create something new? {The} effect of employee diversity on innovation},
	shorttitle   = {Does a different view create something new?},
	author       = {Østergaard, Christian R. and Timmermans, Bram and Kristinsson, Kari},
	year         = 2011,
	month        = apr,
	journal      = {Research Policy},
	volume       = 40,
	number       = 3,
	pages        = {500--509},
	doi          = {10.1016/j.respol.2010.11.004},
	issn         = {0048-7333},
	url          = {https://www.sciencedirect.com/science/article/pii/S0048733310002398},
	urldate      = {2022-08-28},
	abstract     = {A growing literature is analysing the relation between diversity in the knowledge base and the performance of firms; nevertheless, studies that investigate the impact of employee diversity on innovation are scarce. Innovation is an interactive process that often involves communication and interaction among employees in a firm and draws on their different qualities from all levels of the organisation. This paper investigates the relation between employee diversity and innovation in terms of gender, age, ethnicity, and education. The analyses draw on data from a recent innovation survey. This data is merged with a linked employer–employee dataset that allow us to identify the employee composition of each firm. We test the hypothesis that employee diversity is associated with better innovative performance. The econometric analysis reveals a positive relation between diversity in education and gender on the likelihood of introducing an innovation. Furthermore, we find a negative effect of age diversity and no significant effect of ethnicity on the firm's likelihood to innovate. In addition, the logistic regression reveals a positive relationship between an open culture towards diversity and innovative performance. We find no support of any curvilinear relation between diversity and innovation.},
	language     = {en},
	keywords     = {Diversity, Education, Ethnicity, Gender, Innovation},
	file         = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/DDZXCUKX/Østergaard et al. - 2011 - Does a different view create something new The ef.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/T6UUJC7P/S0048733310002398.html:text/html}
}
@article{oswald_how_2021,
	title        = {How do climate change skeptics engage with opposing views? {Understanding} mechanisms of social identity and cognitive dissonance in an online forum},
	shorttitle   = {How do climate change skeptics engage with opposing views?},
	author       = {Oswald, Lisa and Bright, Jonathan},
	year         = 2021,
	month        = feb,
	journal      = {arXiv:2102.06516 [cs]},
	url          = {http://arxiv.org/abs/2102.06516},
	urldate      = {2021-12-08},
	note         = {arXiv: 2102.06516},
	abstract     = {Does engagement with opposing views help break down ideological `echo chambers'; or does it backfire and reinforce them? This question remains critical as academics, policymakers and activists grapple with the question of how to regulate political discussion on social media. In this study, we contribute to the debate by examining the impact of opposing views within a major climate change skeptic online community on Reddit. A large sample of posts (N = 3000) was manually coded as either dissonant or consonant which allowed the automated classification of the full dataset of more than 50,000 posts, with codes inferred from linked websites. We find that ideologically dissonant submissions act as a stimulant to activity in the community: they received more attention (comments) than consonant submissions, even though they received lower scores through up-voting and down-voting. Users who engaged with dissonant submissions were also more likely to return to the forum. Consistent with identity theory, confrontation with opposing views triggered activity in the forum, particularly among users that are highly engaged with the community. In light of the findings, theory of social identity and echo chambers is discussed and enhanced.},
	keywords     = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/5HAWKSMK/2102.html:text/html;Oswald_Bright_2021_How do climate change skeptics engage with opposing views.pdf:/Users/neilnatarajan/Zotero/storage/EZ7YB9BL/Oswald_Bright_2021_How do climate change skeptics engage with opposing views.pdf:application/pdf}
}
@article{otterbacher_why_2023,
	title        = {Why technical solutions for detecting {AI}-generated content in research and education are insufficient},
	author       = {Otterbacher, Jahna},
	year         = 2023,
	month        = jul,
	journal      = {Patterns},
	volume       = 4,
	number       = 7,
	pages        = 100796,
	doi          = {10.1016/j.patter.2023.100796},
	issn         = {2666-3899},
	url          = {https://www.sciencedirect.com/science/article/pii/S2666389923001514},
	urldate      = {2024-07-31},
	abstract     = {Artificial intelligence (AI)-generated content detectors are not foolproof and often introduce other problems, as shown by Desaire et al. and Liang et al. in papers published recently in Patterns and Cell Reports Physical Science. Rather than “fighting” AI with more AI, we must develop an academic culture that promotes the use of generative AI in a creative, ethical manner.},
	file         = {Full Text:/Users/neilnatarajan/Zotero/storage/WX7LGIGV/Otterbacher - 2023 - Why technical solutions for detecting AI-generated.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/PPCW7V4I/S2666389923001514.html:text/html}
}
@article{page_diversity_2017,
	title        = {The {Diversity} {Bonus}: {How} {Great} {Teams} {Pay} {Off} in the {Knowledge} {Economy}},
	author       = {Page, Scott E. and Lewis, Earl and Cantor, Nancy},
	year         = 2017,
	month        = sep,
	publisher    = {Princeton University Press},
	note         = {MAG ID: 2885284796 S2ID: e47d67055846c132fc8274170fe9adf1250a7727},
	abstract     = {How businesses and other organisations can improve their performance by tapping the power of differences in how people think What if workforce diversity is more than simply the right thing to do in order to make society more integrated and just? What if diversity can also improve the bottom line of businesses and other organisations facing complex challenges in the knowledge economy? It can. And The Diversity Bonus shows how and why. Scott Page, a leading thinker, writer, and speaker whose ideas and advice are sought after by corporations, nonprofits, universities, and governments around the world, makes a clear and compellingly pragmatic case for diversity and inclusion. He presents overwhelming evidence that teams that include different kinds of thinkers outperform homogenous groups on complex tasks, producing what he calls diversity bonuses. These bonuses include improved problem solving, increased innovation, and more accurate predictions all of which lead to better performance and results. Page shows that various types of cognitive diversity differences in how people perceive, encode, analyze, and organize the same information and experiencesare linked to better outcomes. He then describes how these cognitive differences are influenced by other kinds of diversity, including racial and gender differencesin other words, identity diversity. Identity diversity, therefore, can also produce bonuses. Drawing on research in economics, psychology, computer science, and many other fields, The Diversity Bonus also tells the stories of people and organisations that have tapped the power of diversity to solve complex problems. And the book includes a challenging response from Katherine Phillips of the Columbia Business School. The result changes the way we think about diversity in the workplaceand far beyond it.},
	date-added   = {2023-09-11 11:23:17 -0400},
	date-modified = {2023-09-11 11:23:17 -0400}
}
@article{page_diversity_2010,
	title        = {Diversity and {Complexity}},
	author       = {Page, Scott E.},
	year         = 2010,
	month        = nov,
	publisher    = {Princeton University Press},
	note         = {MAG ID: 1653902796},
	abstract     = {This book provides an introduction to the role of diversity in complex adaptive systems. A complex system--such as an economy or a tropical ecosystem--consists of interacting adaptive entities that produce dynamic patterns and structures. Diversity plays a different role in a complex system than it does in an equilibrium system, where it often merely produces variation around the mean for performance measures. In complex adaptive systems, diversity makes fundamental contributions to system performance. Scott Page gives a concise primer on how diversity happens, how it is maintained, and how it affects complex systems. He explains how diversity underpins system level robustness, allowing for multiple responses to external shocks and internal adaptations; how it provides the seeds for large events by creating outliers that fuel tipping points; and how it drives novelty and innovation. Page looks at the different kinds of diversity--variations within and across types, and distinct community compositions and interaction structures--and covers the evolution of diversity within complex systems and the factors that determine the amount of maintained diversity within a system.Provides a concise and accessible introduction Shows how diversity underpins robustness and fuels tipping points Covers all types of diversity The essential primer on diversity in complex adaptive systems},
	date-added   = {2023-09-11 11:22:14 -0400},
	date-modified = {2023-09-11 11:22:14 -0400}
}
@book{page2008difference,
	title        = {The difference: How the power of diversity creates better groups, firms, schools, and societies-new edition},
	author       = {Page, Scott},
	year         = 2008,
	publisher    = {Princeton University Press},
	date-added   = {2023-09-11 11:21:19 -0400},
	date-modified = {2023-09-11 11:21:19 -0400}
}
@article{pallais2014inefficient,
	title        = {Inefficient hiring in entry-level labor markets},
	author       = {Pallais, Amanda},
	year         = 2014,
	journal      = {American Economic Review},
	volume       = 104,
	number       = 11,
	pages        = {3565--99}
}
@inproceedings{pandey_applicants_2022,
	title        = {Applicants' {Perception} {Towards} the {Application} of {AI} in {Recruitment} {Process}},
	author       = {Pandey, Suruchi and Bahukhandi, Medha},
	year         = 2022,
	month        = feb,
	booktitle    = {2022 {Interdisciplinary} {Research} in {Technology} and {Management} ({IRTM})},
	pages        = {1--6},
	doi          = {10.1109/IRTM54583.2022.9791587},
	abstract     = {An organisation or business needs professional employees to achieve its targets in order to succeed in this dynamic period. They are now at the forefront of the fourth technological revolution. In this new era, everybody needs strong, future and creative workers to stay successful. To order to navigate the new landscape and improve the market climate, companies with a successful management plan will include an acceptable employee. Recruitment strategy is the primary consideration for any company to employ skilled employees who are willing to accomplish their job objectives more efficiently and effectively. Clearly, the recruiting approach as a core feature of the company is focused on data collection for decisionmaking. This paper discusses artificial intelligence (AI) and the effect it has on the recruiting industry. This thesis explored the effect of AI on managers and applicants during the first phases of the recruiting process.},
	keywords     = {Artificial Intelligence, Artificial intelligence, Recruitment, AI, Brand management, Companies, Data collection, Employee, Industries, Navigation, Perception, Standards organisations},
	file         = {IEEE Xplore Abstract Record:/Users/neilnatarajan/Zotero/storage/3KIW4ZZE/9791587.html:text/html;Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process_annotated.pdf:/Users/neilnatarajan/Zotero/storage/VPTUKG3U/Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process_annotated.pdf:application/pdf;Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process.pdf:/Users/neilnatarajan/Zotero/storage/VPTUKG3U/Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process.pdf:application/pdf}
}
@article{hanley1989receiver,
	title        = {Receiver operating characteristic (ROC) methodology: the state of the art},
	author       = {Hanley, John A and others},
	year         = 1989,
	journal      = {Crit Rev Diagn Imaging},
	volume       = 29,
	number       = 3,
	pages        = {307--335}
}
@article{VanderSchee2022UsingCP,
	title        = {Using cross-course peer grading with content expertise, anonymity, and perceived justice},
	author       = {Brian A Vander Schee and Tony Stovall and Demetra Andrews},
	year         = 2022,
	journal      = {Active Learning in Higher Education},
	volume       = 25,
	pages        = {101--114},
	url          = {https://api.semanticscholar.org/CorpusID:248899447}
}
@article{Anvari2021EffectivenessOP,
	title        = {Effectiveness of Peer Review in Teaching and Learning User Centered Conceptual Design Among Large Cohorts of Information Technology Students},
	author       = {Farshid Anvari and Hien Minh Thi Tran and Deborah Richards},
	year         = 2021,
	journal      = {2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)},
	pages        = {66--77},
	url          = {https://api.semanticscholar.org/CorpusID:235639168}
}
@article{Rahmatillah2022AnalyzingFA,
	title        = {Analyzing Factors Affecting the Effectiveness of Peer Assessment in EFL Teaching},
	author       = {Refanja Rahmatillah and Rizki Fajrita},
	year         = 2022,
	journal      = {Indonesian Journal of Teaching and Teacher Education},
	url          = {https://api.semanticscholar.org/CorpusID:256757919}
}
@book{Dassin_Marsh_Mawer_2018,
	title        = {International Scholarships in Higher Education},
	year         = 2018,
	publisher    = {Springer International Publishing},
	address      = {Cham},
	doi          = {10.1007/978-3-319-62734-2},
	isbn         = {978-3-319-62733-5},
	url          = {http://link.springer.com/10.1007/978-3-319-62734-2},
	rights       = {http://www.springer.com/tdm},
	language     = {en},
	author		 = {Dassin, Joan R. and Marsh, Robin R. and Mawer, Matt}
}
@article{DilraboJonbekova_Ruby_2023,
	title        = {How international higher education graduates contribute to their home country: an example from government scholarship recipients in Kazakhstan},
	author       = {Dilrabo Jonbekova and Zakir Jumakulov and Yevgeniya Serkova and Zhanar Mazbulova and Alan Ruby},
	year         = 2023,
	journal      = {Higher Education Research and Development},
	publisher    = {Routledge},
	volume       = 42,
	number       = 1,
	pages        = {126–140},
	doi          = {10.1080/07294360.2021.2019200}
}
@book{good2013permutation,
	title        = {Permutation tests: a practical guide to resampling methods for testing hypotheses},
	author       = {Good, Phillip},
	year         = 2013,
	publisher    = {Springer Science \& Business Media}
}
@article{Kim_2015,
	title        = {Statistical notes for clinical researchers: post-hoc multiple comparisons},
	author       = {Kim, Hae-Young},
	year         = 2015,
	month        = may,
	journal      = {Restorative Dentistry and Endodontics},
	volume       = 40,
	number       = 2,
	pages        = {172–176},
	doi          = {10.5395/rde.2015.40.2.172},
	issn         = {2234-7658}
}
@article{Schober_Boer_Schwarte_2018,
	title        = {Correlation Coefficients: Appropriate Use and Interpretation},
	author       = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
	year         = 2018,
	month        = may,
	journal      = {Anesthesia and Analgesia},
	volume       = 126,
	number       = 5,
	pages        = {1763–1768},
	doi          = {10.1213/ANE.0000000000002864},
	issn         = {1526-7598},
	abstractnote = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from -1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
	language     = {eng}
}
@article{Mishra_Singh_Pandey_Mishra_Pandey_2019,
	title        = {Application of Student’s t-test, Analysis of Variance, and Covariance},
	author       = {Mishra, Prabhaker and Singh, Uttam and Pandey, Chandra M and Mishra, Priyadarshni and Pandey, Gaurav},
	year         = 2019,
	journal      = {Annals of Cardiac Anaesthesia},
	volume       = 22,
	number       = 4,
	pages        = {407–411},
	doi          = {10.4103/aca.ACA_94_19},
	issn         = {0971-9784},
	abstractnote = {Student’s t test (t test), analysis of variance (ANOVA), and analysis of covariance (ANCOVA) are statistical methods used in the testing of hypothesis for comparison of means between the groups. The Student’s t test is used to compare the means between two groups, whereas ANOVA is used to compare the means among three or more groups. In ANOVA, first gets a common P value. A significant P value of the ANOVA test indicates for at least one pair, between which the mean difference was statistically significant. To identify that significant pair(s), we use multiple comparisons. In ANOVA, when using one categorical independent variable, it is called one-way ANOVA, whereas for two categorical independent variables, it is called two-way ANOVA. When using at least one covariate to adjust with dependent variable, ANOVA becomes ANCOVA. When the size of the sample is small, mean is very much affected by the outliers, so it is necessary to keep sufficient sample size while using these methods.}
}
@article{Arslan_formr_2019,
	title        = {formr: A study framework allowing for automated feedback generation and complex longitudinal experience-sampling studies using R},
	author       = {Arslan, Ruben and Walther, Matthias and Tata, Cyril},
	year         = 2019,
	month        = {04},
	journal      = {Behavior Research Methods},
	volume       = 52,
	doi          = {10.3758/s13428-019-01236-y}
}
@article{papenmeier_its_2022,
	title        = {It’s {Complicated}: {The} {Relationship} between {User} {Trust}, {Model} {Accuracy} and {Explanations} in {AI}},
	shorttitle   = {It’s {Complicated}},
	author       = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
	year         = 2022,
	month        = aug,
	journal      = {ACM Transactions on Computer-Human Interaction},
	volume       = 29,
	number       = 4,
	pages        = {1--33},
	doi          = {10.1145/3495013},
	issn         = {1073-0516, 1557-7325},
	url          = {https://dl.acm.org/doi/10.1145/3495013},
	urldate      = {2023-12-05},
	abstract     = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier’s accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results.},
	language     = {en},
	file         = {Papenmeier et al. - 2022 - It’s Complicated The Relationship between User Tr.pdf:/Users/neilnatarajan/Zotero/storage/4LMCZAYI/Papenmeier et al. - 2022 - It’s Complicated The Relationship between User Tr.pdf:application/pdf}
}
@misc{parrish_single-turn_2022,
	title        = {Single-{Turn} {Debate} {Does} {Not} {Help} {Humans} {Answer} {Hard} {Reading}-{Comprehension} {Questions}},
	author       = {Parrish, Alicia and Trivedi, Harsh and Perez, Ethan and Chen, Angelica and Nangia, Nikita and Phang, Jason and Bowman, Samuel R.},
	year         = 2022,
	month        = apr,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2204.05212},
	urldate      = {2022-08-03},
	note         = {arXiv:2204.05212 [cs]},
	abstract     = {Current QA systems can generate reasonable-sounding yet false answers without explanation or evidence for the generated answer, which is especially problematic when humans cannot readily check the model's answers. This presents a challenge for building trust in machine learning systems. We take inspiration from real-world situations where difficult questions are answered by considering opposing sides (see Irving et al., 2018). For multiple-choice QA examples, we build a dataset of single arguments for both a correct and incorrect answer option in a debate-style set-up as an initial step in training models to produce explanations for two candidate answers. We use long contexts -- humans familiar with the context write convincing explanations for pre-selected correct and incorrect answers, and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer. We do not find that explanations in our set-up improve human accuracy, but a baseline condition shows that providing human-selected text snippets does improve accuracy. We use these findings to suggest ways of improving the debate set up for future data collection efforts.},
	keywords     = {Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/MFCPFMUZ/Parrish et al. - 2022 - Single-Turn Debate Does Not Help Humans Answer Har.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/PY666PZT/2204.html:text/html}
}
@book{pearl_causal_2016,
	title        = {Causal inference in statistics: a primer},
	shorttitle   = {Causal inference in statistics},
	author       = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	year         = 2016,
	publisher    = {Wiley},
	address      = {Chichester, West Sussex},
	isbn         = {978-1-119-18684-7},
	language     = {en},
	keywords     = {Causation, Mathematical statistics, Probabilities},
	file         = {Pearl et al_2016_Causal inference in statistics.pdf:/Users/neilnatarajan/Zotero/storage/53URLYGC/Pearl et al_2016_Causal inference in statistics.pdf:application/pdf}
}
@misc{pearl_book_nodate,
	title        = {The {Book} of {Why}},
	author       = {Pearl, Judea and Mackenzie, Dana},
	url          = {https://www.penguin.co.uk/books/289825/the-book-of-why/9780141982410},
	urldate      = {2021-11-25},
	abstract     = {The hugely influential book on how the understanding of causality revolutionized science and the world, by the pioneer of artificial intelligence 'Wonderful ... illuminating and fun to read' Daniel Kahneman, Nobel Prize-winner and author of Thinking, Fast and Slow 'Correlation does not imply causation.' For decades, this mantra was invoked by scientists in order to avoid taking positions as to whether one thing caused another, such as smoking and cancer, or carbon dioxide and global warming. But today, that taboo is dead. The causal revolution, sparked by world-renowned computer scientist Judea Pearl and his colleagues, has cut through a century of confusion and placed cause and effect on a firm scientific basis. Now, Pearl and science journalist Dana Mackenzie explain causal thinking to general readers for the first time, showing how it allows us to explore the world that is and the worlds that could have been. It is the essence of human and artificial intelligence. And just as Pearl's discoveries have enabled machines to think better, The Book of Why explains how we too can think better. 'Pearl's accomplishments over the last 30 years have provided the theoretical basis for progress in artificial intelligence and have redefined the term "thinking machine"' Vint Cerf},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/PZ2NS99C/9780141982410.html:text/html}
}
@article{pearl_-calculus_2012,
	title        = {The {Do}-{Calculus} {Revisited}},
	author       = {Pearl, Judea},
	year         = 2012,
	month        = oct,
	journal      = {arXiv:1210.4852 [cs, stat]},
	url          = {http://arxiv.org/abs/1210.4852},
	urldate      = {2021-11-25},
	note         = {arXiv: 1210.4852},
	abstract     = {The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl\_papers.html},
	keywords     = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/HVMV2WQD/1210.html:text/html;Pearl_2012_The Do-Calculus Revisited.pdf:/Users/neilnatarajan/Zotero/storage/Y4WUAN8H/Pearl_2012_The Do-Calculus Revisited.pdf:application/pdf}
}
@article{peters2021hidden,
	title        = {Hidden figures: epistemic costs and benefits of detecting (invisible) diversity in science},
	author       = {Peters, Uwe},
	year         = 2021,
	journal      = {European Journal for Philosophy of Science},
	publisher    = {Springer},
	volume       = 11,
	number       = 1,
	pages        = 33
}
@article{pieters_explanation_2011,
	title        = {Explanation and trust: what to tell the user in security and {AI}?},
	shorttitle   = {Explanation and trust},
	author       = {Pieters, Wolter},
	year         = 2011,
	month        = mar,
	journal      = {Ethics and Information Technology},
	volume       = 13,
	number       = 1,
	pages        = {53--64},
	doi          = {10.1007/s10676-010-9253-3},
	issn         = {1572-8439},
	url          = {https://doi.org/10.1007/s10676-010-9253-3},
	urldate      = {2022-01-21},
	abstract     = {There is a common problem in artificial intelligence (AI) and information security. In AI, an expert system needs to be able to justify and explain a decision to the user. In information security, experts need to be able to explain to the public why a system is secure. In both cases, an important goal of explanation is to acquire or maintain the users’ trust. In this paper, I investigate the relation between explanation and trust in the context of computing science. This analysis draws on literature study and concept analysis, using elements from system theory as well as actor-network theory. I apply the conceptual framework to both AI and information security, and show the benefit of the framework for both fields by means of examples. The main focus is on expert systems (AI) and electronic voting systems (security). Finally, I discuss consequences of the analysis for ethics in terms of (un)informed consent and dissent, and the associated division of responsibilities.},
	language     = {en},
	file         = {Springer Full Text PDF:/Users/neilnatarajan/Zotero/storage/YIIRHZIQ/Pieters - 2011 - Explanation and trust what to tell the user in se.pdf:application/pdf}
}
@incollection{pietsch_causation_2017,
	title        = {Causation, probability, and all that: {Data} science as a novel inductive paradigm},
	shorttitle   = {Causation, probability, and all that},
	author       = {Pietsch, Wolfgang ∗},
	year         = 2017,
	booktitle    = {Frontiers in {Data} {Science}},
	publisher    = {CRC Press},
	isbn         = {978-1-315-15640-8},
	note         = {Num Pages: 25},
	abstract     = {A brief survey of the literature suggests that there is anything but a consensus on the and related issues concerning the foundations of data science. The fundamental distinction between enumerative and eliminative induction is briefly introduced, the former focusing on the mere repetition of phenomena, the latter on the variation of phenomena. The chapter argues that eliminative induction provides a much more plausible and realistic picture of actual scientific practice. Moreover, an account of causation is outlined that corresponds to eliminative induction and that allows establishing the crucial distinction between relationships that are purely accidental and those that allow for prediction and manipulation. The methodological framework sketched in section "Causation" relies on the assumption of determinism, which certainly cannot be upheld for most applications of data science. The chapter sketches an objective, nonfrequency interpretation of probability that relies on symmetries in the causal structure of probabilistic phenomena to establish probability values.}
}
@article{pietsch_aspects_2015,
	title        = {Aspects of {Theory}-{Ladenness} in {Data}-{Intensive} {Science}},
	author       = {Pietsch, Wolfgang},
	year         = 2015,
	month        = dec,
	journal      = {Philosophy of Science},
	volume       = 82,
	number       = 5,
	pages        = {905--916},
	doi          = {10.1086/683328},
	issn         = {0031-8248, 1539-767X},
	url          = {https://www.journals.uchicago.edu/doi/10.1086/683328},
	urldate      = {2021-10-15},
	abstract     = {Recent claims, mainly from computer scientists, concerning a largely automated and modelfree data-intensive science have been countered by critical reactions from a number of philosophers of science. The debate suffers from a lack of detail in two respects, regarding (i) the actual methods used in data-intensive science and (ii) the specific ways in which these methods presuppose theoretical assumptions. I examine two widely-used algorithms, classificatory trees and non-parametric regression, and argue that these are theory-laden in an external sense, regarding the framing of research questions, but not in an internal sense concerning the causal structure of the examined phenomenon. With respect to the novelty of data-intensive science, I draw an analogy to exploratory as opposed to theory-directed experimentation.},
	language     = {en},
	file         = {markup:/Users/neilnatarajan/Zotero/storage/YKB4W25Z/Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:application/pdf;Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:/Users/neilnatarajan/Zotero/storage/2SS27WGB/Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:application/pdf}
}
@article{pillai_adoption_2020,
	title        = {Adoption of artificial intelligence ({AI}) for talent acquisition in {IT}/{ITeS} organisations},
	author       = {Pillai, Rajasshrie and Sivathanu, Brijesh},
	year         = 2020,
	month        = aug,
	journal      = {Benchmarking: An International Journal},
	volume       = 27,
	number       = 9,
	pages        = {2599--2629},
	doi          = {10.1108/bij-04-2020-0186},
	note         = {MAG ID: 3049223401},
	abstract     = {Human resource managers are adopting AI technology for conducting various tasks of human resource management, starting from manpower planning till employee exit. AI technology is prominently used for talent acquisition in organisations. This research investigates the adoption of AI technology for talent acquisition.,This study employs Technology-Organisation-Environment (TOE) and Task-Technology-Fit (TTF) framework and proposes a model to explore the adoption of AI technology for talent acquisition. The survey was conducted among the 562 human resource managers and talent acquisition managers with a structured questionnaire. The analysis of data was completed using PLS-SEM.,This research reveals that cost-effectiveness, relative advantage, top management support, HR readiness, competitive pressure and support from AI vendors positively affect AI technology adoption for talent acquisition. Security and privacy issues negatively influence the adoption of AI technology. It is found that task and technology characteristics influence the task technology fit of AI technology for talent acquisition. Adoption and task technology fit of AI technology influence the actual usage of AI technology for talent acquisition. It is revealed that stickiness to traditional talent acquisition methods negatively moderates the association between adoption and actual usage of AI technology for talent acquisition. The proposed model was empirically validated and revealed the predictors of adoption and actual usage of AI technology for talent acquisition.,This paper provides the predictors of the adoption of AI technology for talent acquisition, which is emerging extensively in the human resource domain. It provides vital insights to the human resource managers to benchmark AI technology required for talent acquisition. Marketers can develop their marketing plan considering the factors of adoption. It would help designers to understand the factors of adoption and design the AI technology algorithms and applications for talent acquisition. It contributes to advance the literature of technology adoption by interweaving it with the human resource domain literature on talent acquisition.,This research uniquely validates the model for the adoption of AI technology for talent acquisition using the TOE and TTF framework. It reveals the factors influencing the adoption and actual usage of AI technology for talent acquisition.},
	keywords     = {\_tablet},
	file         = {Pillai_Sivathanu_2020_Adoption of artificial intelligence (AI) for talent acquisition in IT-ITeS.pdf:/Users/neilnatarajan/Zotero/storage/RZVICFBU/Pillai_Sivathanu_2020_Adoption of artificial intelligence (AI) for talent acquisition in IT-ITeS.pdf:application/pdf}
}
@book{pinkett2023data,
	title        = {Data-driven DEI: The tools and metrics you need to measure, analyze, and improve diversity, equity, and inclusion},
	author       = {Pinkett, Randal},
	year         = 2023,
	publisher    = {John Wiley \& Sons}
}
@article{platt_probabilistic_2000,
	title        = {Probabilistic {Outputs} for {Support} {Vector} {Machines} and {Comparisons} to {Regularized} {Likelihood} {Methods}},
	author       = {Platt, John},
	year         = 2000,
	month        = jun,
	journal      = {Adv. Large Margin Classif.},
	volume       = 10,
	abstract     = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/U3HNMIGP/Platt - 2000 - Probabilistic Outputs for Support Vector Machines .pdf:application/pdf}
}
@inproceedings{poursabzi-sangdeh_manipulating_2021,
	title        = {Manipulating and {Measuring} {Model} {Interpretability}},
	author       = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Wortman Vaughan, Jennifer Wortman and Wallach, Hanna},
	year         = 2021,
	month        = may,
	booktitle    = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} '21},
	pages        = {1--52},
	doi          = {10.1145/3411764.3445315},
	isbn         = {978-1-4503-8096-6},
	url          = {https://doi.org/10.1145/3411764.3445315},
	urldate      = {2023-12-05},
	abstract     = {With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.},
	keywords     = {interpretability, human-centered machine learning, machine-assisted decision making},
	file         = {Submitted Version:/Users/neilnatarajan/Zotero/storage/SJTLWT5B/Poursabzi-Sangdeh et al. - 2021 - Manipulating and Measuring Model Interpretability.pdf:application/pdf}
}
@incollection{rader_explanations_2018,
	title        = {Explanations as {Mechanisms} for {Supporting} {Algorithmic} {Transparency}},
	author       = {Rader, Emilee and Cotter, Kelley and Cho, Janghee},
	year         = 2018,
	month        = apr,
	booktitle    = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {1--13},
	isbn         = {978-1-4503-5620-6},
	url          = {https://doi.org/10.1145/3173574.3173677},
	urldate      = {2022-01-05},
	abstract     = {Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebook's News Feed algorithm might affect participants' beliefs and judgments about the News Feed. We found that  . The explanations were less effective for helping participants evaluate the correctness of the system's output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.},
	keywords     = {algorithmic decision-making, explanations, transparency, \_tablet, User Study},
	file         = {Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:/Users/neilnatarajan/Zotero/storage/CY7PY3LM/Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:application/pdf}
}
@inproceedings{raghavan2020mitigating,
	title        = {Mitigating bias in algorithmic hiring: Evaluating claims and practices},
	shorttitle   = {Mitigating bias in algorithmic hiring},
	author       = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
	year         = 2020,
	month        = jan,
	booktitle    = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAT}* '20},
	pages        = {469--481},
	doi          = {10.1145/3351095.3372828},
	isbn         = {978-1-4503-6936-7},
	url          = {https://dl.acm.org/doi/10.1145/3351095.3372828},
	urldate      = {2023-08-01},
	date-added   = {2023-09-12 16:00:15 -0400},
	date-modified = {2023-09-12 16:00:15 -0400},
	abstract     = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
	keywords     = {algorithmic bias, algorithmic hiring, discrimination law},
	file         = {Raghavan et al_2020_Mitigating bias in algorithmic hiring_annotated.pdf:/Users/neilnatarajan/Zotero/storage/QN32Z9MK/Raghavan et al_2020_Mitigating bias in algorithmic hiring_annotated.pdf:application/pdf;Raghavan et al_2020_Mitigating bias in algorithmic hiring.pdf:/Users/neilnatarajan/Zotero/storage/QN32Z9MK/Raghavan et al_2020_Mitigating bias in algorithmic hiring.pdf:application/pdf}
}
@article{rambachan_economic_2020,
	title        = {An {Economic} {Perspective} on {Algorithmic} {Fairness}},
	author       = {Rambachan, Ashesh and Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil},
	year         = 2020,
	month        = may,
	journal      = {AEA Papers and Proceedings},
	volume       = 110,
	pages        = {91--95},
	doi          = {10.1257/pandp.20201036},
	issn         = {2574-0768, 2574-0776},
	url          = {https://pubs.aeaweb.org/doi/10.1257/pandp.20201036},
	urldate      = {2022-03-04},
	abstract     = {There are widespread concerns that the growing use of machine learning algorithms in important decisions may reproduce and reinforce existing discrimination against legally protected groups. Most of the attention to date on issues of “algorithmic bias” or “algorithmic fairness” has come from computer scientists and machine learning researchers. We argue that concerns about algorithmic fairness are at least as much about questions of how discrimination manifests itself in data, decision-making under uncertainty, and optimal regulation. To fully answer these questions, an economic framework is necessary--and as a result, economists have much to contribute.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Rambachan et al_2020_An Economic Perspective on Algorithmic Fairness.pdf:/Users/neilnatarajan/Zotero/storage/N7ZEFMZX/Rambachan et al_2020_An Economic Perspective on Algorithmic Fairness.pdf:application/pdf}
}
@article{rambachan2024identifying,
	title        = {Identifying prediction mistakes in observational data},
	author       = {Rambachan, Ashesh},
	year         = 2024,
	journal      = {The Quarterly Journal of Economics},
	publisher    = {Oxford University Press},
	pages        = {qjae013}
}
@misc{ramesh_hierarchical_2022,
	title        = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	author       = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	year         = 2022,
	month        = apr,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2204.06125},
	url          = {http://arxiv.org/abs/2204.06125},
	urldate      = {2022-08-24},
	note         = {arXiv:2204.06125 [cs]},
	abstract     = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/9836QDJ6/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/WZWW244S/2204.html:text/html}
}
@article{ramesh_automated_2022,
	title        = {An automated essay scoring systems: a systematic literature review},
	shorttitle   = {An automated essay scoring systems},
	author       = {Ramesh, Dadi and Sanampudi, Suresh Kumar},
	year         = 2022,
	month        = mar,
	journal      = {Artificial Intelligence Review},
	volume       = 55,
	number       = 3,
	pages        = {2495--2527},
	doi          = {10.1007/s10462-021-10068-2},
	issn         = {1573-7462},
	url          = {https://doi.org/10.1007/s10462-021-10068-2},
	urldate      = {2023-08-11},
	abstract     = {Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers' student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence.},
	language     = {en},
	keywords     = {\_tablet, Deep learning, Assessment, Essay grading, Natural language processing, Short answer scoring},
	file         = {Ramesh_Sanampudi_2022_An automated essay scoring systems.pdf:/Users/neilnatarajan/Zotero/storage/E4AA56VV/Ramesh_Sanampudi_2022_An automated essay scoring systems.pdf:application/pdf}
}
@article{Rebitschek_Gigerenzer_Wagner_2021,
	title        = {People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors},
	author       = {Rebitschek, Felix G. and Gigerenzer, Gerd and Wagner, Gert G.},
	year         = 2021,
	journal      = {Scientific Reports},
	publisher    = {Nature Publishing Group},
	volume       = 11,
	number       = 1,
	pages        = 20171,
	doi          = {10.1038/s41598-021-99802-y},
	issn         = {2045-2322},
	rights       = {2021 The Author(s)},
	abstractnote = {This study provides the first representative analysis of error estimations and willingness to accept errors in a Western country (Germany) with regards to algorithmic decision-making systems (ADM). We examine people’s expectations about the accuracy of algorithms that predict credit default, recidivism of an offender, suitability of a job applicant, and health behavior. Also, we ask whether expectations about algorithm errors vary between these domains and how they differ from expectations about errors made by human experts. In a nationwide representative study (N = 3086) we find that most respondents underestimated the actual errors made by algorithms and are willing to accept even fewer errors than estimated. Error estimates and error acceptance did not differ consistently for predictions made by algorithms or human experts, but people’s living conditions (e.g. unemployment, household income) affected domain-specific acceptance (job suitability, credit defaulting) of misses and false alarms. We conclude that people have unwarranted expectations about the performance of ADM systems and evaluate errors in terms of potential personal consequences. Given the general public’s low willingness to accept errors, we further conclude that acceptance of ADM appears to be conditional to strict accuracy requirements.},
	language     = {en}
}
@article{rhoades1993herfindahl,
	title        = {The herfindahl-hirschman index},
	author       = {Rhoades, Stephen A},
	year         = 1993,
	journal      = {Fed. Res. Bull.},
	publisher    = {HeinOnline},
	volume       = 79,
	pages        = 188
}
@article{ribeiro_nothing_2016,
	title        = {Nothing {Else} {Matters}: {Model}-{Agnostic} {Explanations} {By} {Identifying} {Prediction} {Invariance}},
	shorttitle   = {Nothing {Else} {Matters}},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	month        = nov,
	journal      = {arXiv:1611.05817 [cs, stat]},
	url          = {http://arxiv.org/abs/1611.05817},
	urldate      = {2022-01-05},
	note         = {arXiv: 1611.05817},
	abstract     = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/36RCH2M7/1611.html:text/html;Ribeiro et al_2016_Nothing Else Matters.pdf:/Users/neilnatarajan/Zotero/storage/NCWBRPW5/Ribeiro et al_2016_Nothing Else Matters.pdf:application/pdf},
	abstractnote = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model’s behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model’s behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model’s behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.}
}
@inproceedings{ribeiro_why_2016,
	title        = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle   = {"{Why} {Should} {I} {Trust} {You}?},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	month        = aug,
	booktitle    = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher    = {ACM},
	address      = {San Francisco California USA},
	pages        = {1135--1144},
	doi          = {10.1145/2939672.2939778},
	isbn         = {978-1-4503-4232-2},
	url          = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	urldate      = {2022-01-21},
	abstract     = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Ribeiro et al_2016_Why Should I Trust You.pdf:/Users/neilnatarajan/Zotero/storage/X6X63T57/Ribeiro et al_2016_Why Should I Trust You.pdf:application/pdf}
}
@inproceedings{ribeiro_anchors_2018,
	title        = {Anchors: {High} {Precision} {Model}-{Agnostic} {Explanations}},
	shorttitle   = {Anchors},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2018,
	booktitle    = {{AAAI}},
	pages        = 9,
	abstract     = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language     = {en},
	file         = {Ribeiro et al. - Anchors High Precision Model-Agnostic Explanation.pdf:/Users/neilnatarajan/Zotero/storage/B6X96F9E/Ribeiro et al. - Anchors High Precision Model-Agnostic Explanation.pdf:application/pdf}
}
@article{ribers2020machine,
	title        = {Machine predictions and human decisions with variation in payoffs and skill},
	author       = {Ribers, Michael Allan and Ullrich, Hannes},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2011.11017}
}
@inproceedings{robert_multi-level_2013,
	title        = {A multi-level analysis of the impact of shared leadership in diverse virtual teams},
	author       = {Robert, Lionel P.},
	year         = 2013,
	month        = feb,
	booktitle    = {Proceedings of the 2013 conference on {Computer} supported cooperative work},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CSCW} '13},
	pages        = {363--374},
	doi          = {10.1145/2441776.2441818},
	isbn         = {978-1-4503-1331-5},
	url          = {https://doi.org/10.1145/2441776.2441818},
	urldate      = {2024-07-23},
	abstract     = {Although organisations are using more virtual teams to accomplish work, they are finding it difficult to use traditional forms of leadership to manage these teams. Many organisations are encouraging a shared leadership approach over the traditional individual leader. Yet, there have been only a few empirical studies directly examining the effectiveness of such an approach and none have taken into account the team diversity. To address this gap, this paper reports the results of an empirical examination of the impacts of shared leadership in virtual teams. Results confirm the proposed research model. The impacts of shared leadership are multilevel and vary by race and gender. In addition, while shared leadership promotes team satisfaction despite prior assumptions, it actually reduces rather than increases team performance.},
	file         = {Full Text:/Users/neilnatarajan/Zotero/storage/ME9BXCBE/Robert - 2013 - A multi-level analysis of the impact of shared lea.pdf:application/pdf}
}
@article{rockoff2010subjective,
	title        = {Subjective and objective evaluations of teacher effectiveness},
	author       = {Rockoff, Jonah E and Speroni, Cecilia},
	year         = 2010,
	journal      = {American Economic Review},
	volume       = 100,
	number       = 2,
	pages        = {261--66}
}
@article{rockoff2011subjective,
	title        = {Subjective and objective evaluations of teacher effectiveness: Evidence from New York City},
	author       = {Rockoff, Jonah E and Speroni, Cecilia},
	year         = 2011,
	journal      = {Labour Economics},
	publisher    = {Elsevier},
	volume       = 18,
	number       = 5,
	pages        = {687--696}
}
@inproceedings{rojas2022dollar,
	title        = {The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world},
	author       = {Rojas, William A Gaviria and Diamos, Sudnya and Kini, Keertan Ranjan and Kanter, David and Reddi, Vijay Janapa and Coleman, Cody},
	year         = 2022,
	booktitle    = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}
}
@article{rolnick_greedy_nodate,
	title        = {Greedy {Maximization} of {Submodular} {Functions}},
	author       = {Rolnick, David and Weed, Jonathan},
	language     = {en},
	file         = {Rolnick and Weed - Greedy Maximization of Submodular Functions.pdf:/Users/neilnatarajan/Zotero/storage/PVH2X8MX/Rolnick and Weed - Greedy Maximization of Submodular Functions.pdf:application/pdf}
}
@inproceedings{ron2021corporate,
	title        = {Corporate social responsibility via multi-armed bandits},
	author       = {Ron, Tom and Ben-Porat, Omer and Shalit, Uri},
	year         = 2021,
	month        = mar,
	booktitle    = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{FAccT} '21},
	pages        = {26--40},
	doi          = {10.1145/3442188.3445868},
	isbn         = {978-1-4503-8309-7},
	url          = {https://doi.org/10.1145/3442188.3445868},
	urldate      = {2021-10-15},
	date-added   = {2023-09-11 11:24:11 -0400},
	date-modified = {2023-09-11 11:24:11 -0400},
	abstract     = {We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.},
	file         = {Ron et al_2021_Corporate Social Responsibility via Multi-Armed Bandits.pdf:/Users/neilnatarajan/Zotero/storage/WHH4SDZU/Ron et al_2021_Corporate Social Responsibility via Multi-Armed Bandits.pdf:application/pdf}
}
@incollection{Rossi2020-ROSWNA-2,
	title        = {What's New About Woke Racial Capitalism (and What Isn't): ``Wokewashing'' and the Limits of Representation},
	author       = {Rossi, Enzo and Taiwo, Olufemi},
	year         = 2020,
	booktitle    = {Spectre}
}
@article{rudin_interpretable_2021,
	title        = {Interpretable {Machine} {Learning}: {Fundamental} {Principles} and 10 {Grand} {Challenges}},
	shorttitle   = {Interpretable {Machine} {Learning}},
	author       = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
	year         = 2021,
	month        = jul,
	journal      = {arXiv:2103.11251 [cs, stat]},
	url          = {http://arxiv.org/abs/2103.11251},
	urldate      = {2022-04-14},
	note         = {arXiv: 2103.11251},
	abstract     = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, \_tablet, 68T01},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/XK8MDDNN/2103.html:text/html;Rudin et al_2021_Interpretable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/HGA2DHWE/Rudin et al_2021_Interpretable Machine Learning.pdf:application/pdf}
}
@article{rudin_why_2019,
	title        = {Why {Are} {We} {Using} {Black} {Box} {Models} in {AI} {When} {We} {Don}’t {Need} {To}? {A} {Lesson} {From} {An} {Explainable} {AI} {Competition}},
	author       = {Rudin, Cynthia and Radin, Joanna},
	year         = 2019,
	month        = nov,
	journal      = {Harvard Data Science Review},
	volume       = 1,
	number       = 2,
	doi          = {10.1162/99608f92.5a8a3a3d},
	url          = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8}
}
@inproceedings{sanchez2020does,
	title        = {What does it mean to'solve'the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems},
	author       = {S{\'a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 conference on fairness, accountability, and transparency},
	pages        = {458--468}
}
@inproceedings{suhr2021does,
	title        = {Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring},
	author       = {S{\"u}hr, Tom and Hilgard, Sophie and Lakkaraju, Himabindu},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
	pages        = {989--999}
}
@inproceedings{sadigh_active_2017,
	title        = {Active {Preference}-{Based} {Learning} of {Reward} {Functions}},
	author       = {Sadigh, Dorsa and Dragan, Anca and Sastry, Shankar and Seshia, Sanjit},
	year         = 2017,
	month        = jul,
	booktitle    = {Robotics: {Science} and {Systems} {XIII}},
	publisher    = {Robotics: Science and Systems Foundation},
	doi          = {10.15607/RSS.2017.XIII.053},
	isbn         = {978-0-9923747-3-0},
	url          = {http://www.roboticsproceedings.org/rss13/p53.pdf},
	urldate      = {2022-08-03},
	abstract     = {Our goal is to efﬁciently learn reward functions encoding a human’s preferences for how a dynamical system should act. There are two challenges with this. First, in many problems it is difﬁcult for people to provide demonstrations of the desired system trajectory (like a high-DOF robot arm motion or an aggressive driving maneuver), or to even assign how much numerical reward an action or trajectory should get. We build on work in label ranking and propose to learn from preferences (or comparisons) instead: the person provides the system a relative preference between two trajectories. Second, the learned reward function strongly depends on what environments and trajectories were experienced during the training phase. We thus take an active learning approach, in which the system decides on what preference queries to make. A novel aspect of our work is the complexity and continuous nature of the queries: continuous trajectories of a dynamical system in environments with other moving agents (humans or robots). We contribute a method for actively synthesizing queries that satisfy the dynamics of the system. Further, we learn the reward function from a continuous hypothesis space by maximizing the volume removed from the hypothesis space by each query. We assign weights to the hypothesis space in the form of a log-concave distribution and provide a bound on the number of iterations required to converge. We show that our algorithm converges faster to the desired reward compared to approaches that are not active or that do not synthesize queries in an autonomous driving domain. We then run a user study to put our method to the test with real people.},
	language     = {en},
	file         = {Sadigh et al. - 2017 - Active Preference-Based Learning of Reward Functio.pdf:/Users/neilnatarajan/Zotero/storage/TIF4AZNQ/Sadigh et al. - 2017 - Active Preference-Based Learning of Reward Functio.pdf:application/pdf}
}
@inproceedings{salem2022don,
	title        = {Don’t let Ricci v. DeStefano hold you back: A bias-aware legal solution to the hiring paradox},
	author       = {Salem, Jad and Desai, Deven and Gupta, Swati},
	year         = 2022,
	booktitle    = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
	pages        = {651--666}
}
@misc{saunders_self-critiquing_2022,
	title        = {Self-critiquing models for assisting human evaluators},
	author       = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
	year         = 2022,
	month        = jun,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2206.05802},
	url          = {http://arxiv.org/abs/2206.05802},
	urldate      = {2022-08-03},
	note         = {arXiv:2206.05802 [cs]},
	abstract     = {We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/VFJIMKYC/Saunders et al. - 2022 - Self-critiquing models for assisting human evaluat.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/7SYGZ72F/2206.html:text/html}
}
@article{scheuerman2021auto,
	title        = {Auto-essentialization: Gender in automated facial analysis as extended colonial project},
	author       = {Scheuerman, Morgan Klaus and Pape, Madeleine and Hanna, Alex},
	year         = 2021,
	journal      = {Big Data \& Society},
	publisher    = {SAGE Publications Sage UK: London, England},
	volume       = 8,
	number       = 2,
	pages        = 20539517211053712
}
@article{scheuerman2019computers,
	title        = {How computers see gender: An evaluation of gender classification in commercial facial analysis services},
	author       = {Scheuerman, Morgan Klaus and Paul, Jacob M and Brubaker, Jed R},
	year         = 2019,
	journal      = {Proceedings of the ACM on Human-Computer Interaction},
	publisher    = {ACM New York, NY, USA},
	volume       = 3,
	number       = {CSCW},
	pages        = {1--33}
}
@article{schmidt1998validity,
	title        = {The validity and utility of selection methods in personnel psychology: Practical and theoretical implications of 85 years of research findings.},
	author       = {Schmidt, Frank L and Hunter, John E},
	year         = 1998,
	month        = jan,
	journal      = {Psychological bulletin},
	publisher    = {American Psychological Association},
	volume       = 124,
	number       = 2,
	pages        = 262,
	doi          = {10.1037/0033-2909.124.2.262},
	note         = {MAG ID: 2136971664},
	date-added   = {2023-09-11 11:25:28 -0400},
	date-modified = {2023-09-11 11:25:28 -0400},
	abstract     = {This article summarizes the practical and theoretical implications of 85 years of research in personnel selection. On the basis of meta-analytic findings, this article presents the validity of 19 selection procedures for predicting job performance and training performance and the validity of paired combinations of general mental ability (GMA) and Ihe 18 other selection procedures. Overall, the 3 combinations with the highest multivariate validity and utility for job performance were GMA plus a work sample test (mean validity of .63), GMA plus an integrity test (mean validity of .65), and GMA plus a structured interview (mean validity of .63). A further advantage of the latter 2 combinations is that they can be used for both entry level selection and selection of experienced employees. The practical utility implications of these summary findings are substantial. The implications of these research findings for the development of theories of job performance are discussed. From the point of view of practical value, the most important property of a personnel assessment method is predictive validity: the ability to predict future job performance, job-related learning (such as amount of learning in training and development programs), and other criteria. The predictive validity coefficient is directly proportional to the practical economic value (utility) of the assessment method (Brogden, 1949; Schmidt, Hunter, McKenzie, \& Muldrow, 1979). Use of hiring methods with increased predictive validity leads to substantial increases in employee performance as measured in percentage increases in output, increased monetary value of output, and increased learning of job-related skills (Hunter, Schmidt, \& Judiesch, 1990). Today, the validity of different personnel measures can be determined with the aid of 85 years of research. The most wellknown conclusion from this research is that for hiring employees without previous experience in the job the most valid predictor of future performance and learning is general mental ability ([GMA], i.e., intelligence or general cognitive ability; Hunter \& Hunter, 1984; Ree \& Earles, 1992). GMA can be measured using commercially available tests. However, many other measures can also contribute to the overall validity of the selection process. These include, for example, measures of}
}
@inproceedings{schrills_color_2020,
	title        = {Color for {Characters} - {Effects} of {Visual} {Explanations} of {AI} on {Trust} and {Observability}},
	author       = {Schrills, Tim and Franke, Thomas},
	year         = 2020,
	booktitle    = {Artificial {Intelligence} in {HCI}},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {Lecture {Notes} in {Computer} {Science}},
	pages        = {121--135},
	doi          = {10.1007/978-3-030-50334-5_8},
	isbn         = {978-3-030-50334-5},
	abstract     = {The present study investigates the effects of prototypical visualization approaches aimed at increasing the explainability of machine learning systems in regard to perceived trustworthiness and observability. As the amount of processes automated by artificial intelligence (AI) increases, so does the need to investigate users’ perception. Previous research on explainable AI (XAI) tends to focus on technological optimization. The limited amount of empirical user research leaves key questions unanswered, such as which XAI designs actually improve perceived trustworthiness and observability. We assessed three different visual explanation approaches, consisting of either only a table with classification scores used for classification, or, additionally, one of two different backtraced visual explanations. In a within-subjects design with N = 83 we examined the effects on trust and observability in an online experiment. While observability benefitted from visual explanations, information-rich explanations also led to decreased trust. Explanations can support human-AI interaction, but differentiated effects on trust and observability have to be expected. The suitability of different explanatory approaches for individual AI applications should be further examined to ensure a high level of trust and observability in e.g. automated image processing.},
	language     = {en},
	editor       = {Degen, Helmut and Reinerman-Jones, Lauren},
	keywords     = {Explainable AI, Human-AI interaction, Human-automation interaction, Machine learning, Trust in Automation}
}
@article{schumann2017diverse,
	title        = {The diverse cohort selection problem},
	author       = {Schumann, Candice and Counts, Samsara N and Foster, Jeffrey S and Dickerson, John P},
	year         = 2017,
	month        = mar,
	journal      = {arXiv preprint arXiv:1709.03441},
	url          = {http://arxiv.org/abs/1709.03441},
	urldate      = {2022-04-12},
	note         = {arXiv: 1709.03441},
	date-added   = {2023-09-11 11:26:37 -0400},
	date-modified = {2023-09-11 11:26:37 -0400},
	abstract     = {How should a firm allocate its limited interviewing resources to select the optimal cohort of new employees from a large set of job applicants? How should that firm allocate cheap but noisy resume screenings and expensive but in-depth in-person interviews? We view this problem through the lens of combinatorial pure exploration (CPE) in the multi-armed bandit setting, where a central learning agent performs costly exploration of a set of arms before selecting a final subset with some combinatorial structure. We generalize a recent CPE algorithm to the setting where arm pulls can have different costs and return different levels of information. We then prove theoretical upper bounds for a general class of arm-pulling strategies in this new setting. We apply our general algorithm to a real-world problem with combinatorial structure: incorporating diversity into university admissions. We take real data from admissions at one of the largest US-based computer science graduate programs and show that a simulation of our algorithm produces a cohort with hiring overall utility while spending comparable budget to the current admissions process at that university.},
	keywords     = {Computer Science - Machine Learning},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/2TIBJVN5/1709.html:text/html;Schumann et al_2019_The Diverse Cohort Selection Problem.pdf:/Users/neilnatarajan/Zotero/storage/QVALQ3QA/Schumann et al_2019_The Diverse Cohort Selection Problem.pdf:application/pdf}
}
@article{shannon1948mathematical,
	title        = {A mathematical theory of communication},
	author       = {Shannon, Claude Elwood},
	year         = 1948,
	journal      = {The Bell system technical journal},
	publisher    = {Nokia Bell Labs},
	volume       = 27,
	number       = 3,
	pages        = {379--423}
}
@article{shneiderman_human-centered_2020,
	title        = {Human-{Centered} {Artificial} {Intelligence}: {Reliable}, {Safe} \& {Trustworthy}},
	shorttitle   = {Human-{Centered} {Artificial} {Intelligence}},
	author       = {Shneiderman, Ben},
	year         = 2020,
	month        = feb,
	journal      = {arXiv:2002.04087 [cs]},
	url          = {http://arxiv.org/abs/2002.04087},
	urldate      = {2022-04-14},
	note         = {arXiv: 2002.04087},
	abstract     = {Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe \& Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, H.5.0},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/ZYZUQ3GP/Shneiderman - 2020 - Human-Centered Artificial Intelligence Reliable, .pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/PFVN9S2I/2002.html:text/html}
}
@article{shore2018inclusive,
	title        = {Inclusive workplaces: A review and model},
	author       = {Shore, Lynn M and Cleveland, Jeanette N and Sanchez, Diana},
	year         = 2018,
	journal      = {Human resource management review},
	publisher    = {Elsevier},
	volume       = 28,
	number       = 2,
	pages        = {176--189}
}
@article{lewis2020retrieval,
	title        = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
	author       = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {9459--9474}
}
@article{Shavit_O’Keefe_Eloundou_McMillan_Agarwal_Brundage_Adler_Campbell_Lee_Mishkin_et,
	title        = {Practices for Governing Agentic AI Systems},
	author       = {Shavit, Yonadav and O’Keefe, Cullen and Eloundou, Tyna and McMillan, Paul and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and Campbell, Rosie and Lee, Teddy and Mishkin, Pamela and Hickey, Alan and Slama, Katarina and Ahmad, Lama and Beutel, Alex and Passos, Alexandre and Robinson, David G},
	abstractnote = {Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more eﬃciently and eﬀectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a deﬁnition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we oﬀer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codiﬁed. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.},
	language     = {en}
}
@misc{shumailov_curse_2023,
	title        = {The {Curse} of {Recursion}: {Training} on {Generated} {Data} {Makes} {Models} {Forget}},
	shorttitle   = {The {Curse} of {Recursion}},
	author       = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	year         = 2023,
	month        = may,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2305.17493},
	url          = {http://arxiv.org/abs/2305.17493},
	urldate      = {2023-11-14},
	note         = {arXiv:2305.17493 [cs]},
	abstract     = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/DSDBV8L6/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/ILIGJLUD/2305.html:text/html}
}
@book{silberberg2000structure,
	title        = {The Structure of Economics: A Mathematical Analysis 3rd Edition},
	author       = {Silberberg, Eugene and Suen, Wing Chuen},
	year         = 2000,
	publisher    = {McGraw-Hill.},
	date-added   = {2023-11-27 18:33:16 -0500},
	date-modified = {2023-11-27 18:33:16 -0500}
}
@inproceedings{smart_why_2020,
	title        = {Why {Reliabilism} {Is} not {Enough}: {Epistemic} and {Moral} {Justification} in {Machine} {Learning}},
	shorttitle   = {Why {Reliabilism} {Is} not {Enough}},
	author       = {Smart, Andrew and James, Larry and Hutchinson, Ben and Wu, Simone and Vallor, Shannon},
	year         = 2020,
	month        = feb,
	booktitle    = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{AIES} '20},
	pages        = {372--377},
	doi          = {10.1145/3375627.3375866},
	isbn         = {978-1-4503-7110-0},
	url          = {https://doi.org/10.1145/3375627.3375866},
	urldate      = {2021-11-25},
	abstract     = {In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of {\textbackslash}em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method {\textbackslash}citegoldman2012reliabilism. We argue that, in cases where model deployments require {\textbackslash}em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral "wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.},
	keywords     = {epistemology, explainability, interpretability, machine learning, moral justification, neural networks, \_tablet},
	file         = {Smart et al_2020_Why Reliabilism Is not Enough.pdf:/Users/neilnatarajan/Zotero/storage/V57Y3F9I/Smart et al_2020_Why Reliabilism Is not Enough.pdf:application/pdf}
}
@article{sokol_explainability_2020,
	title        = {Explainability {Fact} {Sheets}: {A} {Framework} for {Systematic} {Assessment} of {Explainable} {Approaches}},
	shorttitle   = {Explainability {Fact} {Sheets}},
	author       = {Sokol, Kacper and Flach, Peter},
	year         = 2020,
	month        = jan,
	journal      = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	pages        = {56--67},
	doi          = {10.1145/3351095.3372870},
	url          = {http://arxiv.org/abs/1912.05100},
	urldate      = {2022-01-05},
	note         = {arXiv: 1912.05100},
	abstract     = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/EF7VNKR6/1912.html:text/html;Sokol_Flach_2020_Explainability Fact Sheets.pdf:/Users/neilnatarajan/Zotero/storage/QEN8DP73/Sokol_Flach_2020_Explainability Fact Sheets.pdf:application/pdf}
}
@article{staiger2010searching,
	title        = {Searching for effective teachers with imperfect information},
	author       = {Staiger, Douglas O and Rockoff, Jonah E},
	year         = 2010,
	journal      = {Journal of Economic perspectives},
	volume       = 24,
	number       = 3,
	pages        = {97--118}
}
@article{steel_multiple_2018,
	title        = {Multiple diversity concepts and their ethical-epistemic implications},
	author       = {Steel, Daniel and Fazelpour, Sina and Gillette, Kinley and Crewe, Bianca and Burgess, Michael},
	year         = 2018,
	month        = oct,
	journal      = {European Journal for Philosophy of Science},
	volume       = 8,
	number       = 3,
	pages        = {761--780},
	doi          = {10.1007/s13194-018-0209-5},
	issn         = {1879-4920},
	url          = {https://doi.org/10.1007/s13194-018-0209-5},
	urldate      = {2024-02-28},
	abstract     = {A concept of diversity is an understanding of what makes a group diverse that may be applicable in a variety of contexts. We distinguish three diversity concepts, show that each can be found in discussions of diversity in science, and explain how they tend to be associated with distinct epistemic and ethical rationales. Yet philosophical literature on diversity among scientists has given little attention to distinct concepts of diversity. This is significant because the unappreciated existence of multiple diversity concepts can generate unclarity about the meaning of “diversity,” lead to problematic inferences from empirical research, and obscure complex ethical-epistemic questions about how to define diversity in specific cases. We illustrate some ethical-epistemic implications of our proposal by reference to an example of deliberative mini-publics on human tissue biobanking.},
	language     = {en},
	keywords     = {Diversity, Deliberative mini-publics, Social epistemology},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/EB75WR2A/Steel et al. - 2018 - Multiple diversity concepts and their ethical-epis.pdf:application/pdf}
}
@misc{stein_philosophy_and_ethics_of_informationpdf_nodate,
	title        = {Philosophy\_and\_ethics\_of\_information.pdf},
	author       = {Stein, Jacob},
	file         = {Markup:/Users/neilnatarajan/Zotero/storage/SH5FD7CE/1056156_Philosophy_and_ethics_of_information.pdf:application/pdf;Stein - Philosophy_and_ethics_of_information.pdf.pdf:/Users/neilnatarajan/Zotero/storage/X4KKG57C/Stein - Philosophy_and_ethics_of_information.pdf.pdf:application/pdf}
}
@article{sterkenburg_no-free-lunch_2021,
	title        = {The no-free-lunch theorems of supervised learning},
	author       = {Sterkenburg, Tom F. and Grünwald, Peter D.},
	year         = 2021,
	month        = jun,
	journal      = {Synthese},
	doi          = {10.1007/s11229-021-03233-1},
	issn         = {1573-0964},
	url          = {https://doi.org/10.1007/s11229-021-03233-1},
	urldate      = {2021-11-25},
	abstract     = {The no-free-lunch theorems promote a skeptical conclusion that all possible machine learning algorithms equally lack justification. But how could this leave room for a learning theory, that shows that some algorithms are better than others? Drawing parallels to the philosophy of induction, we point out that the no-free-lunch results presuppose a conception of learning algorithms as purely data-driven. On this conception, every algorithm must have an inherent inductive bias, that wants justification. We argue that many standard learning algorithms should rather be understood as model-dependent: in each application they also require for input a model, representing a bias. Generic algorithms themselves, they can be given a model-relative justification.},
	language     = {en},
	file         = {Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning_annotated.pdf:/Users/neilnatarajan/Zotero/storage/5F83F632/Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning_annotated.pdf:application/pdf;Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning.pdf:/Users/neilnatarajan/Zotero/storage/5F83F632/Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning.pdf:application/pdf}
}
@book{stevenson2019algorithmic,
	title        = {Algorithmic risk assessment in the hands of humans},
	author       = {Stevenson, Megan T and Doleac, Jennifer L},
	year         = 2019,
	publisher    = {IZA-Institute of Labor Economics}
}
@article{subotic2020psychometric,
	title        = {Psychometric validation of the ICAR Matrix Reasoning test},
	author       = {Suboti{\'c}, Sini{\v{s}}a and Bao{\v{s}}i{\'c}, Jovana and Velimir, An{\dj}ela and Malen{\v{c}}i{\'c}, Marija and Mihajlovi{\'c}, Bojana and Lovri{\'c}, Sanja Radeti{\'c}},
	year         = 2020,
	journal      = {Empirical Studies in Psychology},
	volume       = 59,
	pages        = 62
}
@article{sundararajan_many_2020,
	title        = {The many {Shapley} values for model explanation},
	author       = {Sundararajan, Mukund and Najmi, Amir},
	year         = 2020,
	month        = feb,
	journal      = {arXiv:1908.08474 [cs, econ]},
	url          = {http://arxiv.org/abs/1908.08474},
	urldate      = {2022-01-05},
	note         = {arXiv: 1908.08474},
	abstract     = {The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing [16] showing that it is the {\textbackslash}emph\{unique\} method that satisfies certain good properties ({\textbackslash}emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Economics - Theoretical Economics},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/YLSBC8Y5/1908.html:text/html;Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:/Users/neilnatarajan/Zotero/storage/552GV96A/Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:application/pdf}
}
@article{suresh_framework_2021,
	title        = {A {Framework} for {Understanding} {Sources} of {Harm} throughout the {Machine} {Learning} {Life} {Cycle}},
	author       = {Suresh, Harini and Guttag, John V.},
	year         = 2021,
	month        = jun,
	journal      = {arXiv:1901.10002 [cs, stat]},
	url          = {http://arxiv.org/abs/1901.10002},
	urldate      = {2021-10-13},
	note         = {arXiv: 1901.10002},
	abstract     = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/W9EYYBFW/1901.html:text/html;Suresh and Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:/Users/neilnatarajan/Zotero/storage/S7MC52HY/Suresh and Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:application/pdf;Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/C6YZDUWC/Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning.pdf:application/pdf}
}
@article{sutton_reinforcement_nodate,
	title        = {Reinforcement {Learning}: {An} {Introduction}},
	author       = {Sutton, Richard S and Barto, Andrew G},
	pages        = 352,
	language     = {en},
	file         = {Sutton and Barto - Reinforcement Learning An Introduction.pdf:/Users/neilnatarajan/Zotero/storage/XKA9BDZ6/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf}
}
@article{swartz_science_2019,
	title        = {The {Science} and {Value} of {Diversity}: {Closing} the {Gaps} in {Our} {Understanding} of {Inclusion} and {Diversity}},
	shorttitle   = {The {Science} and {Value} of {Diversity}},
	author       = {Swartz, Talia H and Palermo, Ann-Gel S and Masur, Sandra K and Aberg, Judith A},
	year         = 2019,
	month        = sep,
	journal      = {The Journal of Infectious Diseases},
	volume       = 220,
	number       = {Suppl 2},
	pages        = {S33--S41},
	doi          = {10.1093/infdis/jiz174},
	issn         = {0022-1899},
	url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6701939/},
	urldate      = {2024-07-31},
	abstract     = {Diversity drives excellence. Diversity enhances innovation in biomedical sciences and, as it relates to novel findings and treatment of diverse populations, in the field of infectious diseases. There are many obstacles to achieving diversity in the biomedical workforce, which create challenges at the levels of recruitment, retention, education, and promotion of individuals. Here we present the challenges, opportunities, and suggestions for the field, institutions, and individuals to adopt in mitigating bias and achieving greater levels of equity, representation, and excellence in clinical practice and research. Our findings provide optimism for a bright future of fair and collaborative approaches that will enhance the power of our biomedical workforce.},
	pmid         = 31430380,
	pmcid        = {PMC6701939},
	file         = {PubMed Central Full Text PDF:/Users/neilnatarajan/Zotero/storage/5QRFELJV/Swartz et al. - 2019 - The Science and Value of Diversity Closing the Ga.pdf:application/pdf}
}
@article{swartz2019science,
	title        = {The science and value of diversity: closing the gaps in our understanding of inclusion and diversity},
	author       = {Swartz, Talia H and Palermo, Ann-Gel S and Masur, Sandra K and Aberg, Judith A},
	year         = 2019,
	journal      = {The Journal of infectious diseases},
	publisher    = {Oxford University Press US},
	volume       = 220,
	number       = {Supplement\_2},
	pages        = {S33--S41}
}
@article{sweidan_probabilistic_nodate,
	title        = {Probabilistic {Prediction} in scikit-learn},
	author       = {Sweidan, Dirar and Johansson, Ulf},
	abstract     = {Adding conﬁdence measures to predictive models should increase the trustworthiness, but only if the models are well-calibrated. Historically, some algorithms like logistic regression, but also neural networks, have been considered to produce well-calibrated probability estimates oﬀ-the-shelf. Other techniques, like decision trees and Naive Bayes, on the other hand, are infamous for being signiﬁcantly overconﬁdent in their probabilistic predictions. In this paper, a large experimental study is conducted to investigate how well-calibrated models produced by a number of algorithms in the scikit-learn library are out-of-the-box, but also if either the built-in calibration techniques Platt scaling and isotonic regression, or Venn-Abers, can be used to improve the calibration. The results show that of the seven algorithms evaluated, the only one obtaining well-calibrated models without the external calibration is logistic regression. All other algorithms, i.e., decision trees, adaboost, gradient boosting, kNN, naive Bayes and random forest beneﬁt from using any of the calibration techniques. In particular, decision trees, Naive Bayes and the boosted models are substantially improved using external calibration. From a practitioner’s perspective, the obvious recommendation becomes to incorporate calibration when using probabilistic prediction. Comparing the diﬀerent calibration techniques, Platt scaling and VennAbers generally outperform isotonic regression, on these rather small datasets. Finally, the unique ability of Venn-Abers to output not only well-calibrated probability estimates, but also the conﬁdence in these estimates is demonstrated.},
	language     = {en},
	file         = {Sweidan_Johansson_Probabilistic Prediction in scikit-learn_annotated.pdf:/Users/neilnatarajan/Zotero/storage/LM4G8XP8/Sweidan_Johansson_Probabilistic Prediction in scikit-learn_annotated.pdf:application/pdf;Sweidan_Johansson_Probabilistic Prediction in scikit-learn.pdf:/Users/neilnatarajan/Zotero/storage/LM4G8XP8/Sweidan_Johansson_Probabilistic Prediction in scikit-learn.pdf:application/pdf}
}
@inproceedings{tam2002new,
	title        = {New selection indices for university admissions: A quantile approach},
	shorttitle   = {New {Selection} {Indices} for {University} {Admissions}},
	author       = {Tam, Mo-Yin S and Bassett, Gilbert W and Sukhatme, Uday},
	year         = 2002,
	booktitle    = {Statistical Data Analysis Based on the L 1-Norm and Related Methods},
	publisher    = {Birkhäuser Basel},
	address      = {Basel},
	pages        = {67--76},
	doi          = {10.1007/978-3-0348-8201-9_6},
	url          = {http://link.springer.com/10.1007/978-3-0348-8201-9_6},
	urldate      = {2022-04-12},
	date-added   = {2023-09-11 10:55:55 -0400},
	date-modified = {2023-09-11 10:55:55 -0400},
	organisation = {Springer},
	abstract     = {All universities seek to admit a freshmen cohort of a speciﬁed size who will be successful in graduating from college. Previous work has shown a high correlation between students’ eventual success and their ability to do well in their ﬁrst term. This objective translates into admitting students whose ﬁrst term grade point average (GPA) exceeds a minimum acceptable level. Current admissions decisions are based however on a selection index that is constructed from a GPA regression that estimates expected GPA. We consider a new approach in which the selection index is based directly on the quantiles of the GPA distribution. The new approach realistically assumes that student characteristics have diﬀerential impacts at diﬀerent parts of the GPA distribution. Since impacts usually vary for the low, middle and upper parts of the GPA distribution, the quantile approach provides additional information. The quantile method also provides admissions oﬃcers the ﬂexibility to target diﬀerent GPA properties for the freshman class. They can directly implement a criterion that selects students with maximal probability of a ﬁrst term GPA that is better than any speciﬁed value. We illustrate the quantile method by application to actual admission practices at the University of Illinois at Chicago.},
	language     = {en},
	editor       = {Dodge, Yadolah},
	keywords     = {\_tablet},
	file         = {Tam et al_2002_New Selection Indices for University Admissions.pdf:/Users/neilnatarajan/Zotero/storage/TM77DPPG/Tam et al_2002_New Selection Indices for University Admissions.pdf:application/pdf}
}
@article{tambe2019artificial,
	title        = {Artificial intelligence in human resources management: Challenges and a path forward},
	author       = {Tambe, Prasanna and Cappelli, Peter and Yakubovich, Valery},
	year         = 2019,
	journal      = {California Management Review},
	publisher    = {SAGE Publications Sage CA: Los Angeles, CA},
	volume       = 61,
	number       = 4,
	pages        = {15--42},
	date-added   = {2023-09-12 15:58:37 -0400},
	date-modified = {2023-09-12 15:58:37 -0400}
}
@article{tetlock_psychology_2000,
	title        = {The psychology of the unthinkable: {Taboo} trade-offs, forbidden base rates, and heretical counterfactuals.},
	shorttitle   = {The psychology of the unthinkable},
	author       = {Tetlock, Philip E. and Kristel, Orie V. and Elson, S. Beth and Green, Melanie C. and Lerner, Jennifer S.},
	year         = 2000,
	journal      = {Journal of Personality and Social Psychology},
	volume       = 78,
	number       = 5,
	pages        = {853--870},
	doi          = {10.1037/0022-3514.78.5.853},
	issn         = {1939-1315, 0022-3514},
	url          = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.78.5.853},
	urldate      = {2023-04-13},
	language     = {en},
	file         = {Tetlock et al. - 2000 - The psychology of the unthinkable Taboo trade-off.pdf:/Users/neilnatarajan/Zotero/storage/P89ZLI2Z/Tetlock et al. - 2000 - The psychology of the unthinkable Taboo trade-off.pdf:application/pdf}
}
@article{thejaswi2024diversity,
	title        = {Diversity-aware clustering: Computational Complexity and Approximation Algorithms},
	author       = {Thejaswi, Suhas and Gadekar, Ameet and Ordozgoiti, Bruno and Gionis, Aristides},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2401.05502}
}
@article{h_holden_thorp_chatgpt_2023,
	title        = {{ChatGPT} is fun, but not an author},
	author       = {Thorp, H Holden},
	year         = 2023,
	month        = jan,
	volume       = 379,
	number       = 6630,
	pages        = {313--313},
	doi          = {10.1126/science.adg7879},
	note         = {MAG ID: 4318263917},
	abstract     = {In less than 2 months, the artificial intelligence (AI) program ChatGPT has become a cultural sensation. It is freely accessible through a web portal created by the tool’s developer, OpenAI. The program—which automatically creates text based on written prompts—is so popular that it’s likely to be “at capacity right now” if you attempt to use it. When you do get through, ChatGPT provides endless entertainment. I asked it to rewrite the first scene of the classic American play Death of a Salesman , but to feature Princess Elsa from the animated movie Frozen as the main character instead of Willy Loman. The output was an amusing conversation in which Elsa—who has come home from a tough day of selling—is told by her son Happy, “Come on, Mom. You’re Elsa from Frozen . You have ice powers and you’re a queen. You’re unstoppable.” Mash-ups like this are certainly fun, but there are serious implications for generative AI programs like ChatGPT in science and academia.},
	pmid         = 36701446,
	keywords     = {\_tablet},
	file         = {H Holden Thorp_2023_ChatGPT is fun, but not an author.pdf:/Users/neilnatarajan/Zotero/storage/WPTVXC2V/H Holden Thorp_2023_ChatGPT is fun, but not an author.pdf:application/pdf}
}
@article{tucci_introduction_2013,
	title        = {Introduction to {Judea} {Pearl}'s {Do}-{Calculus}},
	author       = {Tucci, Robert R.},
	year         = 2013,
	month        = apr,
	journal      = {arXiv:1305.5506 [cs]},
	url          = {http://arxiv.org/abs/1305.5506},
	urldate      = {2021-11-25},
	note         = {arXiv: 1305.5506},
	abstract     = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.},
	keywords     = {Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/VPX3N5EA/1305.html:text/html;Tucci_2013_Introduction to Judea Pearl's Do-Calculus_annotated.pdf:/Users/neilnatarajan/Zotero/storage/22HM7TJ9/Tucci_2013_Introduction to Judea Pearl's Do-Calculus_annotated.pdf:application/pdf;Tucci_2013_Introduction to Judea Pearl's Do-Calculus.pdf:/Users/neilnatarajan/Zotero/storage/22HM7TJ9/Tucci_2013_Introduction to Judea Pearl's Do-Calculus.pdf:application/pdf}
}
@misc{university_7_2020,
	title        = {7 - {Causal} {Inference}},
	author       = {University, Carnegie Mellon, Machine Learning Department},
	year         = 2020,
	month        = aug,
	journal      = {Machine Learning Blog {\textbar} ML@CMU {\textbar} Carnegie Mellon University},
	url          = {https://blog.ml.cmu.edu/2020/08/31/7-causality/},
	urldate      = {2021-11-25},
	note         = {Section: Educational},
	abstract     = {The rules of causality play a role in almost everything we do. Criminal conviction is based on the principle of being the cause of a crime (guilt) as judged by a jury and most of us consider the effects of our actions before we make a decision. Therefore, it is reasonable to assume that considering},
	language     = {en-US},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/BTVK8Y82/7-causality.html:text/html}
}
@misc{university_ai_2021,
	title        = {{AI} empowers environmental regulators},
	author       = {University, Stanford},
	year         = 2021,
	month        = apr,
	journal      = {Stanford News},
	url          = {https://news.stanford.edu/2021/04/19/ai-empowers-environmental-regulators/},
	urldate      = {2021-10-15},
	note         = {Section: Science \& Technology},
	abstract     = {Monitoring environmental compliance is a particular challenge for governments in poor countries. A new machine learning approach that uses satellite imagery to pinpoint highly polluting brick kilns in Bangladesh could provide a low-cost solution.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/LUVEPW3N/ai-empowers-environmental-regulators.html:text/html;University - 2021 - AI empowers environmental regulators.pdf:/Users/neilnatarajan/Zotero/storage/JNAAFLZX/University - 2021 - AI empowers environmental regulators.pdf:application/pdf}
}
@misc{university_assessing_2021,
	title        = {Assessing regulatory fairness through machine learning},
	author       = {University, Stanford},
	year         = 2021,
	month        = mar,
	journal      = {Stanford News},
	url          = {https://news.stanford.edu/2021/03/08/assessing-regulatory-fairness-machine-learning/},
	urldate      = {2021-10-15},
	note         = {Section: Science \& Technology},
	abstract     = {Applying machine learning to a U.S. Environmental Protection Agency initiative reveals how key design elements determine what communities bear the burden of pollution. The approach could help ensure fairness and accountability in machine learning used by government regulators.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/MYVIMTMC/assessing-regulatory-fairness-machine-learning.html:text/html}
}
@article{upadhyay_applying_2018,
	title        = {Applying artificial intelligence: implications for recruitment},
	shorttitle   = {Applying artificial intelligence},
	author       = {Upadhyay, Ashwani Kumar and Khandelwal, Komal},
	year         = 2018,
	journal      = {Strategic HR Review},
	volume       = 17,
	number       = 5,
	pages        = {255--258},
	doi          = {10.1108/SHR-07-2018-0051},
	issn         = 14754398,
	url          = {https://www.proquest.com/docview/2133758924/abstract/BB1306B422284823PQ/1},
	urldate      = {2023-08-01},
	copyright    = {© Emerald Publishing Limited 2018},
	note         = {Num Pages: 4 Place: Bingley, United Kingdom Publisher: Emerald Group Publishing Limited},
	abstract     = {Purpose This paper aims to review the applications of artificial intelligence (AI) in the hiring process and its practical implications. This paper highlights the strategic shift in recruitment industry caused due to the adoption of AI in the recruitment process. This paper is prepared by independent academicians who have synthesized their views by a review of the latest reports, articles, research papers and other relevant literature. This paper describes the impact of developments in the field of AI on the hiring process and the recruitment industry. The application of AI for managing the recruitment process is leading to efficiency as well as qualitative gains for both clients and candidates. This paper offers strategic insights into automation of the recruitment process and presents practical ideas for implementation of AI in the recruitment industry. It also discusses the strategic implications of the usage of AI in the recruitment industry. This article describes the role of technological advancements in AI and its application for creating value for the recruitment industry as well as the clients. It saves the valuable reading time of practitioners and researchers by highlighting the AI applications in the recruitment industry in a concise and simple format.},
	language     = {English},
	keywords     = {\_tablet, Artificial intelligence, Automation, Hiring, Recruitment, Strategic},
	file         = {Upadhyay_Khandelwal_2018_Applying artificial intelligence.pdf:/Users/neilnatarajan/Zotero/storage/V8C6L7KB/Upadhyay_Khandelwal_2018_Applying artificial intelligence.pdf:application/pdf}
}
@article{ustun_learning_nodate,
	title        = {Learning {Optimized} {Risk} {Scores}},
	author       = {Ustun, Berk and Rudin, Cynthia},
	pages        = 75,
	abstract     = {Risk scores are simple classiﬁcation models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are diﬃcult to learn from data because they need to be calibrated, sparse, use small integer coeﬃcients, and obey application-speciﬁc constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-speciﬁc constraints without parameter tuning or post-processing. We benchmark the performance of diﬀerent methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical beneﬁts of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
	language     = {en},
	file         = {Ustun_Rudin_Learning Optimized Risk Scores.pdf:/Users/neilnatarajan/Zotero/storage/5BAWEDEC/Ustun_Rudin_Learning Optimized Risk Scores.pdf:application/pdf},
	keywords     = {\_tablet}
}
@article{ustun_actionable_2019,
	title        = {Actionable {Recourse} in {Linear} {Classification}},
	author       = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
	year         = 2019,
	month        = jan,
	journal      = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	pages        = {10--19},
	doi          = {10.1145/3287560.3287566},
	url          = {http://arxiv.org/abs/1809.06514},
	urldate      = {2022-01-05},
	note         = {arXiv: 1809.06514},
	abstract     = {Machine learning models are increasingly used to automate decisions that affect humans - deciding who should receive a loan, a job interview, or a social service. In such applications, a person should have the ability to change the decision of a model. When a person is denied a loan by a credit score, for example, they should be able to alter its input variables in a way that guarantees approval. Otherwise, they will be denied the loan as long as the model is deployed. More importantly, they will lack the ability to influence a decision that affects their livelihood. In this paper, we frame these issues in terms of recourse, which we define as the ability of a person to change the decision of a model by altering actionable input variables (e.g., income vs. age or marital status). We present integer programming tools to ensure recourse in linear classification problems without interfering in model development. We demonstrate how our tools can inform stakeholders through experiments on credit scoring problems. Our results show that recourse can be significantly affected by standard practices in model development, and motivate the need to evaluate recourse in practice.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Y6ISCE6J/1809.html:text/html;Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:/Users/neilnatarajan/Zotero/storage/33INW2LS/Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:application/pdf},
	abstractnote = {Machine learning models are increasingly used to automate decisions that affect humans - deciding who should receive a loan, a job interview, or a social service. In such applications, a person should have the ability to change the decision of a model. When a person is denied a loan by a credit score, for example, they should be able to alter its input variables in a way that guarantees approval. Otherwise, they will be denied the loan as long as the model is deployed. More importantly, they will lack the ability to influence a decision that affects their livelihood. In this paper, we frame these issues in terms of recourse, which we define as the ability of a person to change the decision of a model by altering actionable input variables (e.g., income vs. age or marital status). We present integer programming tools to ensure recourse in linear classification problems without interfering in model development. We demonstrate how our tools can inform stakeholders through experiments on credit scoring problems. Our results show that recourse can be significantly affected by standard practices in model development, and motivate the need to evaluate recourse in practice.}
}
@article{van2013translating,
	title        = {Translating values into design requirements},
	author       = {Van de Poel, Ibo},
	year         = 2013,
	journal      = {Philosophy and engineering: Reflections on practice, principles and process},
	booktitle    = {Philosophy and {Engineering}: {Reflections} on {Practice}, {Principles} and {Process}},
	publisher    = {Springer},
	address      = {Dordrecht},
	series       = {Philosophy of {Engineering} and {Technology}},
	pages        = {253--266},
	doi          = {10.1007/978-94-007-7762-0_20},
	isbn         = {978-94-007-7762-0},
	url          = {https://doi.org/10.1007/978-94-007-7762-0_20},
	urldate      = {2022-03-11},
	date-added   = {2023-09-11 11:27:41 -0400},
	date-modified = {2023-09-11 11:27:41 -0400},
	abstract     = {A crucial step in Value Sensitive Design (VSD) is the translation of values into design requirements. However, few research has been done on how this translation can be made. In this contribution, I first consider an example of this translation. I then introduce the notion of values hierarchy, a hierarchy structure of values, norms and design requirements. I discuss the relation of specification, by which values can be translated into design requirements, and the for the sake of relation which connects design requirements to underlying norms and values. I discuss conditions under which a certain specification of values into design requirements is adequate or at least tenable.},
	language     = {en},
	editor       = {Michelfelder, Diane P and McCarthy, Natasha and Goldberg, David E.},
	keywords     = {Design, Requirements, Specification, Value, Value sensitive design}
}
@article{van_esch_marketing_2019,
	title        = {Marketing {AI} recruitment: {The} next phase in job application and selection},
	shorttitle   = {Marketing {AI} recruitment},
	author       = {van Esch, Patrick and Black, J. Stewart and Ferolie, Joseph},
	year         = 2019,
	month        = jan,
	journal      = {Computers in Human Behavior},
	volume       = 90,
	pages        = {215--222},
	doi          = {10.1016/j.chb.2018.09.009},
	issn         = {0747-5632},
	url          = {https://www.sciencedirect.com/science/article/pii/S0747563218304497},
	urldate      = {2023-08-01},
	abstract     = {Organisations are beginning to adopt and capitalize on the functionality of AI in their recruitment processes. However, little is known about how potential candidates regard the use of AI as part of the recruitment process and whether or not it influences their likelihood to apply for a job. Our research finds that attitudes towards organisations that use AI in the recruitment process, significantly influences the likelihood that potential candidates will complete the application process. The novelty factor of using AI in the recruitment process, mediates and further positively influences job application likelihood. These positive relationships between attitudes towards the use of AI in the recruitment process and the likelihood of applying for a job have several important practical implications. First, it means that whilst anxiety is naturally present when AI is part of the recruitment process, the anxiety doesn't really affect the completion of job applications and therefore, organisations do not need to spend money on either hiding their use of AI or reducing the anxiety levels of potential candidates. To the contrary, the research suggests that organisations do not need to hide their use of AI in fear of alienating potential candidates, rather organisations may want to promote their use of AI in the recruitment process and focus on potential candidates that already have positive views of both the organisation and AI.},
	language     = {en},
	keywords     = {Artificial intelligence (AI), Technology, Recruitment, Job application likelihood, Marketing, Selection},
	file         = {ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ZZBI8NGN/S0747563218304497.html:text/html;van Esch et al_2019_Marketing AI recruitment_annotated.pdf:/Users/neilnatarajan/Zotero/storage/BHMBX3PU/van Esch et al_2019_Marketing AI recruitment_annotated.pdf:application/pdf;van Esch et al_2019_Marketing AI recruitment.pdf:/Users/neilnatarajan/Zotero/storage/BHMBX3PU/van Esch et al_2019_Marketing AI recruitment.pdf:application/pdf}
}
@inproceedings{VanKleek_Seymour_Binns_Shadbolt_2018,
	title        = {Respectful things: Adding social intelligence to “smart” devices},
	author       = {Van Kleek, Max and Seymour, William and Binns, Reuben and Shadbolt, Nigel},
	year         = 2018,
	month        = mar,
	booktitle    = {Living in the Internet of Things: Cybersecurity of the IoT - 2018},
	pages        = {1–6},
	doi          = {10.1049/cp.2018.0006},
	url          = {https://ieeexplore.ieee.org/document/8379693},
	abstractnote = {In this paper, we propose that the idea of devices respecting their end-users may serve as a strong design goal for highly personal and intimate smart devices. We ask what respect is, how it shapes interaction, and how good-faith simulation of respect might inform user-friendly smart device design. Respect is a natural and integral part of natural human relationships that is seen to shape work and personal relations. In a basic sense, this is the core purpose of smart things: we expect them to be ready and willing to help us. In this vein, we distill the characteristics of more complex respectful behaviours into 4 main types relevant to smart devices, drawing from philosophical analyses of the conceptual dimensions of respect: directive respect, obstacle respect, recognition respect, and care respect. We discuss the implications of each of these kinds of respect for the future of smart personal devices.}
}
@article{ashish_vaswani_attention_2017,
	title        = {Attention is {All} you {Need}},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year         = 2017,
	month        = jun,
	volume       = 30,
	pages        = {5998--6008},
	note         = {MAG ID: 2963403868},
	abstract     = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.}
}
@article{Hussain2014OverviewOV,
	title        = {Overview of Visualization Tool as an Experimental Method in Iterative Participatory Design},
	author       = {Hussain, Hanafizan},
	year         = 2014,
	journal      = {Applied Mechanics and Materials},
	volume       = 527,
	pages        = {117--120},
	url          = {https://api.semanticscholar.org/CorpusID:61037700}
}
@inproceedings{Chowdhury2023ReflectionsOO,
	title        = {Reflections on Online Child-Centric Participatory Design Approaches: Two Case Studies with Children and Early Adolescents},
	author       = {Ananta Chowdhury and Andrea Bunt},
	year         = 2023,
	url          = {https://api.semanticscholar.org/CorpusID:259343837}
}
@article{Brankaert2019IntersectionsIH,
	title        = {Intersections in HCI, Design and Dementia: Inclusivity in Participatory Approaches},
	author       = {Rens Brankaert and Gail Kenning and Daniel Welsh and Sarah Foley and James Hodge and David Unbehaun},
	year         = 2019,
	journal      = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
	url          = {https://api.semanticscholar.org/CorpusID:195353099}
}
@article{Hult1980TOWARDSAD,
	title        = {TOWARDS A DEFINITION OF ACTION RESEARCH: A NOTE AND BIBLIOGRAPHY},
	author       = {Margareta Hult and Sven-{\AA}ke Lennung},
	year         = 1980,
	journal      = {Journal of Management Studies},
	volume       = 17,
	pages        = {241--250},
	url          = {https://api.semanticscholar.org/CorpusID:154965544}
}
@inproceedings{10.1145/3544548.3580933,
	title        = {‘Treat me as your friend, not a number in your database’: Co-designing with Children to Cope with Datafication Online},
	author       = {Wang, Ge and Zhao, Jun and Van Kleek, Max and Shadbolt, Nigel},
	year         = 2023,
	booktitle    = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	location     = {Hamburg, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '23},
	doi          = {10.1145/3544548.3580933},
	isbn         = 9781450394215,
	url          = {https://doi.org/10.1145/3544548.3580933},
	abstract     = {Datafication refers to the practices through which children’s online actions are pervasively recorded, tracked, aggregated, analysed, and exploited by online services in ways including behavioural engineering and monetisation. Previous research has shown that not only do children care significantly about various aspects of datafication, but they demand a chance to take action. Through 10 co-design sessions with 53 children, we examined how children in the UK want to be supported to cope with the datafication practices. Our findings provide insights for creating age-appropriate support for children’s algorithmic literacy development, highlighting and unpacking the importance of no one-size-fitting-all designs to support children’s coping with datafication. We contribute a first understanding of how children aged 7–14 would like to be supported with datafication and what future data-driven digital experiences should be like for them, who demand a shift of the current data ecosystem towards a more humane-by-design and autonomy-supportive future.},
	articleno    = 95,
	numpages     = 21,
	keywords     = {Children, Co-design, Data Inference, Datafication, Online Platforms}
}
@inproceedings{10.1145/3544549.3573821,
	title        = {Child-Centred AI Design: Definition, Operation, and Considerations},
	author       = {Wang, Ge and Sun, Kaiwen and Atabey, Ay\c{c}a and Pothong, Kruakae and Lin, Grace C. and Zhao, Jun and Yip, Jason},
	year         = 2023,
	booktitle    = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
	location     = {Hamburg, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI EA '23},
	doi          = {10.1145/3544549.3573821},
	isbn         = 9781450394222,
	url          = {https://doi.org/10.1145/3544549.3573821},
	abstract     = {AI systems and related algorithms are starting to play a variety of roles in the digital ecosystems of children - being embedded in the connected toys, smart home IoT technologies, apps, and services they interact with on a daily basis. Going forward, AI systems will, in all likelihood, become even more pervasive in children’s applications simply due to their sheer usefulness in creating compelling, adaptive, and personal user experiences. Yet, understanding the ways that AI-driven systems used by children operate, and how AI could be designed to better anticipate and respond to children’s diverse requirements is still a new and emerging area of investigation. Our goals of this workshop are to (1) extend the current critically constructive dialogue around the meaning of child-centred AI design and (2) explore ways to operationalise such child-centred AI design in practice, and finally (3) further expand and foster a community for those who are interested in designing and developing child-centred AI systems.},
	articleno    = 338,
	numpages     = 6,
	keywords     = {Child-Centred AI design, child-computer interaction}
}
@article{Tokranova2022ApplyingPD,
	title        = {Applying participatory design principles to collaborative art-creating sessions},
	author       = {Darja Tokranova and Merja Lina Bauters},
	year         = 2022,
	journal      = {Proceedings of the 25th International Academic Mindtrek Conference},
	url          = {https://api.semanticscholar.org/CorpusID:253421927}
}
@article{venn-wycherley_realities_2024,
	title        = {The {Realities} of {Evaluating} {Educational} {Technology} in {School} {Settings}},
	author       = {Venn-Wycherley, Megan and Kharrufa, Ahmed and Lechelt, Susan and Nicholson, Rebecca and Howland, Kate and Almjally, Abrar and Trory, Anthony and Sarangapani, Vidya},
	year         = 2024,
	month        = feb,
	journal      = {ACM Trans. Comput.-Hum. Interact.},
	volume       = 31,
	number       = 2,
	pages        = {26:1--26:33},
	doi          = {10.1145/3635146},
	issn         = {1073-0516},
	url          = {https://doi.org/10.1145/3635146},
	urldate      = {2024-07-12},
	abstract     = {HCI researchers are increasingly interested in the evaluation of educational technologies in context, yet acknowledge that challenges remain regarding the logistical, material and methodological constraints of this approach to research [18, 53].Through the analysis of the authors’ contributed thematic research vignettes, the following article exposes the practical realities of evaluating educational technologies in school settings. This includes insights into the planning stages of evaluation, the relationship between the researcher and the school environment, and the impact of the school context on the data collection process.We conclude by providing an orientation for the design of HCI educational technology research undertaken in school contexts, providing guidance such as considering the role of modular research design, clarifying goals and expectations with school partners, and reporting researcher positionality.},
	keywords     = {\_tablet},
	file         = {Venn-Wycherley et al_2024_The Realities of Evaluating Educational Technology in School Settings.pdf:/Users/neilnatarajan/Zotero/storage/59UM7683/Venn-Wycherley et al_2024_The Realities of Evaluating Educational Technology in School Settings.pdf:application/pdf},
	abstractnote = {HCI researchers are increasingly interested in the evaluation of educational technologies in context, yet acknowledge that challenges remain regarding the logistical, material and methodological constraints of this approach to research [18, 53].Through the analysis of the authors’ contributed thematic research vignettes, the following article exposes the practical realities of evaluating educational technologies in school settings. This includes insights into the planning stages of evaluation, the relationship between the researcher and the school environment, and the impact of the school context on the data collection process.We conclude by providing an orientation for the design of HCI educational technology research undertaken in school contexts, providing guidance such as considering the role of modular research design, clarifying goals and expectations with school partners, and reporting researcher positionality.}
}
@inproceedings{Vereschak_Alizadeh_Bailly_Caramiaux_2024,
	title        = {Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made},
	author       = {Vereschak, Oleksandra and Alizadeh, Fatemeh and Bailly, Gilles and Caramiaux, Baptiste},
	year         = 2024,
	month        = may,
	booktitle    = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI ’24},
	pages        = {1–14},
	doi          = {10.1145/3613904.3642018},
	isbn         = 9798400703300,
	url          = {https://dl.acm.org/doi/10.1145/3613904.3642018},
	abstractnote = {Trust between humans and AI in the context of decision-making has acquired an important role in public policy, research and industry. In this context, Human-AI Trust has often been tackled from the lens of cognitive science and psychology, but lacks insights from the stakeholders involved. In this paper, we conducted semi-structured interviews with 7 AI practitioners and 7 decision subjects from various decision domains. We found that 1) interviewees identified the prerequisites for the existence of trust and distinguish trust from trustworthiness, reliance, and compliance; 2) trust in AI-integrated systems is strongly influenced by other human actors, more than the system’s features; 3) the role of Human-AI trust factors is stakeholder-dependent. These results provide clues for the design of Human-AI interactions in which trust plays a major role, as well as outline new research directions in Human-AI Trust.},
	collection   = {CHI ’24}
}
@article{vereschak_how_2021,
	title        = {How to {Evaluate} {Trust} in {AI}-{Assisted} {Decision} {Making}? {A} {Survey} of {Empirical} {Methodologies}},
	shorttitle   = {How to {Evaluate} {Trust} in {AI}-{Assisted} {Decision} {Making}?},
	author       = {Vereschak, Oleksandra and Bailly, Gilles and Caramiaux, Baptiste},
	year         = 2021,
	month        = oct,
	journal      = {Proceedings of the ACM on Human-Computer Interaction},
	volume       = 5,
	number       = {CSCW2},
	pages        = {327:1--327:39},
	doi          = {10.1145/3476068},
	url          = {https://doi.org/10.1145/3476068},
	urldate      = {2022-04-14},
	abstract     = {The spread of AI-embedded systems involved in human decision making makes studying human trust in these systems critical. However, empirically investigating trust is challenging. One reason is the lack of standard protocols to design trust experiments. In this paper, we present a survey of existing methods to empirically investigate trust in AI-assisted decision making and analyse the corpus along the constitutive elements of an experimental protocol. We find that the definition of trust is not commonly integrated in experimental protocols, which can lead to findings that are overclaimed or are hard to interpret and compare across studies. Drawing from empirical practices in social and cognitive studies on human-human trust, we provide practical guidelines to improve the methodology of studying Human-AI trust in decision-making contexts. In addition, we bring forward research opportunities of two types: one focusing on further investigation regarding trust methodologies and the other on factors that impact Human-AI trust.},
	keywords     = {artificial intelligence, trust, \_tablet, decision making, methodology},
	file         = {Vereschak et al_2021_How to Evaluate Trust in AI-Assisted Decision Making.pdf:/Users/neilnatarajan/Zotero/storage/TCP8T6PK/Vereschak et al_2021_How to Evaluate Trust in AI-Assisted Decision Making.pdf:application/pdf},
	abstractnote = {The spread of AI-embedded systems involved in human decision making makes studying human trust in these systems critical. However, empirically investigating trust is challenging. One reason is the lack of standard protocols to design trust experiments. In this paper, we present a survey of existing methods to empirically investigate trust in AI-assisted decision making and analyse the corpus along the constitutive elements of an experimental protocol. We find that the definition of trust is not commonly integrated in experimental protocols, which can lead to findings that are overclaimed or are hard to interpret and compare across studies. Drawing from empirical practices in social and cognitive studies on human-human trust, we provide practical guidelines to improve the methodology of studying Human-AI trust in decision-making contexts. In addition, we bring forward research opportunities of two types: one focusing on further investigation regarding trust methodologies and the other on factors that impact Human-AI trust.}
}
@misc{verma_ghostbuster_2023,
	title        = {Ghostbuster: {Detecting} {Text} {Ghostwritten} by {Large} {Language} {Models}},
	shorttitle   = {Ghostbuster},
	author       = {Verma, Vivek and Fleisig, Eve and Tomlin, Nicholas and Klein, Dan},
	year         = 2023,
	month        = nov,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2305.15047},
	urldate      = {2024-03-18},
	note         = {arXiv:2305.15047 [cs]},
	abstract     = {We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text. Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated. Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box models or unknown model versions. In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to a variety of existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the robustness of our system to a variety of perturbations and paraphrasing attacks and evaluate its performance on documents written by non-native English speakers.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/UXNLBQ68/2305.html:text/html;Full Text PDF:/Users/neilnatarajan/Zotero/storage/WJP6FRRI/Verma et al. - 2023 - Ghostbuster Detecting Text Ghostwritten by Large .pdf:application/pdf}
}
@article{vilone_explainable_2020,
	title        = {Explainable {Artificial} {Intelligence}: a {Systematic} {Review}},
	shorttitle   = {Explainable {Artificial} {Intelligence}},
	author       = {Vilone, Giulia and Longo, Luca},
	year         = 2020,
	month        = oct,
	journal      = {arXiv:2006.00093 [cs]},
	url          = {http://arxiv.org/abs/2006.00093},
	urldate      = {2022-01-05},
	note         = {arXiv: 2006.00093},
	abstract     = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.0, I.2.6, I.2.m},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GLMKTK42/2006.html:text/html;Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/KXUTCJLA/Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:application/pdf},
	annote       = {Comment: 78 pages, 18 figures, journal paper to be submitted to Information Fusion}
}
@inproceedings{viswanathan_situational_2022,
	title        = {Situational {Recommender}: {Are} {You} {On} the {Spot}, {Refining} {Plans}, or {Just} {Bored}?},
	shorttitle   = {Situational {Recommender}},
	author       = {Viswanathan, Sruthi and Boulard, Cecile and Bruyat, Adrien and Maria Grasso, Antonietta},
	year         = 2022,
	month        = apr,
	booktitle    = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} '22},
	pages        = {1--19},
	doi          = {10.1145/3491102.3501909},
	isbn         = {978-1-4503-9157-3},
	url          = {https://dl.acm.org/doi/10.1145/3491102.3501909},
	urldate      = {2023-04-05},
	abstract     = {When people engage in urban exploration, the tool they are most likely to use today is a mobile phone. In this paper, we present observations of users’ “home” and “away” conducted to refine our understanding of situational Point-of-Interest (POI) needs. Our findings suggest three distinct categories of situations in which users seek POI information: On-the-spot, Refining plans, and Moments of boredom. Based on the similarities and differences of these three situations in five observed underlying constraints – distance of interest, engagement threshold, ambiguity of the search, profile matching, and other imperative constraints, we derive implications for designing and ranking POIs for a Situational Recommender. To further access our concept, we designed and prototyped Situational Recommender by providing an interactional representation of the situation, and ran a Wizard-of-Oz concept validation study. Our results suggest that participants understood the concept without much effort and appreciated its usefulness.},
	keywords     = {CAI, Interaction Design, Mobile Devices, POI Recommender System, Prototyping, Situation Aware, Situation modelling, User Studies},
	file         = {Viswanathan et al_2022_Situational Recommender_annotated.pdf:/Users/neilnatarajan/Zotero/storage/Y5UBUJJ7/Viswanathan et al_2022_Situational Recommender_annotated.pdf:application/pdf;Viswanathan et al_2022_Situational Recommender.pdf:/Users/neilnatarajan/Zotero/storage/Y5UBUJJ7/Viswanathan et al_2022_Situational Recommender.pdf:application/pdf}
}
@inproceedings{viswanathan_unlockme_2021,
	title        = {{UnlockMe}: {Social} {Interactions} when {Co}-located in {Online} {Activities}},
	shorttitle   = {{UnlockMe}},
	author       = {Viswanathan, Sruthi and Legras, Christophe},
	year         = 2021,
	month        = may,
	booktitle    = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} {EA} '21},
	pages        = {1--7},
	doi          = {10.1145/3411763.3451757},
	isbn         = {978-1-4503-8095-9},
	url          = {https://dl.acm.org/doi/10.1145/3411763.3451757},
	urldate      = {2023-04-05},
	abstract     = {All the activities that we do online, by either preference or obligation, deprive us of social interactions especially the impromptu ones. We present UnlockMe, a concept that aims at preserving the social link, the disposition to come across other people serendipitously when engaged in online activities such as purchasing goods, working from home or when viewing media and entertainment. Our concept relies on virtual co-location detection (both synchronous and asynchronous), to allow users to engage with other people with whom they would have been likely to interact when doing the same activities offline in the physical world. We developed and illustrated UnlockMe with six scenarios and low-fidelity prototyping to test it with 10 participants who were isolated due to the coronavirus disease (COVID-19) pandemic. Our findings reveal multimedia recommendations from close social connections to be the best scenario for UnlockMe followed by online shopping and connecting with the local community.},
	keywords     = {Collaborative Decision Making, Network Segregation, Recommendations, Serendipitous Messaging, Social Connections, Virtual Co-location},
	file         = {Viswanathan_Legras_2021_UnlockMe_annotated.pdf:/Users/neilnatarajan/Zotero/storage/IGZIP3NB/Viswanathan_Legras_2021_UnlockMe_annotated.pdf:application/pdf;Viswanathan_Legras_2021_UnlockMe.pdf:/Users/neilnatarajan/Zotero/storage/IGZIP3NB/Viswanathan_Legras_2021_UnlockMe.pdf:application/pdf}
}
@inproceedings{viswanathan_designing_2020,
	title        = {Designing {Ambient} {Wanderer}: {Mobile} {Recommendations} for {Urban} {Exploration}},
	shorttitle   = {Designing {Ambient} {Wanderer}},
	author       = {Viswanathan, Sruthi and Omidvar-Tehrani, Behrooz and Bruyat, Adrien and Roulland, Frédéric and Grasso, Antonietta Maria},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 2020 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{DIS} '20},
	pages        = {1405--1418},
	doi          = {10.1145/3357236.3395518},
	isbn         = {978-1-4503-6974-9},
	url          = {https://dl.acm.org/doi/10.1145/3357236.3395518},
	urldate      = {2024-03-13},
	abstract     = {Recommender systems are widely integrated into our everyday activities. These intelligent systems succeed in learning the user's profile to recommend movies, music, news and more. However, for designing context-aware recommendations, new challenges emerge in predicting the situational needs of the user. We prototyped Ambient Wanderer, our personalised and contextualised Point-of-Interest (POI) recommender system and experimented it with new locals, people who have recently relocated to a city. Our key findings include: sudden breakdowns during urban exploration, trust issues with the recommendations from people unlike them, feeling bored as the trigger to POI search, intent to find free activities, information needs on areas-of-interest beyond points-of-interest and the demand to build a new social life. For each of these needs, we present the implications to design mobile recommendations for urban exploration.},
	keywords     = {context-aware, personalised, poi recommender, prototyping, wizard of oz},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/ZJM4D2JJ/Viswanathan et al. - 2020 - Designing Ambient Wanderer Mobile Recommendations.pdf:application/pdf}
}
@misc{vizzuality_climate_nodate,
	title        = {Climate {Change} {Laws} of the {World}},
	author       = {Vizzuality},
	url          = {https://climate-laws.org/litigation_cases},
	urldate      = {2021-10-15},
	abstract     = {Climate Change Laws of the World is a global database of climate change laws, policies, climate targets and litigation cases},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/6MKRG3VH/litigation_cases.html:text/html}
}
@article{wachter_counterfactual_2017,
	title        = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	author       = {Wachter, Sandra and Mittelstadt, Brent D. and Russell, Chris},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1711.00399},
	url          = {http://arxiv.org/abs/1711.00399},
	note         = {\_eprint: 1711.00399}
}
@article{walser_quasi-experiments_nodate,
	title        = {Quasi-{Experiments} in {Schools}: {The} {Case} for {Historical} {Cohort} {Control} {Groups}},
	shorttitle   = {Quasi-{Experiments} in {Schools}},
	author       = {Walser, Tamara M.},
	doi          = {10.7275/17HJ-1K58},
	url          = {https://scholarworks.umass.edu/pare/vol19/iss1/6/},
	urldate      = {2022-04-12},
	note         = {Publisher: University of Massachusetts Amherst},
	abstract     = {There is increased emphasis on using experimental and quasi-experimental methods to evaluate educational programs; however, educational evaluators and school leaders are often faced with challenges when implementing such designs in educational settings. Use of a historical cohort control group design provides a viable option for conducting quasi-experiments in school-based outcome evaluation. A cohort is a successive group that goes through some experience together, such as a grade level or a training program. A historical cohort comparison group is a cohort group selected from pre-treatment archival data and matched to a subsequent cohort currently receiving a treatment. Although prone to the same threats to study validity as any quasi-experiment, issues related to selection, history, and maturation can be particularly challenging. However, use of a historical cohort control group can reduce noncomparability of treatment and control conditions through local, focal matching. In addition, a historical cohort control group design can alleviate concerns about denying program access to students in order to form a control group, minimize resource requirements and disruption to school routines, and make use of archival data schools and school districts collect and find meaningful. Accessed 12,614 times on https://pareonline.net from June 23, 2014 to December 31, 2019. For downloads from January 1, 2020 forward, please click on the PlumX Metrics link to the right.},
	language     = {en},
	keywords     = {\_tablet},
	file         = {Walser_Quasi-Experiments in Schools.pdf:/Users/neilnatarajan/Zotero/storage/6FB35U2V/Walser_Quasi-Experiments in Schools.pdf:application/pdf}
}
@article{walser2014quasi,
	title        = {Quasi-experiments in schools: The case for historical cohort control groups},
	author       = {Walser, Tamara M},
	year         = 2014,
	journal      = {Practical Assessment, Research, and Evaluation},
	volume       = 19,
	number       = 1,
	pages        = 6,
	date-added   = {2023-09-11 11:28:30 -0400},
	date-modified = {2023-09-11 11:28:30 -0400}
}
@inproceedings{wan_explainabilitys_2022,
	title        = {Explainability's {Gain} is {Optimality}'s {Loss}?: {How} {Explanations} {Bias} {Decision}-making},
	shorttitle   = {Explainability's {Gain} is {Optimality}'s {Loss}?},
	author       = {Wan, Charles and Belo, Rodrigo and Zejnilovic, Leid},
	year         = 2022,
	month        = jul,
	booktitle    = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher    = {ACM},
	address      = {Oxford United Kingdom},
	pages        = {778--787},
	doi          = {10.1145/3514094.3534156},
	isbn         = {978-1-4503-9247-1},
	url          = {https://dl.acm.org/doi/10.1145/3514094.3534156},
	urldate      = {2022-08-23},
	abstract     = {Decisions in organisations are about evaluating alternatives and choosing the one that would best serve organisational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations’ semantics of causal models, however, induce leakage from the decision-maker’s prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decisionmaker’s confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.},
	language     = {en},
	file         = {Wan et al_2022_Explainability's Gain is Optimality's Loss_annotated.pdf:/Users/neilnatarajan/Zotero/storage/UVJBBMTA/Wan et al_2022_Explainability's Gain is Optimality's Loss_annotated.pdf:application/pdf;Wan et al_2022_Explainability's Gain is Optimality's Loss.pdf:/Users/neilnatarajan/Zotero/storage/UVJBBMTA/Wan et al_2022_Explainability's Gain is Optimality's Loss.pdf:application/pdf}
}
@article{wang_transparency_2022,
	title        = {Transparency as {Manipulation}? {Uncovering} the {Disciplinary} {Power} of {Algorithmic} {Transparency}},
	shorttitle   = {Transparency as {Manipulation}?},
	author       = {Wang, Hao},
	year         = 2022,
	month        = sep,
	journal      = {Philosophy \& Technology},
	volume       = 35,
	number       = 3,
	pages        = 69,
	doi          = {10.1007/s13347-022-00564-w},
	issn         = {2210-5433, 2210-5441},
	url          = {https://link.springer.com/10.1007/s13347-022-00564-w},
	urldate      = {2022-08-04},
	abstract     = {Automated algorithms are silently making crucial decisions about our lives, but most of the time we have little understanding of how they work. To counter this hidden influence, there have been increasing calls for algorithmic transparency. Much ink has been spilled over the informational account of algorithmic transparency—about how much information should be revealed about the inner workings of an algorithm. But few studies question the power structure beneath the informational disclosure of the algorithm. As a result, the information disclosure itself can be a means of manipulation used by a group of people to advance their own interests. Instead of concentrating on information disclosure, this paper examines algorithmic transpar‑ency from the perspective of power, explaining how algorithmic transparency under a disciplinary power structure can be a technique of normalizing people’s behav‑ior. The informational disclosure of an algorithm can not only set up some de facto norms, but also build a scientific narrative of its algorithm to justify those norms. In doing so, people would be internally motivated to follow those norms with less criti‑cal analysis. This article suggests that we should not simply open the black box of an algorithm without challenging the existing power relations.},
	language     = {en},
	file         = {Wang_2022_Transparency as Manipulation_annotated.pdf:/Users/neilnatarajan/Zotero/storage/EM733RJZ/Wang_2022_Transparency as Manipulation_annotated.pdf:application/pdf;Wang_2022_Transparency as Manipulation.pdf:/Users/neilnatarajan/Zotero/storage/EM733RJZ/Wang_2022_Transparency as Manipulation.pdf:application/pdf}
}
@inproceedings{wang2020deontological,
	title        = {Deontological ethics by monotonicity shape constraints},
	author       = {Wang, Serena and Gupta, Maya},
	year         = 2020,
	month        = jun,
	booktitle    = {International conference on artificial intelligence and statistics},
	publisher    = {PMLR},
	pages        = {2043--2054},
	issn         = {2640-3498},
	url          = {https://proceedings.mlr.press/v108/wang20e.html},
	urldate      = {2022-02-18},
	note         = {ISSN: 2640-3498},
	date-added   = {2023-09-11 11:29:02 -0400},
	date-modified = {2023-09-11 11:29:02 -0400},
	organisation = {PMLR},
	abstract     = {We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as “favor the less fortunate,” and “do not penalize good attributes.” We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity.  This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.},
	language     = {en},
	file         = {Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints_annotated.pdf:/Users/neilnatarajan/Zotero/storage/VBSVHX6B/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints_annotated.pdf:application/pdf;Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:/Users/neilnatarajan/Zotero/storage/VBSVHX6B/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:application/pdf;Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:/Users/neilnatarajan/Zotero/storage/4GQ5G589/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:application/pdf},
	abstractnote = {We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as “favor the less fortunate,” and “do not penalize good attributes.” We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity.  This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.}
}
@article{wang_are_2021,
	title        = {Are {Explanations} {Helpful}? {A} {Comparative} {Study} of the {Effects} of {Explanations} in {AI}-{Assisted} {Decision}-{Making}},
	author       = {Wang, Xinru and Yin, Ming},
	year         = 2021,
	month        = apr,
	pages        = {318--328},
	doi          = {10.1145/3397481.3450650},
	note         = {MAG ID: 3156106752},
	abstract     = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.}
}
@misc{wang_use_2022,
	title        = {On the {Use} of {BERT} for {Automated} {Essay} {Scoring}: {Joint} {Learning} of {Multi}-{Scale} {Essay} {Representation}},
	shorttitle   = {On the {Use} of {BERT} for {Automated} {Essay} {Scoring}},
	author       = {Wang, Yongjie and Wang, Chuan and Li, Ruobing and Lin, Hui},
	year         = 2022,
	month        = may,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2205.03835},
	url          = {http://arxiv.org/abs/2205.03835},
	urldate      = {2023-08-08},
	note         = {arXiv:2205.03835 [cs]},
	abstract     = {In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, \_tablet},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/8PDW63WZ/2205.html:text/html;Wang et al_2022_On the Use of BERT for Automated Essay Scoring.pdf:/Users/neilnatarajan/Zotero/storage/MHI9UBMN/Wang et al_2022_On the Use of BERT for Automated Essay Scoring.pdf:application/pdf}
}
@book{Warikoo_2019,
	title        = {The Diversity Bargain: And Other Dilemmas of Race, Admissions, and Meritocracy at Elite Universities},
	author       = {Warikoo, Natasha},
	year         = 2019,
	month        = feb,
	publisher    = {University of Chicago Press},
	address      = {Chicago, IL},
	isbn         = {978-0-226-65107-1},
	url          = {https://press.uchicago.edu/ucp/books/book/chicago/D/bo24550619.html},
	abstractnote = {We’ve heard plenty from politicians and experts on affirmative action and higher education, about how universities should intervene—if at all—to ensure a diverse but deserving student population. But what about those for whom these issues matter the most? In this book, Natasha K. Warikoo deeply explores how students themselves think about merit and race at a uniquely pivotal moment: after they have just won the most competitive game of their lives and gained admittance to one of the world’s top universities.             What Warikoo uncovers—talking with both white students and students of color at Harvard, Brown, and Oxford—is absolutely illuminating; and some of it is positively shocking. As she shows, many elite white students understand the value of diversity abstractly, but they ignore the real problems that racial inequality causes and that diversity programs are meant to solve. They stand in fear of being labeled a racist, but they are quick to call foul should a diversity program appear at all to hamper their own chances for advancement. The most troubling result of this ambivalence is what she calls the “diversity bargain,” in which white students reluctantly agree with affirmative action as long as it benefits them by providing a diverse learning environment—racial diversity, in this way, is a commodity, a selling point on a brochure. And as Warikoo shows, universities play a big part in creating these situations. The way they talk about race on campus and the kinds of diversity programs they offer have a huge impact on student attitudes, shaping them either toward ambivalence or, in better cases, toward more productive and considerate understandings of racial difference.             Ultimately, this book demonstrates just how slippery the notions of race, merit, and privilege can be. In doing so, it asks important questions not just about college admissions but what the elite students who have succeeded at it—who will be the world’s future leaders—will do with the social inequalities of the wider world.},
	language     = {en}
}
@inproceedings{warren_categorical_2023,
	title        = {Categorical and {Continuous} {Features} in {Counterfactual} {Explanations} of {AI} {Systems}},
	author       = {Warren, Greta and Byrne, Ruth M. J. and Keane, Mark T.},
	year         = 2023,
	month        = mar,
	booktitle    = {Proceedings of the 28th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{IUI} '23},
	pages        = {171--187},
	doi          = {10.1145/3581641.3584090},
	isbn         = 9798400701061,
	url          = {https://dl.acm.org/doi/10.1145/3581641.3584090},
	urldate      = {2023-04-05},
	abstract     = {Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The proponents of these algorithms claim they meet users’ requirements for counterfactual explanations. For instance, many claim that the output of their algorithms work as explanations because they prioritise "plausible", "actionable" or "causally important" features in their generated counterfactuals. However, very few of these claims have been tested in controlled psychological studies, and we know very little about which aspects of counterfactual explanations help users to understand AI system decisions. Furthermore, we do not know whether counterfactual explanations are an advance on more traditional causal explanations that have a much longer history in AI (in explaining expert systems and decision trees). Accordingly, we carried out two user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, and user responses were measured objectively (users’ predictive accuracy) and subjectively (users’ satisfaction and trust judgments). Study 1 (N=127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy of predictions than no-explanation control descriptions but no higher accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust judgments than causal explanations. Study 2 (N=211) found that users were more accurate for categorically-transformed features compared to continuous ones, and also replicated the results of Study 1. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.},
	keywords     = {counterfactual, explanation, user study, XAI},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/MZFMHMGC/Warren et al. - 2023 - Categorical and Continuous Features in Counterfact.pdf:application/pdf}
}
@article{webb2019impact,
	title        = {The impact of artificial intelligence on the labor market},
	author       = {Webb, Michael},
	year         = 2019,
	journal      = {Available at SSRN 3482150}
}
@article{weber-wulff_testing_2023,
	title        = {Testing of detection tools for {AI}-generated text},
	author       = {Weber-Wulff, Debora and Anohina-Naumeca, Alla and Bjelobaba, Sonja and Foltýnek, Tomáš and Guerrero-Dib, Jean and Popoola, Olumide and Šigut, Petr and Waddington, Lorna},
	year         = 2023,
	month        = dec,
	journal      = {International Journal for Educational Integrity},
	volume       = 19,
	number       = 1,
	pages        = {1--39},
	doi          = {10.1007/s40979-023-00146-z},
	issn         = {1833-2595},
	url          = {https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00146-z},
	urldate      = {2024-07-31},
	copyright    = {2023 The Author(s)},
	note         = {Number: 1 Publisher: BioMed Central},
	abstract     = {Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for AI-generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AI-generated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text. Furthermore, content obfuscation techniques significantly worsen the performance of tools. The study makes several significant contributions. First, it summarises up-to-date similar scientific and non-scientific efforts in the field. Second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. Third, it discusses the implications and drawbacks of using detection tools for AI-generated text in academic settings.},
	language     = {en},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/9IE32FPG/Weber-Wulff et al. - 2023 - Testing of detection tools for AI-generated text.pdf:application/pdf}
}
@article{weerts_human-grounded_2019,
	title        = {A {Human}-{Grounded} {Evaluation} of {SHAP} for {Alert} {Processing}},
	author       = {Weerts, Hilde J. P. and van Ipenburg, Werner and Pechenizkiy, Mykola},
	year         = 2019,
	month        = jul,
	journal      = {arXiv:1907.03324 [cs, stat]},
	url          = {http://arxiv.org/abs/1907.03324},
	urldate      = {2022-04-14},
	note         = {arXiv: 1907.03324},
	abstract     = {In the past years, many new explanation methods have been proposed to achieve interpretability of machine learning predictions. However, the utility of these methods in practical applications has not been researched extensively. In this paper we present the results of a human-grounded evaluation of SHAP, an explanation method that has been well-received in the XAI and related communities. In particular, we study whether this local model-agnostic explanation method can be useful for real human domain experts to assess the correctness of positive predictions, i.e. alerts generated by a classifier. We performed experimentation with three different groups of participants (159 in total), who had basic knowledge of explainable machine learning. We performed a qualitative analysis of recorded reflections of experiment participants performing alert processing with and without SHAP information. The results suggest that the SHAP explanations do impact the decision-making process, although the model's confidence score remains to be a leading source of evidence. We statistically test whether there is a significant difference in task utility metrics between tasks for which an explanation was available and tasks in which it was not provided. As opposed to common intuitions, we did not find a significant difference in alert processing performance when a SHAP explanation is available compared to when it is not.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction, \_tablet},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/8Y57BJUL/1907.html:text/html;Weerts et al_2019_A Human-Grounded Evaluation of SHAP for Alert Processing.pdf:/Users/neilnatarajan/Zotero/storage/PBXD8NP3/Weerts et al_2019_A Human-Grounded Evaluation of SHAP for Alert Processing.pdf:application/pdf}
}
@article{weerts_case-based_2019,
	title        = {Case-{Based} {Reasoning} for {Assisting} {Domain} {Experts} in {Processing} {Fraud} {Alerts} of {Black}-{Box} {Machine} {Learning} {Models}},
	author       = {Weerts, Hilde J. P. and van Ipenburg, Werner and Pechenizkiy, Mykola},
	year         = 2019,
	month        = jul,
	journal      = {arXiv:1907.03334 [cs, stat]},
	url          = {http://arxiv.org/abs/1907.03334},
	urldate      = {2022-05-12},
	note         = {arXiv: 1907.03334},
	abstract     = {In many contexts, it can be useful for domain experts to understand to what extent predictions made by a machine learning model can be trusted. In particular, estimates of trustworthiness can be useful for fraud analysts who process machine learning-generated alerts of fraudulent transactions. In this work, we present a case-based reasoning (CBR) approach that provides evidence on the trustworthiness of a prediction in the form of a visualization of similar previous instances. Different from previous works, we consider similarity of local post-hoc explanations of predictions and show empirically that our visualization can be useful for processing alerts. Furthermore, our approach is perceived useful and easy to use by fraud analysts at a major Dutch bank.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction},
	file         = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/RSKRRF88/Weerts et al. - 2019 - Case-Based Reasoning for Assisting Domain Experts .pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IAJMXE6R/1907.html:text/html}
}
@article{westen_quantifying_2003,
	title        = {Quantifying construct validity: {Two} simple measures},
	shorttitle   = {Quantifying construct validity},
	author       = {Westen, Drew and Rosenthal, Robert},
	year         = 2003,
	journal      = {Journal of Personality and Social Psychology},
	publisher    = {American Psychological Association},
	address      = {US},
	volume       = 84,
	pages        = {608--618},
	doi          = {10.1037/0022-3514.84.3.608},
	issn         = {1939-1315},
	note         = {Place: US Publisher: American Psychological Association},
	abstract     = {Construct validity is one of the most central concepts in psychology. Researchers generally establish the construct validity of a measure by correlating it with a number of other measures and arguing from the pattern of correlations that the measure is associated with these variables in theoretically predictable ways. This article presents 2 simple metrics for quantifying construct validity that provide effect size estimates indicating the extent to which the observed pattern of correlations in a convergent-discriminant validity matrix matches the theoretically predicted pattern of correlations. Both measures, based on contrast analysis, provide simple estimates of validity that can be compared across studies, constructs, and measures meta-analytically, and can be implemented without the use of complex statistical procedures that may limit their accessibility. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	keywords     = {Construct Validity, Discriminant Validity, Effect Size (Statistical), Measurement, Prediction, Statistical Significance, Statistical Validity},
	file         = {Full Text:/Users/neilnatarajan/Zotero/storage/HQBEHIMP/Westen and Rosenthal - 2003 - Quantifying construct validity Two simple measure.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/JLJH3NZU/doiLanding.html:text/html},
	abstractnote = {Construct validity is one of the most central concepts in psychology. Researchers generally establish the construct validity of a measure by correlating it with a number of other measures and arguing from the pattern of correlations that the measure is associated with these variables in theoretically predictable ways. This article presents 2 simple metrics for quantifying construct validity that provide effect size estimates indicating the extent to which the observed pattern of correlations in a convergent-discriminant validity matrix matches the theoretically predicted pattern of correlations. Both measures, based on contrast analysis, provide simple estimates of validity that can be compared across studies, constructs, and measures meta-analytically, and can be implemented without the use of complex statistical procedures that may limit their accessibility. (PsycINFO Database Record (c) 2019 APA, all rights reserved)}
}
@article{will2023people,
	title        = {People versus machines: introducing the HIRE framework},
	author       = {Will, Paris and Krpan, Dario and Lordan, Grace},
	year         = 2023,
	journal      = {Artificial Intelligence Review},
	publisher    = {Springer},
	volume       = 56,
	number       = 2,
	pages        = {1071--1100}
}
@incollection{woodward_scientific_2021,
	title        = {Scientific {Explanation}},
	author       = {Woodward, James},
	year         = 2021,
	booktitle    = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher    = {Metaphysics Research Lab, Stanford University},
	url          = {https://plato.stanford.edu/archives/spr2021/entries/scientific-explanation/},
	edition      = {Spring 2021},
	editor       = {Zalta, Edward N.}
}
@article{wylie2006introduction,
	title        = {Introduction: when difference makes a difference},
	author       = {Wylie, Alison},
	year         = 2006,
	journal      = {Episteme},
	publisher    = {Cambridge University Press},
	volume       = 3,
	number       = {1-2},
	pages        = {1--7}
}
@article{xu2020diversity,
	title        = {Diversity in biology: definitions, quantification and models},
	author       = {Xu, Song and B{\"o}ttcher, Lucas and Chou, Tom},
	year         = 2020,
	journal      = {Physical Biology},
	publisher    = {IOP Publishing},
	volume       = 17,
	number       = 3,
	pages        = {031001}
}
@misc{xuan_can_2023,
	title        = {Can {Users} {Correctly} {Interpret} {Machine} {Learning} {Explanations} and {Simultaneously} {Identify} {Their} {Limitations}?},
	author       = {Xuan, Yueqing and Small, Edward and Sokol, Kacper and Hettiachchi, Danula and Sanderson, Mark},
	year         = 2023,
	month        = sep,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2309.08438},
	url          = {http://arxiv.org/abs/2309.08438},
	urldate      = {2023-10-09},
	note         = {arXiv:2309.08438 [cs]},
	abstract     = {Automated decision-making systems are becoming increasingly ubiquitous, motivating an immediate need for their explainability. However, it remains unclear whether users know what insights an explanation offers and, more importantly, what information it lacks. We conducted an online study with 200 participants to assess explainees' ability to realise known and unknown information for four representative explanations: transparent modelling, decision boundary visualisation, counterfactual explainability and feature importance. Our findings demonstrate that feature importance and decision boundary visualisation are the most comprehensible, but their limitations are not necessarily recognised by the users. In addition, correct interpretation of an explanation -- i.e., understanding known information -- is accompanied by high confidence, but a failure to gauge its limits -- thus grasp unknown information -- yields overconfidence; the latter phenomenon is especially prominent for feature importance and transparent modelling. Machine learning explanations should therefore embrace their richness and limitations to maximise understanding and curb misinterpretation.},
	keywords     = {Computer Science - Human-Computer Interaction},
	file         = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/INK7UMRK/2309.html:text/html;Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously_annotated.pdf:/Users/neilnatarajan/Zotero/storage/9W22XXZ6/Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously_annotated.pdf:application/pdf;Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously.pdf:/Users/neilnatarajan/Zotero/storage/9W22XXZ6/Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously.pdf:application/pdf}
}
@inproceedings{yang_how_2020,
	title        = {How do visual explanations foster end users' appropriate trust in machine learning?},
	author       = {Yang, Fumeng and Huang, Zhuanyi and Scholtz, Jean and Arendt, Dustin L.},
	year         = 2020,
	month        = mar,
	booktitle    = {Proceedings of the 25th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{IUI} '20},
	pages        = {189--201},
	doi          = {10.1145/3377325.3377480},
	isbn         = {978-1-4503-7118-6},
	url          = {https://doi.org/10.1145/3377325.3377480},
	urldate      = {2022-01-26},
	abstract     = {We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.},
	keywords     = {trust, explainable artificial intelligence, classification, human-machine collaboration, information visualization, supervised-learning, trust calibration, User Study},
	file         = {Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:/Users/neilnatarajan/Zotero/storage/WSQ8FI2J/Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:application/pdf;Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:/Users/neilnatarajan/Zotero/storage/WSQ8FI2J/false:application/pdf}
}
@article{yarger2020algorithmic,
	title        = {Algorithmic equity in the hiring of underrepresented IT job candidates},
	author       = {Yarger, Lynette and Cobb Payton, Fay and Neupane, Bikalpa},
	year         = 2020,
	journal      = {Online information review},
	publisher    = {Emerald Publishing Limited},
	volume       = 44,
	number       = 2,
	pages        = {383--395}
}
@inproceedings{yin_understanding_2019,
	title        = {Understanding the {Effect} of {Accuracy} on {Trust} in {Machine} {Learning} {Models}},
	author       = {Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
	year         = 2019,
	month        = may,
	booktitle    = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{CHI} '19},
	pages        = {1--12},
	doi          = {10.1145/3290605.3300509},
	isbn         = {978-1-4503-5970-2},
	url          = {https://doi.org/10.1145/3290605.3300509},
	urldate      = {2022-09-05},
	abstract     = {We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
	keywords     = {trust, machine learning, human-subject experiments},
	file         = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/BKL5C96V/Yin et al. - 2019 - Understanding the Effect of Accuracy on Trust in M.pdf:application/pdf},
	abstractnote = {We address a relatively under-explored aspect of human-computer interaction: people’s abilities to understand the relationship between a machine learning model’s stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople’s trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model’s stated accuracy on held-out data and on its observed accuracy in practice. We find that people’s trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
	collection   = {CHI ’19}
}
@inproceedings{zadrozny_transforming_2002,
	title        = {Transforming classifier scores into accurate multiclass probability estimates},
	author       = {Zadrozny, Bianca and Elkan, Charles},
	year         = 2002,
	month        = jul,
	booktitle    = {Proceedings of the eighth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {{KDD} '02},
	pages        = {694--699},
	doi          = {10.1145/775047.775151},
	isbn         = {978-1-58113-567-1},
	url          = {https://doi.org/10.1145/775047.775151},
	urldate      = {2024-02-22},
	abstract     = {Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.}
}
@incollection{zangwill_aesthetic_2023,
	title        = {Aesthetic {Judgment}},
	author       = {Zangwill, Nick},
	year         = 2023,
	booktitle    = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher    = {Metaphysics Research Lab, Stanford University},
	url          = {https://plato.stanford.edu/archives/sum2023/entries/aesthetic-judgment/},
	urldate      = {2023-12-05},
	edition      = {Summer 2023},
	abstract     = {Beauty is an important part of our lives. Ugliness too. It is nosurprise then that philosophers since antiquity have been interestedin our experiences of and judgments about beauty and ugliness. Theyhave tried to understand the nature of these experiences andjudgments, and they have also wanted to know whether these experiencesand judgments were legitimate. Both these projects took a sharpenedform in the twentieth century, when this part of our lives came undera sustained attack in both European and North American intellectualcircles. Much of the discourse about beauty since the eighteenthcentury had deployed a notion of the “aesthetic”, and sothat notion in particular came in for criticism. This disdain for theaesthetic may have roots in a broader cultural Puritanism, which fearsthe connection between the aesthetic and pleasure. At one time, fromthe 1960s to the 1990s, even to suggest that an artwork might be goodbecause it is pleasurable, as opposed to cognitively, morally orpolitically beneficial, was to court derision. (This is less truenow.) The twentieth century was not kind to the notions of beauty orthe aesthetic. Nevertheless, there were always somethinkers—philosophers, as well as others in the study ofparticular arts—who persisted in thinking seriously about beautyand the aesthetic. In the first part of this essay, we will look atthe particularly rich account of judgments of beauty given to us byImmanuel Kant. The notion of a “judgment of taste” iscentral to Kant’s account and also to virtually everyone workingin traditional aesthetics; so we begin by examining Kant’scharacterization of the judgment of taste. In the second part, we lookat the issues that twentieth century thinkers raised. In the thirdpart, we consider disinterestedness, which is taken by Kant to be partof the judgment of taste. We end, in the fourth part, by drawing onKant’s account of the judgment of taste to consider whether thenotion of the aesthetic is viable.},
	editor       = {Zalta, Edward N. and Nodelman, Uri},
	keywords     = {aesthetic, concept of the, Aristotle, General Topics: aesthetics, Hume, David: aesthetics, Kant, Immanuel: aesthetics and teleology, pleasure, relativism},
	file         = {SEP - Snapshot:/Users/neilnatarajan/Zotero/storage/43SUM3E6/aesthetic-judgment.html:text/html}
}
@article{zellers_defending_2019,
	title        = {Defending {Against} {Neural} {Fake} {News}},
	author       = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
	year         = 2019,
	month        = may,
	volume       = 32,
	pages        = {9051--9062},
	note         = {MAG ID: 2971008823},
	abstract     = {Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73\% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92\% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.}
}
@article{zerilli_how_nodate,
	title        = {How {Transparency} {Modulates} {Trust} in {Artificial} {Intelligence}},
	author       = {Zerilli, John and Bhatt, Umang and Weller, Adrian},
	pages        = 28,
	language     = {en},
	file         = {Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence_annotated.pdf:/Users/neilnatarajan/Zotero/storage/DVWBI8HB/Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence_annotated.pdf:application/pdf;Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/DVWBI8HB/Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence.pdf:application/pdf}
}
@misc{zerilli_explaining_2020,
	title        = {Explaining machine learning decisions},
	author       = {Zerilli, John},
	year         = 2020,
	month        = jan,
	journal      = {Philosophy of Science},
	volume       = 89,
	number       = 1,
	pages        = {1--19},
	doi          = {10.1017/psa.2021.13},
	issn         = {0031-8248, 1539-767X},
	url          = {http://philsci-archive.pitt.edu/19096/},
	urldate      = {2022-01-26},
	type         = {Preprint},
	abstract     = {The operations of deep networks are widely acknowledged to be inscrutable. The growing field of “Explainable AI” (XAI) has emerged in direct response to this problem. However, owing to the nature of the opacity in question, XAI has been forced to prioritise interpretability at the expense of completeness, and even realism, so that its explanations are frequently interpretable without being underpinned by more comprehensive explanations faithful to the way a network computes its predictions. While this has been taken to be a shortcoming of the field of XAI, I argue that it is broadly the right approach to the problem.},
	language     = {en},
	file         = {Snapshot:/Users/neilnatarajan/Zotero/storage/PFXRZAPS/Zerilli - 2020 - Explaining machine learning decisions.html:text/html;Zerilli_2020_Explaining machine learning decisions.pdf:/Users/neilnatarajan/Zotero/storage/W9RBSPVU/Zerilli_2020_Explaining machine learning decisions.pdf:application/pdf;Zerilli_2020_Explaining machine learning decisions.pdf:/Users/neilnatarajan/Zotero/storage/W9RBSPVU/false:application/pdf}
}
@inproceedings{zhang_effect_2020,
	title        = {Effect of confidence and explanation on accuracy and trust calibration in {AI}-assisted decision making},
	author       = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
	year         = 2020,
	month        = jan,
	booktitle    = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher    = {ACM},
	address      = {Barcelona Spain},
	pages        = {295--305},
	doi          = {10.1145/3351095.3372852},
	isbn         = {978-1-4503-6936-7},
	url          = {https://dl.acm.org/doi/10.1145/3351095.3372852},
	urldate      = {2023-01-25},
	abstract     = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model’s to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people’s trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI’s errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
	language     = {en},
	file         = {Zhang et al. - 2020 - Effect of confidence and explanation on accuracy a.pdf:/Users/neilnatarajan/Zotero/storage/USWMYA2K/Zhang et al. - 2020 - Effect of confidence and explanation on accuracy a.pdf:application/pdf},
	abstractnote = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model’s to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people’s trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI’s errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.}
}
@article{zhao2024position,
	title        = {Position: Measure Dataset Diversity, Don't Just Claim It},
	author       = {Zhao, Dora and Andrews, Jerone TA and Papakyriakopoulos, Orestis and Xiang, Alice},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2407.08188}
}
@article{zhao2023fairness,
	title        = {Fairness and diversity in recommender systems: a survey},
	author       = {Zhao, Yuying and Wang, Yu and Liu, Yunchao and Cheng, Xueqi and Aggarwal, Charu C and Derr, Tyler},
	year         = 2023,
	journal      = {ACM Transactions on Intelligent Systems and Technology},
	publisher    = {ACM New York, NY}
}
@incollection{zimmerman_research_2014,
	title        = {Research {Through} {Design} in {HCI}},
	author       = {Zimmerman, John and Forlizzi, Jodi},
	year         = 2014,
	booktitle    = {Ways of {Knowing} in {HCI}},
	publisher    = {Springer},
	address      = {New York, NY},
	pages        = {167--189},
	doi          = {10.1007/978-1-4939-0378-8_8},
	isbn         = {978-1-4939-0378-8},
	url          = {https://doi.org/10.1007/978-1-4939-0378-8_8},
	urldate      = {2023-11-03},
	abstract     = {In Research through Design (RtD), researchers generate new knowledge by understanding the current state and then suggesting an improved future state in the form of a design. It involves deep reflection in iteratively understanding the people, problem, and context around a situation that researchers feel they can improve.},
	language     = {en},
	editor       = {Olson, Judith S. and Kellogg, Wendy A.},
	keywords     = {Interaction Design, Design Team, Ethical Stance, Participatory Design, Problem Framing}
}
@article{Zimmerman_Forlizzi_2017,
	title        = {Speed Dating: Providing a Menu of Possible Futures},
	author       = {Zimmerman, John and Forlizzi, Jodi},
	year         = 2017,
	month        = mar,
	journal      = {She Ji: The Journal of Design, Economics, and Innovation},
	volume       = 3,
	number       = 1,
	pages        = {30–50},
	doi          = {10.1016/j.sheji.2017.08.003},
	issn         = {2405-8726},
	abstractnote = {As user experience (UX) design continues to grow and expand, designers often work in areas with few design patterns or social mores. It is easy to make things that people do not want or will not adopt. To help avoid this problem, we developed a method called speed dating. This method allows design teams to explore possible futures with target users. Speed dating helps to reduce the risk of making things that people will not adopt. It also discloses opportunities and user needs that design teams might not observe during fieldwork. Over the last decade, we have used this method in many research projects, teaching it to hundreds of UX design students. This article describes the speed dating method, presenting cases to show how speed dating aids UX design.}
}
@techreport{zolas2021advanced,
	title        = {Advanced technologies adoption and use by us firms: Evidence from the annual business survey},
	author       = {Zolas, Nikolas and Kroff, Zachary and Brynjolfsson, Erik and McElheran, Kristina and Beede, David N and Buffington, Cathy and Goldschlag, Nathan and Foster, Lucia and Dinlersoz, Emin},
	year         = 2021,
	institution  = {National Bureau of Economic Research}
}
@article{Ahnaf2023AHPAP,
	title        = {AHP and PROMETHEE Comparison on Decision Support System for Scholarship Selection in Universitas Sebelas Maret Surakarta},
	author       = {Faiz Ahnaf and Eka Putra and Yusfia Hafid Aristyagama and Nurcahya Pradana and Taufik Prakisya and Cucuk Wawan Budiyanto},
	year         = 2023,
	journal      = {Proceedings of the International Conference on Industrial Engineering and Operations Management},
	url          = {https://api.semanticscholar.org/CorpusID:258635921}
}
@misc{xiao2024humanaicollaborativeessayscoring,
	title        = {Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs},
	author       = {Changrong Xiao and Wenxing Ma and Qingping Song and Sean Xin Xu and Kunpeng Zhang and Yufang Wang and Qi Fu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2401.06431},
	eprint       = {2401.06431},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inbook{10.5555/1074100.1074233,
author = {Papadimitriou, Christos H.},
title = {Computational complexity},
year = {2003},
isbn = {0470864125},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
abstract = {Once we have developed an algorithm (q.v.) for solving a computational problem and analyzed its worst-case time requirements as a function of the size of its input (most usefully, in terms of the O-notation; see ALGORITHMS, ANALYSIS OF), it is inevitable to ask the question: "Can we do better?" In a typical problem, we may be able to devise new algorithms for the problem that are more and more efficient. But eventually, this line of research often seems to hit an invisible barrier, a level beyond whch improvements are very difficult, seemingly impossible, to come by. After many unsuccessful attempts, algorithm designers inevitably start to wonder if there is something inherent in the problem that makes it impossible to devise algorithms that are faster than the current one. They may try to develop mathematical techniques for proving formally that there can be no algorithm for the given problem which runs faster than the current one. Such a proof would be valuable, as it would suggest that it is futile to keep working on improved algorithms for this problem, that further improvements are certainly impossible. The realm of mathematical models and techniques for establishing such impossibility proofs is called computational complexity.},
booktitle = {Encyclopedia of Computer Science},
pages = {260–265},
numpages = {6}
}

@article{Bleemer_2023, title={Affirmative action and its race-neutral alternatives}, volume={220}, ISSN={0047-2727}, DOI={https://doi.org/10.1016/j.jpubeco.2023.104839}, abstractNote={As affirmative action loses political feasibility, many universities have implemented race-neutral alternatives like top percent policies and holistic review to increase enrollment among disadvantaged students. I study these policies’ application, admission, and enrollment effects using University of California administrative data. UC’s affirmative action and top percent policies increased underrepresented minority (URM) enrollment by over 20 percent and less than 4 percent, respectively. Holistic review increases implementing campuses’ URM enrollment by about 7 percent. Top percent policies and holistic review have negligible effects on lower-income enrollment, while race-based affirmative action modestly increased enrollment among very low-income students. These findings highlight that the most common race-neutral alternatives to affirmative action increase Black and Hispanic enrollment far less than affirmative action itself and reveal that none of these policies substantially affect universities’ socioeconomic composition.}, journal={Journal of Public Economics}, author={Bleemer, Zachary}, year={2023}, pages={104839} }

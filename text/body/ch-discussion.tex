\chapter{\label{ch:discussion}Discussion}

\minitoc 
% High Level: This chapter could use more unifying reflection on work as a whole, and an ethical reflection of Chapter 7.  
% I think it would be great to have a short discussion on generalisability to other selection tasks.
% I also think it would be great to have a short discussion on ‘what could go wrong’ if a scholarship selection committee followed your approach.

\section[The Role of AI Systems in Selection]{The Role of AI Systems in Selection: From Decision Support to Selection-Oriented AI}
\subsection{Current Challenges in AI for Selection}
\subsubsection{To Support or Supplant?}
AI tools have long been posed as both replacements and support systems for decision-makers both without and within selection processes \cite{barocas_big_2016,jacobs_how_2021,hildebrandt_law_nodate,yarger2020algorithmic,mattu_how_nodate}. Naïvely, implementations of AI systems supplant human decision-makers, often to disastrous effect \cite{mattu_how_nodate}.

\subsubsection{Who to Support?}
More human-centric AI systems, e.g., explainable AI (xAI), often instead serve as decision support tools (DSTs), and place the stakeholder (the selector) at the core of the decision-making process. These systems then focus on enhancing the experience of human decision-makers, offering tools that satisfy the subjective desiderata of selectors. \textcite{Lipton} critiques post-hoc notions of explainability because, in seeking to satisfy stakeholder desiderata, xAI tools may prove more misleading than insightful. While Chapter \ref{ch:xai} extends this critique to the practice of post-hoc justification more broadly, we find it applicable to all AI DSTs.

In addition to this, the problem of selection involves a complex interplay of interests. On the one hand, selection organisations are driven by the need to identify the best possible cohort of candidates, aiming to optimize organisational performance and success. On the other hand, candidates are motivated by the desire to be selected for opportunities that match their potential and aspirations. In practice, these two interests are at odds, and while DSTs might support one or the other, the need to support both decisions outstrips the capacity of any individual DST. But beyond these two core stakeholders lies a broader societal interest in the outcomes of selection processes. Society at large has a vested interest in ensuring that these processes uphold essential social values such as fairness, diversity, integrity, and justice. In the case of pro-social programmes such as global scholarships that desire that scholars do good with their careers, society similarly has an interest in ensuring that the most apt scholars are selected.

Centring these DSTs solely on the needs of either applicants or decision-makers within organisations would both suffer from the problem of subjective desires outlined by \textcite{Lipton} and fail to address the larger social implications of these selection decisions.\footnote{I.e., algorithms designed to streamline applicant evaluations may inadvertently reinforce existing biases, thereby undermining efforts to promote diversity and inclusion. Similarly, AI systems may prioritise ease of use and decision-making speed at the expense of fairness and transparency.} Thus, there is a need for DSTs that, rather than centring on a group of stakeholders, orient themselves around the task of selection itself.

\subsubsection{What to Support?}
A common DST paradigm sees practitioners making a series of similar kinds of decisions on different cases, e.g., deciding whether to grant a loan many times \cite{GiveMeSomeCredit,barocas_hidden_2020,ustun_actionable_2019,Rebitschek_Gigerenzer_Wagner_2021,10.1111/j.1467-954X.2007.00740.x}. However, we have explored here many challenges and decision points (enumerated in Table \ref{tab:full_decision_list}) not captured by this notion of decision-making. I.e., the conventional DST paradigm, wherein practitioners are supported for each decision, thinks only of a specific kind of in-process decision. It excludes both other in-process decisions and all ex-post decisions. Thus, any paradigm for DSTs oriented around the task of selection should seek to support all kinds of decisions.

\subsection{Proposing a New Paradigm: Selection-Oriented AI (SOAI)}
In response to these challenges, this thesis proposes a novel paradigm: SOAI. SOAI reimagines the role of AI systems in talent identification, and advocates for a shift away from the human-centric framework toward a hybrid selector-centred and selection-driven approach (wherein the design of AI systems is grounded in the social values that selection processes ought to uphold, but the values are supplied by the selectors making the decisions). While selectors remain important users of these systems, they are not the sole focus of design efforts. Instead, SOAI emphasises the importance of evaluating the broader social goals of AI-driven selection processes and seeks to help selectors achieve these goals \cite{batyavalue}.

The shift toward SOAI represents a necessary evolution in the use of AI systems in talent identification. By prioritising the social values that selection processes ought to reflect, SOAI challenges the current practitioner-centred approach and introduces a new standard for evaluating the success of AI in decision support. The implementation of SOAI can provide a path forward in creating AI systems that not only assist in the identification of talent but also ensure that the processes by which talent is selected are fair, just, and inclusive.

\section{Design Recommendations for SOAI Designers}
\subsection{Design for Specific Social Values}
While we wish to design to support all social values in the selection process, Chapter \ref{ch:diversity} demonstrates the difficulty of unpacking the social value of diversity; in Chapter \ref{ch:diversity}, we find success instead focusing on smaller component values that comprise diversity. Similarly, each decision point supported in Chapter \ref{ch:genai} implies a specific ontology about the role of generative AI in selection; these, too, stem from specific social values promoted by an organisation.

There is support for this from the literature. For example, literature on algorithmic fairness has long wrestled with contradictions between measurements of different kinds of fairnesses \cite{pmlr-v80-kearns18a}. While `individual fairness' draws on procedural notions of justice to ensure that applicants are treated equally regardless of differences in protected or irrelevant characteristics \cite{dwork_fairness_2012}, `group fairness' draws on distributive justice in seeking to achieve parity between different demographic groups \cite{Citron_2008,Olsaretti_2018}. There exists literature attempting to reconcile these notions: \textcite{pmlr-v28-zemel13} attempt to reconcile this in practice by simultaneously optimising for multiple fairness metrics; \textcite{lahoti2019ifairlearningindividuallyfair} seek only to optimise for individual fairness, and yet find increases in group fairness; and \textcite{binns_apparent_2019} contends that standard, blunt implementations of individual fairness should be replaced with a more nuanced formulation compatible with group fairness. Nonetheless, as they are often implemented, these two notions of fairness are often in conflict, and though designing to support both may be possible, it is liable, in a scholarship context, to create unclarity of the sort plaguing diversity, impeding programme desire to assess these concepts with specific targets. 

We suggest this generalises to SOAI practices in general: rather than designing around myriad values, only to find conflicting design implications of these disparate values, designers seeking to support social values in selection processes should focus on specific social values worthy of consideration.

\subsection{Identify Decision Points with the Decision Matrix}
In Chapter \ref{ch:context}, we conceive of selection as a series of decisions. Chapter \ref{ch:genai} expands on this, introducing the Decision Matrix framework to categorise the many decision points that selectors face according to their two most germane axes: the stakes of the decision and its stage in selection. This framework allows designers to reason about groups of decision points in much the same way that the explainable AI community reasons about groups of explainability techniques and to isolate desired or required properties of DSTs based on the taxonomy of the decision point they seek to support and to then determine which categories of decisions different genAI detectors are suitable to support\cite{ford_play_2020,kumar_problems_2020,doshi-velez_towards_2017,friedrich_taxonomy_2011,molnar_interpretable_2019}. 

In Chapter \ref{ch:xai}, we respond to criticisms isolated to \textcite{friedrich_taxonomy_2011}'s `post-hoc' explanations; here, the taxonomic distinctions are used in criticism to expand the scope of individual critiques \cite{barocas_hidden_2020,kumar_problems_2020}. We suggest the Decision Matrix can be used similarly, to discuss and critique decisions in a scholarship context.

However, we caution designers following this design recommendation to ensure that they also follow design recommendation \ref{ssec:real_change} and evaluate real change in addition to subjective desiderata. The Decision Matrix framework can be used to derive a set of a necessary, but perhaps not sufficient, properties of DSTs.

\subsection{Balance Qualitative and Quantitative Information in Presentation}
Human decision-makers often desire both a qualitative understanding of applicants and quantitative metrics to compare them. In Chapters \ref{ch:xai} and \ref{ch:diversity}, we find that selectors from Rise and Ellison Scholars seek to make decisions informed by both kinds of information; despite this, the desired balance between these modes of information varies based both on practitioner and type of decision. When quantitative information is neglected, practitioners are forced to make decisions on a case-by-case basis without important numerical context comparing applicants to a larger group; when qualitative information is neglected, practitioners are unable to consider applicants holistically. Developers following SOAI should consider the balance between quantitative and qualitative information in their systems, and design their systems to provide both when necessary.

We again find parallels in the fairness literature to this balance. Qualitative information enables the selectors' consideration of applicants as individuals, and this combines with the process of holistic review to create full pictures of applicants. \cite{dwork_fairness_2012}'s individual fairness holds a similar lens; rather than looking at applicants in terms of their place in the cohort, this notion of fairness demands equal treatment of applicants as people. However, quantitative information makes possible considerations of distributive notions of justice and group fairness principles \cite{Olsaretti_2018}, as decision-makers have access to the supporting information needed to contextualise applications relative to other members of protected groups. Notably, programmes with different ontologies governing what they consider fair will thus have different preferences considering the balance of quantitative and qualitative information in their systems. (This relationship is not absolute, though, as other differences in programmes may lead to differing priorities.)

\subsection{Evaluate Real Change in Addition to Subjective Satisfaction}\label{ssec:real_change}
\textcite{Lipton} critiques explainable AI (xAI) systems because they risk satisfying the subjective desires of the users while failing to improve objective outcomes. In Chapter \ref{ch:xai}, we confirm that this critique applies to some post-hoc justifications of model recommendations, as the justifications were found to yield an unwarranted increase in trust in human decision-makers. Thus, it is important to define and evaluate measures of the social values that DSTs intend to support; when evaluating these DSTs, they should not be evaluated human-centrically (i.e., according to their users' satisfaction), but should instead be evaluated on whether their employment improves social outcomes.

The fundamental challenge with evaluating ``real change'' in a selection context is the lack of ground truth; i.e., there are not, in general, ``correct'' or ``incorrect'' selection decisions, only those preferred by the organisation. In this thesis, we solve this problem by working with programmes to define measurable criteria that act as a surrogate for ``correct selection decisions''. In Chapter \ref{ch:xai}, these criteria are arrived at through an Action Research (AR) process and expressed in Figure \ref{fig:desiderata_matrix}, while in Chapter \ref{ch:spf}, these criteria are supplied directly by Rise, as the programme has internal metrics for both axes of the SPF. We recommend designers work with organisations to define surrogate criteria that can be used to evaluate the success of their systems.

\section{Implications}
\subsection{Algorithmic Fairness in a Selection Context}\label{ssec:fairness}
The work in this thesis has implications for the broader discussion of algorithmic fairness in selection processes. The design of SOAI DSTs has the potential to impact the lives of many of the world's most vulnerable people; it is thus imperative that these systems are designed fairly. However, the notion of fairness itself is complex and multifaceted. As \textcite{pmlr-v80-kearns18a} highlight, fairness can be understood in both procedural and distributive terms, and different methods of achieving fairness across different subgroups often conflict. Individual fairness is often discussed in the algorithmic fairness literature \cite{dwork_fairness_2012}; this is often contrasted with ``group'' fairness \cite{fleisher_whats_nodate,binns_apparent_2019,barocas2023fairness,Friedler_Scheidegger_Venkatasubramanian_2016}. Despite attempts to reconcile these differing notions of fairness, such as those by \textcite{binns_apparent_2019}, contradictions remain between metrics used to measure different forms of fairness; that is, decisions that may be ruled more fair by certain individual or procedural fairness measures might create group or distributive unfairness. What's worse, scholars disagree even on the best implementations of notions of fairness \cite{Friedler_Scheidegger_Venkatasubramanian_2016,binns_apparent_2019}, and differing interpretations conflict.

It is worth noting, then, that the work on generative AI detection in Chapter \ref{ch:genai} is built on a desire for procedural fairness, while the diversity goals of Chapters \ref{ch:diversity} and \ref{ch:spf}, in practice, accord closely with distributive notions. This raises the possibility that, via SOAI methods, researchers could determine socially beneficial fairness metrics to uphold in DSTs and build to support those.\footnote{This work, in particular, should not be done solely from the decision-maker's perspective. \textcite{10.1145/3351095.3372867} investigate applicant perceptions of appropriate fairness metrics; this work may be a good starting point for SOAI work in this field.}

%  - critical attention on algorithms Kitchin_2017
% I would be remiss to ignore the critical attention that algorithms have garnered as they proliferate through society. NISSENBAUM1998237 

\subsection{New Developments in AI for Selection}
The growing popularity of genAI has already dramatically increased the number of applications that job, university, and scholarship programmes must select from \cite{Kaashoek2024Impact}. While a blanket ban on genAI in application-writing may solve this \cite{h_holden_thorp_chatgpt_2023}, we find in Chapter \ref{ch:genai} that such a ban is unenforcible at present. We note in Chapter \ref{ch:genai} that our research is complicated by the rapidly changing nature of both genAI and detectors. Here, we extend this complication to SOAI as a whole. It may be that, as genAI development moves beyond retrieval-augmented generation to more complex architectures \cite{lewis2020retrieval}, such as integrated reasoning systems or agentic AI \cite{Shavit_O’Keefe_Eloundou_McMillan_Agarwal_Brundage_Adler_Campbell_Lee_Mishkin_et}, these systems will once again fundamentally change the process of selection. In light of this, SOAI is necessary to ensure that new, more powerful AI systems further the social aims of selection processes.

Of particular interest would be the development of AI systems capable of encoding domain knowledge in their structures, which could support decisions in a fundamentally different way. This could be particularly useful in automated essay scoring, where domain-specific knowledge is a significant problem \cite{elijahthesis}. If this is the case, then the work done in this thesis may serve as a precursor to the development of these systems and a guide for how to ensure that these systems are designed to support the social aims of selection processes.

% ⭐️ Towards this end, I was hoping 8.4 would take a critical approach to the research contributions, rather than discussing scholarships more generally. I think that it is quite essential to take a critical lens to especially the work of Chapter 7.  What unintended consequences could result from / be amplified by this work?  
% For instance, could a possible critique of the new approach proposed in Chapter 7 be further overemphasising quantitative metrics? You make great points that “qualitative” needs to be incorporated with “quantitative” for context, but do not discuss why or how – this could really be expanded.  What exactly is it about the qualitative components that shifts one’s reasoning in selection? 
% What about the problem of convergence/amplification; what would happen if all scholarship committees used the approach in Ch 7? Would they all artificially converge on the same candidates, and would this be okay or be essentially overfitting on the quantitative metrics?
% There is also an argument to be said against defending against tokenistic efforts at improving diversity instead of effecting meaningful deep change across demographics. How would SOAI achieve this?


\section{A Critical Reflection on the Position of this Research Within Structures of Power}\label{sec:reflexivity}
In a seminal provocation piece, \textcite{Barocas_Hood_Ziewitz_2013} question whether algorithms challenge or enforce existing power structures; in the case of this thesis, it seems unfortunately clear that these systems enforce existing power structures. For example, \textcite{Ahmed_2012} questions the role of diversity in enabling powerful institutions' dismissal of calls for real change; in helping these organisations better consider diversity, this research may also help them dismiss these calls. More generally designing AI systems for scholarship selection processes (often funded or administered by the most powerful people in the world), we risk contributing to an already vast power disparity between the world's richest and poorest. Scholarships such as those offered by Schmidt, MacArthur, Marshall, or Rhodes, while providing opportunities for individuals from disadvantaged backgrounds, serve to entrench their funders in institutional power structures. By working within these structures and aiming to optimize their selection processes, we may serve to maintain these power dynamics \cite{Ziegler_2008}. By writing this thesis at the University of Oxford, long decried among the most exclusionary institutions in the world \cite{Ziegler_2008}, we benefit from this power imbalance. By distancing this work from the disenfranchised decision subjects that the scholarship claims as its beneficiaries, we risk supporting this disparity in power. 

And yet, without this research, the University of Oxford would still stand. The scholarships of Schmidt, MacArthur, Marshall, and Rhodes would carry on, while selectors make difficult decisions with limited information. While the funding of these programs may carry with it the names of the elite, the programs themselves seek to support the intellectual development of their selected scholars, improve the lives of their target communities, and train talented young people to solve the world's most pressing problems. While the work in this thesis does not undertake to challenge the vast power gap separating decision-maker and decision subject, it does seek to improve the fairness and efficacy of decision-making processes within the existing framework. In doing so, this work enables the fair distribution of intellectual opportunity, seeks out scholars who improve the lives of their community members, and helps solve global problems.

\section{Limitations}
In scenarios outside the selection context, HCI research has generally sought to harmonise the conflicting needs of different user groups through participatory design. Yet, in the case of talent identification, the very nature of the selection process introduces a conflict of incentives between applicants and the selection organisation. Applicants are primarily motivated by the desire to be chosen, while selection organisations, guided by practitioners, seek to optimise the selected cohort according to (biased) measurements of (sometimes idiosyncratic) organisational preferences. This inherent misalignment of objectives raises significant concerns about how to balance the needs of both groups without compromising the overarching social values that SOAI seeks to uphold. Our solution in this thesis has been to centre the organisations and use data from past selections to evaluate their selection decisions based on stated and elicited preferences. However, this solution positions research from the position of the decision-makers, and a lack of engagement with decision subjects may limit the social benefit of the research.

Central to our solution is the notion that the broader social aims of the selection process (e.g., ensuring fairness, diversity, and social benefit) align more closely with practitioners than with applicants and ultimately supersede the immediate interests of both practitioners and applicants. However, we note that some overlap exists between the broader social goals of the selection process and the scholarship applicants' goals. We note that work already exists exploring this from a decision-subject standpoint; \textcite{10.1145/3351095.3372867} find that applicants prefer algorithmic decision-making to human decision-making according to both procedural and distributive notions of fairness. In light of this, SOAI's positioning as a paradigm for building DSTs, rather than a paradigm for building algorithmic decision-making systems, limits the scope of the research. Were it the case that algorithmic decision-making is preferable to algorithmically supported human decision-making, the SOAI paradigm would not be the most effective way to achieve the social aims of selection.

Setting aside the social benefit of the SOAI paradigm as a whole, specifics of SOAI, as it is implemented in this thesis, may limit the work done here. The Decision Matrix framework intentionally elides distinctions between decision points in favour of clarifying a focus on a decision's stage and stakes. This elision was arrived at in concert with participants from Rise, but it may not extend to other decision-making contexts. If it doesn't, though we still call for SOAI work in these contexts, our work may not be directly applicable.

\section{Future Work}
While the work in this thesis articulates SOAI as a paradigm for all AI design oriented around supporting selection problems, we develop and test this paradigm for three specific families of decision points. A straightforward extension of this work would apply SOAI principles to other decision points in selection processes. Natural candidates include: supporting essay judgements with automated essay scoring, where a large body of literature already seeks to score applicant essays via algorithm \cite{cozma_automated_2018,ramesh_automated_2022,wang_use_2022,elijahthesis}, but automated approaches continue to struggle with marking top or bottom essays \cite{elijahthesis}; supporting testing and test evaluation with automated scoring systems \cite{organisciak_beyond_2023,condon2014international}; and supporting pre-application portions of the outreach process, which was requested by several participants in Chapter \ref{ch:diversity}.

Though SOAI, as we investigate it here, aligns most closely with the interests of selectors, there is a need for human-centric work seeking to determine applicant perceptions of positive social outcomes. While work exists examining applicant perceptions of decisions made about them, \cite{pandey_applicants_2022,horodyski_applicants_2023}, this work often approaches research from a fairness or decision-subject-empowerment perspective. No work exists approaching applicant perspectives from the perspective of the ultimate social benefit of selection. Future work should seek to understand how applicants perceive the social outcomes of selection decisions, and how these perceptions can be used to design more effective AI systems for selection.

Though the Decision Matrix provides a useful framework for categorising decision points in selection processes, it is not exhaustive. Future work should seek to augment the Decision Matrix with additional axes that capture the complexity of selection decisions more fully. In particular, though selectors drew a distinction between individual- and group-level in-process decisions in Chapter \ref{ch:diversity} (and though the design prototypes reflect this distinction), the Decision Matrix does not capture this distinction. Future work should seek to augment the Decision Matrix to distinguish individual- from group-level distinctions and implement more individual-level decision support systems in practice.\footnote{Chapters \ref{ch:genai} and \ref{ch:spf} both avoid individual-level implementations in real decision-making pipelines due to risks associated with introduced unfairness or bias \cite{hartigan_fairness_1989,barocas2023fairness,pmlr-v80-kearns18a,Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024,liang_gpt_2023}. Any work implementing tools at the individual-level should consider these risks first.}

\section{Conclusion}
In this thesis, we pioneer a new paradigm of AI design for selection processes, Selection-Oriented AI (SOAI). Chapters \ref{ch:xai} and \ref{ch:genai} find that existing AI systems often fail to meet the needs of selectors, particularly for in-process decision-making. In response, we propose a new paradigm, SOAI, which seeks to centre the design of AI DSTs not around the selectors but around the social aims of selection they seek to practice. Chapters \ref{ch:diversity} and \ref{ch:spf} apply SOAI principles to design DSTs to support considerations of diversity in selection; we implement a prototype designed to satisfy selector desires and find that it improves both diversity and performance outcomes in selection. We then provide a set of design recommendations for SOAI designers, including focusing on specific social values, identifying decision points with the Decision Matrix, balancing quantitative and qualitative information, and evaluating real change in addition to subjective satisfaction.

More broadly, the use of SOAI to support scholarship-specific selection decisions implies the potential to support and improve related decision-making processes, from other selection contexts (e.g., admissions or hiring) to non-selection decision-making contexts (e.g., programme outreach). With a technology-induced flattening of the world \cite{Friedman_2005}, more candidates from more parts of the world find themselves qualified for opportunities. Add to this the ease of application submission created by genAI assistants, and it is clear why applications to job, university, and scholarship opportunities have seen a dramatic increase in recent years \cite{Kaashoek2024Impact}. In light of this, we conclude with a call for SOAI across selection contexts; the need has never been more pressing.
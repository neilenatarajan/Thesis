\chapter{\label{ch:discussion}Discussion}

\minitoc 
% High Level: This chapter could use more unifying reflection on work as a whole, and an ethical reflection of Chapter 7.  
% I think it would be great to have a short discussion on generalisability to other selection tasks.
% I also think it would be great to have a short discussion on 'what could go wrong' if a scholarship selection committee followed your approach.

\section[The Role of AI Systems in Selection]{The Role of AI Systems in Selection: From Decision Support to Selection-Oriented AI}
\subsection{Current Challenges in AI for Selection}
\subsubsection{To Support or Supplant?}
AI tools have long been posed as both replacements for and supports to human decision-makers, both within and outside selection processes \cite{barocas_big_2016,jacobs_how_2021,hildebrandt_law_nodate,yarger2020algorithmic,mattu_how_nodate}. Naïve implementations of AI often supplant human decision-makers, sometimes to disastrous effect \cite{mattu_how_nodate}. As such, we must be careful to design AI systems that support, rather than supplant, human decision-makers.

\subsubsection{Who to Support?}
More human-centric AI systems, such as Explainable AI (XAI), often serve as Decision Support Tools (DSTs) that place the stakeholder—in this case, the selector—at the core of the decision-making process. These systems focus on enhancing the experience of human decision-makers, often by satisfying their subjective desiderata. However, as \textcite{Lipton} argues, post-hoc explanations designed to satisfy these desires may prove more misleading than insightful. In Chapter \ref{ch:xai}, we extend this critique to post-hoc justifications more broadly and argue it applies to all AI DSTs that prioritise user satisfaction over objective outcomes.

Furthermore, selection involves a complex interplay of interests. Organisations aim to identify the best possible cohort, while candidates seek to be selected. These two interests are often at odds. Beyond these immediate stakeholders, society has a vested interest in ensuring that selection processes uphold values like fairness, diversity, and justice. In pro-social programmes like global scholarships, society also has an interest in ensuring that the most deserving and impactful scholars are chosen.

Centring DSTs solely on the needs of either applicants or selectors would not only suffer from the problem of subjective desires identified by \textcite{Lipton} but would also fail to address the broader social implications of selection. Thus, there is a need for DSTs that orient themselves around the social values of selection itself, rather than the preferences of any single stakeholder group.\footnote{We recognise there that selectors' preferences often overlap with these social goals, while other stakeholders may have conflicting preferences. Furthermore, as selectors form the relevant decision-making processes, achieving the social values of selection often requires satisfying the preferences of selectors. Thus, while we centre the DSTs around the social values of selection, we also frequently consider the preferences of selectors.}

\subsubsection{What to Support?}
The conventional DST paradigm often assumes a series of similar decisions on different cases, such as deciding whether to grant a loan \cite{GiveMeSomeCredit,barocas_hidden_2020,ustun_actionable_2019,Rebitschek_Gigerenzer_Wagner_2021,10.1111/j.1467-954X.2007.00740.x}. However, the decision points we enumerated in Table \ref{tab:full_decision_list} are not all captured by this model. The conventional paradigm focuses on a specific type of in-process decision, excluding other in-process decisions and all ex-post decisions. A paradigm for selection-oriented DSTs should therefore seek to support a wider range of decision types.

\subsection{Proposing a New Paradigm: Selection-Oriented AI (SOAI)}
In response to these challenges, this thesis proposes a novel paradigm: Selection-Oriented AI (SOAI). SOAI reimagines the role of AI in talent identification, advocating for a shift away from a purely human-centric framework toward a hybrid selector-centred and selection-driven approach. In this paradigm, the design of AI systems is grounded in the social values that selection processes ought to uphold, with those values supplied and interpreted by the selectors making the decisions. While selectors remain the primary users, they are not the sole focus. Instead, SOAI emphasises evaluating the broader social goals of selection and helping selectors achieve them \cite{batyavalue}.

The shift toward SOAI represents a necessary evolution in AI for talent identification. By prioritising the social values that selection processes ought to reflect, SOAI challenges the current practitioner-centred approach and introduces a new standard for evaluating the success of AI in decision support. It offers a path toward creating AI systems that not only assist in identifying talent but also ensure that the selection process itself is fair, just, and inclusive.

\section{Design Recommendations for SOAI Designers}
\subsection{Design for Specific Social Values}
While we wish to design to support all social values in the selection process, Chapter \ref{ch:diversity} demonstrates the difficulty of unpacking the social value of diversity; in Chapter \ref{ch:diversity}, we find success instead focusing on smaller component values that comprise diversity. Similarly, each decision point supported in Chapter \ref{ch:genai} implies a specific ontology about the role of generative AI in selection; these, too, stem from specific social values promoted by an organisation.

There is support for this from the literature. For example, literature on algorithmic fairness has long wrestled with contradictions between measurements of different kinds of fairnesses \cite{pmlr-v80-kearns18a}. While `individual fairness' draws on procedural notions of justice to ensure that applicants are treated equally regardless of differences in protected or irrelevant characteristics \cite{dwork_fairness_2012}, `group fairness' draws on distributive justice in seeking to achieve parity between different demographic groups \cite{Citron_2008,Olsaretti_2018}. There exists literature attempting to reconcile these notions: \textcite{pmlr-v28-zemel13} attempt to reconcile this in practice by simultaneously optimising for multiple fairness metrics; \textcite{lahoti2019ifairlearningindividuallyfair} seek only to optimise for individual fairness, and yet find increases in group fairness; and \textcite{binns_apparent_2019} contends that standard, blunt implementations of individual fairness should be replaced with a more nuanced formulation compatible with group fairness. Nonetheless, as they are often implemented, these two notions of fairness are often in conflict, and though designing to support both may be possible, it is liable, in a scholarship context, to create unclarity of the sort plaguing diversity, impeding programme desire to assess these concepts with specific targets. 

We suggest this generalises to SOAI practices in general: rather than designing around myriad values, only to find conflicting design implications of these disparate values, designers seeking to support social values in selection processes should focus on specific social values worthy of consideration.

\subsection{Identify Decision Points with the Decision Matrix}
In Chapter \ref{ch:context}, we conceive of selection as a series of decisions. Chapter \ref{ch:genai} expands on this, introducing the Decision Matrix framework to categorise the many decision points that selectors face according to their two most germane axes: the stakes of the decision and its stage in selection. This framework allows designers to reason about groups of decision points in much the same way that the explainable AI community reasons about groups of explainability techniques and to isolate desired or required properties of DSTs based on the taxonomy of the decision point they seek to support and to then determine which categories of decisions different GenAI detectors are suitable to support\cite{ford_play_2020,kumar_problems_2020,doshi-velez_towards_2017,friedrich_taxonomy_2011,molnar_interpretable_2019}. 

In Chapter \ref{ch:xai}, we respond to criticisms isolated to \textcite{friedrich_taxonomy_2011}'s `post-hoc' explanations; here, the taxonomic distinctions are used in criticism to expand the scope of individual critiques \cite{barocas_hidden_2020,kumar_problems_2020}. We suggest the Decision Matrix can be used similarly, to discuss and critique decisions in a scholarship context.

However, we caution designers following this design recommendation to ensure that they also follow design recommendation \ref{ssec:real_change} and evaluate real change in addition to subjective desiderata. The Decision Matrix framework can be used to derive a set of a necessary, but perhaps not sufficient, properties of DSTs.

\subsection{Balance Qualitative and Quantitative Information in Presentation}
Human decision-makers often desire both a qualitative understanding of applicants and quantitative metrics to compare them. In Chapters \ref{ch:xai} and \ref{ch:diversity}, we find that selectors from Rise and Ellison Scholars seek to make decisions informed by both kinds of information; despite this, the desired balance between these modes of information varies based both on practitioner and type of decision. When quantitative information is neglected, practitioners are forced to make decisions on a case-by-case basis without important numerical context comparing applicants to a larger group; when qualitative information is neglected, practitioners are unable to consider applicants holistically. Developers following SOAI should consider the balance between quantitative and qualitative information in their systems, and design their systems to provide both when necessary.

We again find parallels in the fairness literature to this balance. Qualitative information enables the selectors' consideration of applicants as individuals, and this combines with the process of holistic review to create full pictures of applicants. \cite{dwork_fairness_2012}'s individual fairness holds a similar lens; rather than looking at applicants in terms of their place in the cohort, this notion of fairness demands equal treatment of applicants as people. However, quantitative information makes possible considerations of distributive notions of justice and group fairness principles \cite{Olsaretti_2018}, as decision-makers have access to the supporting information needed to contextualise applications relative to other members of protected groups. Notably, programmes with different ontologies governing what they consider fair will thus have different preferences considering the balance of quantitative and qualitative information in their systems. (This relationship is not absolute, though, as other differences in programmes may lead to differing priorities.)

\subsection{Evaluate Real Change in Addition to Subjective Satisfaction}\label{ssec:real_change}
\textcite{Lipton} critiques explainable AI (XAI) systems because they risk satisfying the subjective desires of the users while failing to improve objective outcomes. In Chapter \ref{ch:xai}, we confirm that this critique applies to some post-hoc justifications of model recommendations, as the justifications were found to yield an unwarranted increase in trust in human decision-makers. Thus, it is important to define and evaluate measures of the social values that DSTs intend to support; when evaluating these DSTs, they should not be evaluated human-centrically (i.e., according to their users' satisfaction), but should instead be evaluated on whether their employment improves social outcomes.

The fundamental challenge with evaluating ``real change'' in a selection context is the lack of ground truth; i.e., there are not, in general, ``correct'' or ``incorrect'' selection decisions, only those preferred by the organisation. In this thesis, we solve this problem by working with programmes to define measurable criteria that act as a surrogate for ``correct selection decisions''. In Chapter \ref{ch:xai}, these criteria are arrived at through an Action Research (AR) process and expressed in Figure \ref{fig:desiderata_matrix}, while in Chapter \ref{ch:spf}, these criteria are supplied directly by Rise, as the programme has internal metrics for both axes of the SPF. We recommend designers work with organisations to define surrogate criteria that can be used to evaluate the success of their systems.

\section{Implications}
\subsection{Algorithmic Fairness in a Selection Context}\label{ssec:fairness}
The work in this thesis has implications for the broader discussion of algorithmic fairness in selection processes. The design of SOAI DSTs has the potential to impact the lives of many of the world's most vulnerable people; it is thus imperative that these systems are designed fairly. However, the notion of fairness itself is complex and multifaceted. As \textcite{pmlr-v80-kearns18a} highlight, fairness can be understood in both procedural and distributive terms, and different methods of achieving fairness across different subgroups often conflict. Individual fairness is often discussed in the algorithmic fairness literature \cite{dwork_fairness_2012}; this is often contrasted with ``group'' fairness \cite{fleisher_whats_nodate,binns_apparent_2019,barocas2023fairness,Friedler_Scheidegger_Venkatasubramanian_2016}. Despite attempts to reconcile these differing notions of fairness, such as those by \textcite{binns_apparent_2019}, contradictions remain between metrics used to measure different forms of fairness; that is, decisions that may be ruled more fair by certain individual or procedural fairness measures might create group or distributive unfairness. What's worse, scholars disagree even on the best implementations of notions of fairness \cite{Friedler_Scheidegger_Venkatasubramanian_2016,binns_apparent_2019}, and differing interpretations conflict.

It is worth noting, then, that the work on generative AI detection in Chapter \ref{ch:genai} is built on a desire for procedural fairness, while the diversity goals of Chapters \ref{ch:diversity} and \ref{ch:spf}, in practice, accord closely with distributive notions. This raises the possibility that, via SOAI methods, researchers could determine socially beneficial fairness metrics to uphold in DSTs and build to support those.\footnote{This work, in particular, should not be done solely from the decision-maker's perspective. \textcite{10.1145/3351095.3372867} investigate applicant perceptions of appropriate fairness metrics; this work may be a good starting point for SOAI work in this field.}

%  - critical attention on algorithms Kitchin_2017
% I would be remiss to ignore the critical attention that algorithms have garnered as they proliferate through society. NISSENBAUM1998237 

\subsection{New Developments in AI for Selection}
The growing popularity of GenAI has already dramatically increased the number of applications that job, university, and scholarship programmes must select from \cite{Kaashoek2024Impact}. While a blanket ban on GenAI in application-writing may solve this \cite{h_holden_thorp_chatgpt_2023}, we find in Chapter \ref{ch:genai} that such a ban is unenforcible at present. We note in Chapter \ref{ch:genai} that our research is complicated by the rapidly changing nature of both GenAI and detectors. Here, we extend this complication to SOAI as a whole. It may be that, as GenAI development moves beyond retrieval-augmented generation to more complex architectures \cite{lewis2020retrieval}, such as integrated reasoning systems or agentic AI \cite{OKeefeetal}, these systems will once again fundamentally change the process of selection. In light of this, SOAI is necessary to ensure that new, more powerful AI systems further the social aims of selection processes.

Of particular interest would be the development of AI systems capable of encoding domain knowledge in their structures, which could support decisions in a fundamentally different way. This could be particularly useful in automated essay scoring, where domain-specific knowledge is a significant problem \cite{elijahthesis}. If this is the case, then the work done in this thesis may serve as a precursor to the development of these systems and a guide for how to ensure that these systems are designed to support the social aims of selection processes.

\section{A Critical Reflection on the Position of this Research Within Structures of Power}\label{sec:reflexivity}

\subsection{Critical Assessment of Research Contributions}
While this thesis proposes SOAI as a paradigm for more socially conscious AI design, it is essential to critically examine its potential unintended consequences, particularly those arising from the approach to diversity measurement and optimisation in Chapter \ref{ch:spf}.

\subsubsection{The Risk of Quantitative Overemphasis and Tokenism}
The Selection Possibilities Frontier (SPF) framework developed in Chapter \ref{ch:spf}, while intended to balance quantitative and qualitative information, risks amplifying the tendency to quantify human worth. By creating numerical representations of diversity and performance, we risk reducing complex human experiences to algorithmic inputs. This is particularly concerning because, as we argue throughout this thesis, qualitative components are crucial for understanding an applicant's lived experiences, contextual challenges, and unique perspectives. They provide the scaffolding that allows selectors to understand not just what an applicant has achieved, but how and why those achievements occurred. Our framework, if misused, could encourage selectors to treat these vital qualitative insights as merely supplementary.

Perhaps more critically, the diversity measurement approaches developed here risk enabling a sophisticated form of tokenism. By providing tools that allow programmes to demonstrate measurable improvements in diversity metrics, these systems may satisfy an organisational desire to appear inclusive while failing to address the deeper structural inequalities that cause exclusion. The SOAI paradigm, while oriented toward social values, operates within existing institutional frameworks rather than challenging them. As \textcite{Ahmed_2012} argues, ``diversity work'' can create the appearance of inclusion while leaving fundamental power structures intact. Our tools could inadvertently become part of this dynamic, helping powerful institutions deflect calls for more fundamental reform by pointing to their use of socially conscious AI as evidence of their commitment to equity.

\subsubsection{The Convergence Problem and Algorithmic Monoculture}
A second major concern is the risk of algorithmic convergence. If multiple scholarship programmes adopt similar SPF-based approaches, they may begin selecting for overlapping pools of candidates who excel according to the same quantitative metrics. This could create a new form of algorithmic bias where certain types of applicants—those who perform well on the specific measures captured by these systems—receive disproportionate opportunities, while other forms of excellence are systematically undervalued. This algorithmic monoculture would undermine the very diversity goals these systems are designed to support.

This risk is amplified by the tendency of machine learning systems to optimise for what is easily measurable. If all programmes converge on similar optimisation targets, we risk a narrowing of what is considered valuable, systematically disadvantaging those whose strengths lie outside these predefined frameworks.

\subsection{Broader Implications for Power Structures}
In a seminal piece, \textcite{Barocas_Hood_Ziewitz_2013} ask whether algorithms challenge or reinforce existing power structures. In the case of this thesis, the answer is complex. While SOAI seeks to improve fairness, it operates fundamentally within existing institutional frameworks that themselves can perpetuate inequality. The scholarships examined here—funded by major philanthropic organisations—provide invaluable opportunities for individuals but also serve to entrench their funders in institutional power structures \cite{Ziegler_2008}. By designing AI for these programmes, we may be contributing to the legitimation of these structures.

The generalisability of SOAI to other high-stakes contexts like hiring and university admissions raises additional concerns. Widespread adoption could amplify the risks identified above, potentially creating a `selection-industrial complex' where a narrow set of algorithmic approaches dominates opportunity allocation across society.

\subsection{What Could Go Wrong: Key Failure Modes}\label{ssec:wrongs}
Several specific failure modes could emerge from the widespread adoption of the approaches developed in this thesis:

\begin{itemize}
    \item \textbf{Metric Gaming:} As programmes become more transparent about their metrics, applicants may develop strategies to game them, undermining their validity and creating new advantages for those with the resources to understand and exploit the system.
    \item \textbf{False Precision:} The mathematical sophistication of the SPF may create an illusion of objectivity that masks the subjective value judgments embedded within it, making it harder to critique or adjust these systems when they produce problematic outcomes.
    \item \textbf{Institutional Complacency:} By providing tools that demonstrate measurable progress on diversity metrics, these systems may reduce the pressure for more fundamental, and more difficult, institutional reforms.
\end{itemize}

\subsection{Guarding Against Failure: Deliberation, Contestability, and Iteration}\label{ssec:deliberation_contestability_iteration}
Given these critical risks, SOAI systems must be designed not as static, authoritative solutions, but as evolving tools that support ongoing human deliberation and are open to contestation. This requires several safeguards:

\begin{itemize}
    \item \textbf{Transparency in Values and Trade-offs:} Systems must make their embedded values (e.g., specific definitions of diversity) and trade-offs transparent to selectors. This includes clearly articulating how different metrics are weighted and how qualitative information is incorporated or potentially sidelined.
    \item \textbf{Mechanisms for Contestation:} Selectors and other stakeholders must have avenues to question, critique, and challenge system outputs. This could involve features that allow users to flag problematic recommendations, suggest alternative interpretations of data, or adjust system parameters under controlled conditions.
    \item \textbf{Support for Iterative Refinement:} SOAI tools should be built with the expectation that they will require regular review. This includes designing for the easy updating of models and metrics as organisational goals evolve or as unintended consequences are identified.
    \item \textbf{Interfaces for Qualitative Nuance:} To counteract quantitative overemphasis, interfaces must actively encourage the integration of qualitative nuance. This might involve dashboards that juxtapose quantitative scores with rich qualitative summaries or tools that help selectors document and weigh contextual factors that are not easily quantified.
\end{itemize}

Designing for deliberation, contestability, and iteration aims to foster a more responsible and adaptive use of AI in selection. It positions SOAI not as a replacement for human judgment but as a catalyst for more informed, reflective, and ethically aware decision-making. This approach acknowledges that achieving real change requires continuous engagement with the complexities and potential pitfalls of AI-driven selection.

\subsection{Reconciling Critique with Pragmatic Impact}
Despite these significant concerns, this research operates within a pragmatic context. Without it, the institutional structures examined would continue their existing selection processes, likely with less information and greater inconsistency. While this work does not dismantle the structures that concentrate opportunity, it does seek to improve the fairness and efficacy of decision-making within them.

The tension here is between the perfect and the better. While an ideal solution might be to entirely restructure how opportunity is allocated, the practical reality is that these institutions persist. Given their persistence, there is value in ensuring the opportunities they provide are distributed as fairly and effectively as possible, even while acknowledging that such improvements may inadvertently legitimise the broader system. Future work must actively monitor for the failure modes identified here and be accompanied by transparency about the limitations of these systems and the structural inequalities they cannot resolve.

\section{Limitations}
In scenarios outside selection, HCI research often seeks to harmonise the needs of different user groups. In talent identification, however, the conflict of incentives between applicants and selectors is inherent. Our solution has been to centre the organisations, using their stated preferences and past data to evaluate their decisions. This positions the research from the perspective of the decision-makers and may limit the social benefit by not fully engaging with the perspectives of decision subjects.

We similarly assume that the broader social aims of selection align more closely with practitioners than with applicants. However, there is evidence that applicants themselves value fairness and may even prefer algorithmic decision-making in some contexts \cite{10.1145/3351095.3372867}. By positioning SOAI as a paradigm for DSTs rather than for fully automated systems, we may be limiting its potential impact if algorithmic decision-making proves to be more effective at achieving social goals.

Finally, while the Decision Matrix framework is a useful tool, it intentionally elides certain distinctions to focus on stage and stakes. This simplification, while developed in concert with our partners, may not generalise to all contexts. Furthermore, our focus on improving selection within existing applicant pools means we do not address the broader issues of access and outreach that determine who applies in the first place—a significant limitation, given that the most profound inequities often occur before the formal selection process begins.

\section{Future Work}
While the work in this thesis articulates SOAI as a paradigm for all AI design oriented around supporting selection problems, we develop and test this paradigm for three specific families of decision points. A straightforward extension of this work would apply SOAI principles to other decision points in selection processes. Natural candidates include: supporting essay judgements with automated essay scoring, where a large body of literature already seeks to score applicant essays via algorithm \cite{cozma_automated_2018,ramesh_automated_2022,wang_use_2022,elijahthesis}, but automated approaches continue to struggle with marking top or bottom essays \cite{elijahthesis}; supporting testing and test evaluation with automated scoring systems \cite{organisciak_beyond_2023,condon2014international}; and supporting pre-application portions of the outreach process, which was requested by several participants in Chapter \ref{ch:diversity}.

Though SOAI, as we investigate it here, aligns most closely with the interests of selectors, there is a need for human-centric work seeking to determine applicant perceptions of positive social outcomes. While work exists examining applicant perceptions of decisions made about them, \cite{pandey_applicants_2022,horodyski_applicants_2023}, this work often approaches research from a fairness or decision-subject-empowerment perspective. No work exists approaching applicant perspectives from the perspective of the ultimate social benefit of selection. Future work should seek to understand how applicants perceive the social outcomes of selection decisions, and how these perceptions can be used to design more effective AI systems for selection.

Though the Decision Matrix provides a useful framework for categorising decision points in selection processes, it is not exhaustive. Future work should seek to augment the Decision Matrix with additional axes that capture the complexity of selection decisions more fully. In particular, though selectors drew a distinction between individual- and group-level in-process decisions in Chapter \ref{ch:diversity} (and though the design prototypes reflect this distinction), the Decision Matrix does not capture this distinction. Future work should seek to augment the Decision Matrix to distinguish individual- from group-level distinctions and implement more individual-level decision support systems in practice.\footnote{Chapters \ref{ch:genai} and \ref{ch:spf} both avoid individual-level implementations in real decision-making pipelines due to risks associated with introduced unfairness or bias \cite{hartigan_fairness_1989,barocas2023fairness,pmlr-v80-kearns18a,Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024,liang_gpt_2023}. Any work implementing tools at the individual-level should consider these risks first.}

\section{Conclusion}
In this thesis, we pioneer a new paradigm of AI design for selection processes, Selection-Oriented AI (SOAI). Chapters \ref{ch:xai} and \ref{ch:genai} find that existing AI systems often fail to meet the needs of selectors, particularly for in-process decision-making. In response, we propose a new paradigm, SOAI, which seeks to centre the design of AI DSTs not around the selectors but around the social aims of selection they seek to practice. Chapters \ref{ch:diversity} and \ref{ch:spf} apply SOAI principles to design DSTs to support considerations of diversity in selection; we implement a prototype designed to satisfy selector desires and find that it improves both diversity and performance outcomes in selection. We then provide a set of design recommendations for SOAI designers, including focusing on specific social values, identifying decision points with the Decision Matrix, balancing quantitative and qualitative information, and evaluating real change in addition to subjective satisfaction.

More broadly, the use of SOAI to support scholarship-specific selection decisions implies the potential to support and improve related decision-making processes, from other selection contexts (e.g., admissions or hiring) to non-selection decision-making contexts (e.g., programme outreach). With a technology-induced flattening of the world \cite{Friedman_2005}, more candidates from more parts of the world find themselves qualified for opportunities. Add to this the ease of application submission created by GenAI assistants, and it is clear why applications to job, university, and scholarship opportunities have seen a dramatic increase in recent years \cite{Kaashoek2024Impact}. In light of this, we conclude with a call for SOAI across selection contexts; the need has never been more pressing.
\chapter{\label{ch:discussion}Discussion}

\minitoc 
% High Level: This chapter could use more unifying reflection on work as a whole, and an ethical reflection of Chapter 7.  
% I think it would be great to have a short discussion on generalisability to other selection tasks.
% I also think it would be great to have a short discussion on 'what could go wrong' if a scholarship selection committee followed your approach.

\section[The Role of AI Systems in Selection]{The Role of AI Systems in Selection: From Decision Support to Selection-Oriented AI}
\subsection{Current Challenges in AI for Selection}
\subsubsection{To Support or Supplant?}
AI tools have long been posed as both replacements and support systems for decision-makers both without and within selection processes \cite{barocas_big_2016,jacobs_how_2021,hildebrandt_law_nodate,yarger2020algorithmic,mattu_how_nodate}. Naïvely, implementations of AI systems supplant human decision-makers, often to disastrous effect \cite{mattu_how_nodate}.

\subsubsection{Who to Support?}
More human-centric AI systems, e.g., explainable AI (XAI), often instead serve as decision support tools (DSTs), and place the stakeholder (the selector) at the core of the decision-making process. These systems then focus on enhancing the experience of human decision-makers, offering tools that satisfy the subjective desiderata of selectors. \textcite{Lipton} critiques post-hoc notions of explainability because, in seeking to satisfy stakeholder desiderata, XAI tools may prove more misleading than insightful. While Chapter \ref{ch:xai} extends this critique to the practice of post-hoc justification more broadly, we find it applicable to all AI DSTs.

In addition to this, the problem of selection involves a complex interplay of interests. On the one hand, selection organisations are driven by the need to identify the best possible cohort of candidates, aiming to optimize organisational performance and success. On the other hand, candidates are motivated by the desire to be selected for opportunities that match their potential and aspirations. In practice, these two interests are at odds, and while DSTs might support one or the other, the need to support both decisions outstrips the capacity of any individual DST. But beyond these two core stakeholders lies a broader societal interest in the outcomes of selection processes. Society at large has a vested interest in ensuring that these processes uphold essential social values such as fairness, diversity, integrity, and justice. In the case of pro-social programmes such as global scholarships that desire that scholars do good with their careers, society similarly has an interest in ensuring that the most apt scholars are selected.

Centring these DSTs solely on the needs of either applicants or decision-makers within organisations would both suffer from the problem of subjective desires outlined by \textcite{Lipton} and fail to address the larger social implications of these selection decisions.\footnote{I.e., algorithms designed to streamline applicant evaluations may inadvertently reinforce existing biases, thereby undermining efforts to promote diversity and inclusion. Similarly, AI systems may prioritise ease of use and decision-making speed at the expense of fairness and transparency.} Thus, there is a need for DSTs that, rather than centring on a group of stakeholders, orient themselves around the task of selection itself.

\subsubsection{What to Support?}
A common DST paradigm sees practitioners making a series of similar kinds of decisions on different cases, e.g., deciding whether to grant a loan many times \cite{GiveMeSomeCredit,barocas_hidden_2020,ustun_actionable_2019,Rebitschek_Gigerenzer_Wagner_2021,10.1111/j.1467-954X.2007.00740.x}. However, we have explored here many challenges and decision points (enumerated in Table \ref{tab:full_decision_list}) not captured by this notion of decision-making. I.e., the conventional DST paradigm, wherein practitioners are supported for each decision, thinks only of a specific kind of in-process decision. It excludes both other in-process decisions and all ex-post decisions. Thus, any paradigm for DSTs oriented around the task of selection should seek to support all kinds of decisions.

\subsection{Proposing a New Paradigm: Selection-Oriented AI (SOAI)}
In response to these challenges, this thesis proposes a novel paradigm: SOAI. SOAI reimagines the role of AI systems in talent identification, and advocates for a shift away from the human-centric framework toward a hybrid selector-centred and selection-driven approach (wherein the design of AI systems is grounded in the social values that selection processes ought to uphold, but the values are supplied by the selectors making the decisions). While selectors remain important users of these systems, they are not the sole focus of design efforts. Instead, SOAI emphasises the importance of evaluating the broader social goals of AI-driven selection processes and seeks to help selectors achieve these goals \cite{batyavalue}.

The shift toward SOAI represents a necessary evolution in the use of AI systems in talent identification. By prioritising the social values that selection processes ought to reflect, SOAI challenges the current practitioner-centred approach and introduces a new standard for evaluating the success of AI in decision support. The implementation of SOAI can provide a path forward in creating AI systems that not only assist in the identification of talent but also ensure that the processes by which talent is selected are fair, just, and inclusive.

\section{Design Recommendations for SOAI Designers}
\subsection{Design for Specific Social Values}
While we wish to design to support all social values in the selection process, Chapter \ref{ch:diversity} demonstrates the difficulty of unpacking the social value of diversity; in Chapter \ref{ch:diversity}, we find success instead focusing on smaller component values that comprise diversity. Similarly, each decision point supported in Chapter \ref{ch:genai} implies a specific ontology about the role of generative AI in selection; these, too, stem from specific social values promoted by an organisation.

There is support for this from the literature. For example, literature on algorithmic fairness has long wrestled with contradictions between measurements of different kinds of fairnesses \cite{pmlr-v80-kearns18a}. While `individual fairness' draws on procedural notions of justice to ensure that applicants are treated equally regardless of differences in protected or irrelevant characteristics \cite{dwork_fairness_2012}, `group fairness' draws on distributive justice in seeking to achieve parity between different demographic groups \cite{Citron_2008,Olsaretti_2018}. There exists literature attempting to reconcile these notions: \textcite{pmlr-v28-zemel13} attempt to reconcile this in practice by simultaneously optimising for multiple fairness metrics; \textcite{lahoti2019ifairlearningindividuallyfair} seek only to optimise for individual fairness, and yet find increases in group fairness; and \textcite{binns_apparent_2019} contends that standard, blunt implementations of individual fairness should be replaced with a more nuanced formulation compatible with group fairness. Nonetheless, as they are often implemented, these two notions of fairness are often in conflict, and though designing to support both may be possible, it is liable, in a scholarship context, to create unclarity of the sort plaguing diversity, impeding programme desire to assess these concepts with specific targets. 

We suggest this generalises to SOAI practices in general: rather than designing around myriad values, only to find conflicting design implications of these disparate values, designers seeking to support social values in selection processes should focus on specific social values worthy of consideration.

\subsection{Identify Decision Points with the Decision Matrix}
In Chapter \ref{ch:context}, we conceive of selection as a series of decisions. Chapter \ref{ch:genai} expands on this, introducing the Decision Matrix framework to categorise the many decision points that selectors face according to their two most germane axes: the stakes of the decision and its stage in selection. This framework allows designers to reason about groups of decision points in much the same way that the explainable AI community reasons about groups of explainability techniques and to isolate desired or required properties of DSTs based on the taxonomy of the decision point they seek to support and to then determine which categories of decisions different GenAI detectors are suitable to support\cite{ford_play_2020,kumar_problems_2020,doshi-velez_towards_2017,friedrich_taxonomy_2011,molnar_interpretable_2019}. 

In Chapter \ref{ch:xai}, we respond to criticisms isolated to \textcite{friedrich_taxonomy_2011}'s `post-hoc' explanations; here, the taxonomic distinctions are used in criticism to expand the scope of individual critiques \cite{barocas_hidden_2020,kumar_problems_2020}. We suggest the Decision Matrix can be used similarly, to discuss and critique decisions in a scholarship context.

However, we caution designers following this design recommendation to ensure that they also follow design recommendation \ref{ssec:real_change} and evaluate real change in addition to subjective desiderata. The Decision Matrix framework can be used to derive a set of a necessary, but perhaps not sufficient, properties of DSTs.

\subsection{Balance Qualitative and Quantitative Information in Presentation}
Human decision-makers often desire both a qualitative understanding of applicants and quantitative metrics to compare them. In Chapters \ref{ch:xai} and \ref{ch:diversity}, we find that selectors from Rise and Ellison Scholars seek to make decisions informed by both kinds of information; despite this, the desired balance between these modes of information varies based both on practitioner and type of decision. When quantitative information is neglected, practitioners are forced to make decisions on a case-by-case basis without important numerical context comparing applicants to a larger group; when qualitative information is neglected, practitioners are unable to consider applicants holistically. Developers following SOAI should consider the balance between quantitative and qualitative information in their systems, and design their systems to provide both when necessary.

We again find parallels in the fairness literature to this balance. Qualitative information enables the selectors' consideration of applicants as individuals, and this combines with the process of holistic review to create full pictures of applicants. \cite{dwork_fairness_2012}'s individual fairness holds a similar lens; rather than looking at applicants in terms of their place in the cohort, this notion of fairness demands equal treatment of applicants as people. However, quantitative information makes possible considerations of distributive notions of justice and group fairness principles \cite{Olsaretti_2018}, as decision-makers have access to the supporting information needed to contextualise applications relative to other members of protected groups. Notably, programmes with different ontologies governing what they consider fair will thus have different preferences considering the balance of quantitative and qualitative information in their systems. (This relationship is not absolute, though, as other differences in programmes may lead to differing priorities.)

\subsection{Evaluate Real Change in Addition to Subjective Satisfaction}\label{ssec:real_change}
\textcite{Lipton} critiques explainable AI (XAI) systems because they risk satisfying the subjective desires of the users while failing to improve objective outcomes. In Chapter \ref{ch:xai}, we confirm that this critique applies to some post-hoc justifications of model recommendations, as the justifications were found to yield an unwarranted increase in trust in human decision-makers. Thus, it is important to define and evaluate measures of the social values that DSTs intend to support; when evaluating these DSTs, they should not be evaluated human-centrically (i.e., according to their users' satisfaction), but should instead be evaluated on whether their employment improves social outcomes.

The fundamental challenge with evaluating ``real change'' in a selection context is the lack of ground truth; i.e., there are not, in general, ``correct'' or ``incorrect'' selection decisions, only those preferred by the organisation. In this thesis, we solve this problem by working with programmes to define measurable criteria that act as a surrogate for ``correct selection decisions''. In Chapter \ref{ch:xai}, these criteria are arrived at through an Action Research (AR) process and expressed in Figure \ref{fig:desiderata_matrix}, while in Chapter \ref{ch:spf}, these criteria are supplied directly by Rise, as the programme has internal metrics for both axes of the SPF. We recommend designers work with organisations to define surrogate criteria that can be used to evaluate the success of their systems.

\section{Implications}
\subsection{Algorithmic Fairness in a Selection Context}\label{ssec:fairness}
The work in this thesis has implications for the broader discussion of algorithmic fairness in selection processes. The design of SOAI DSTs has the potential to impact the lives of many of the world's most vulnerable people; it is thus imperative that these systems are designed fairly. However, the notion of fairness itself is complex and multifaceted. As \textcite{pmlr-v80-kearns18a} highlight, fairness can be understood in both procedural and distributive terms, and different methods of achieving fairness across different subgroups often conflict. Individual fairness is often discussed in the algorithmic fairness literature \cite{dwork_fairness_2012}; this is often contrasted with ``group'' fairness \cite{fleisher_whats_nodate,binns_apparent_2019,barocas2023fairness,Friedler_Scheidegger_Venkatasubramanian_2016}. Despite attempts to reconcile these differing notions of fairness, such as those by \textcite{binns_apparent_2019}, contradictions remain between metrics used to measure different forms of fairness; that is, decisions that may be ruled more fair by certain individual or procedural fairness measures might create group or distributive unfairness. What's worse, scholars disagree even on the best implementations of notions of fairness \cite{Friedler_Scheidegger_Venkatasubramanian_2016,binns_apparent_2019}, and differing interpretations conflict.

It is worth noting, then, that the work on generative AI detection in Chapter \ref{ch:genai} is built on a desire for procedural fairness, while the diversity goals of Chapters \ref{ch:diversity} and \ref{ch:spf}, in practice, accord closely with distributive notions. This raises the possibility that, via SOAI methods, researchers could determine socially beneficial fairness metrics to uphold in DSTs and build to support those.\footnote{This work, in particular, should not be done solely from the decision-maker's perspective. \textcite{10.1145/3351095.3372867} investigate applicant perceptions of appropriate fairness metrics; this work may be a good starting point for SOAI work in this field.}

%  - critical attention on algorithms Kitchin_2017
% I would be remiss to ignore the critical attention that algorithms have garnered as they proliferate through society. NISSENBAUM1998237 

\subsection{New Developments in AI for Selection}
The growing popularity of GenAI has already dramatically increased the number of applications that job, university, and scholarship programmes must select from \cite{Kaashoek2024Impact}. While a blanket ban on GenAI in application-writing may solve this \cite{h_holden_thorp_chatgpt_2023}, we find in Chapter \ref{ch:genai} that such a ban is unenforcible at present. We note in Chapter \ref{ch:genai} that our research is complicated by the rapidly changing nature of both GenAI and detectors. Here, we extend this complication to SOAI as a whole. It may be that, as GenAI development moves beyond retrieval-augmented generation to more complex architectures \cite{lewis2020retrieval}, such as integrated reasoning systems or agentic AI \cite{OKeefeetal}, these systems will once again fundamentally change the process of selection. In light of this, SOAI is necessary to ensure that new, more powerful AI systems further the social aims of selection processes.

Of particular interest would be the development of AI systems capable of encoding domain knowledge in their structures, which could support decisions in a fundamentally different way. This could be particularly useful in automated essay scoring, where domain-specific knowledge is a significant problem \cite{elijahthesis}. If this is the case, then the work done in this thesis may serve as a precursor to the development of these systems and a guide for how to ensure that these systems are designed to support the social aims of selection processes.

\section{A Critical Reflection on the Position of this Research Within Structures of Power}\label{sec:reflexivity}

\subsection{Critical Assessment of Research Contributions}
While this thesis proposes SOAI as a paradigm shift toward more socially conscious AI design in selection, it is essential to critically examine the potential unintended consequences of these contributions, particularly those arising from Chapter \ref{ch:spf}'s approach to diversity measurement and optimization.

\subsubsection{The Risk of Quantitative Overemphasis}
The SPF framework developed in Chapter \ref{ch:spf}, while designed to balance quantitative and qualitative information, may inadvertently amplify existing tendencies toward quantification of human worth in selection processes. By creating numerical representations of diversity and potential, we risk reducing complex human experiences and identities to algorithmic inputs. The qualitative components that this thesis argues are essential (e.g., understanding an applicant's lived experiences, contextual challenges, and unique perspectives) serve as crucial counterbalances to quantitative metrics precisely because they capture the irreducible complexity of human potential that cannot be adequately represented numerically.

The qualitative aspects shift reasoning in selection by providing the contextual scaffolding that allows selectors to understand not just \emph{what} an applicant has achieved, but \emph{how} and \emph{why} those achievements occurred within their specific circumstances. For instance, a quantitative metric might show that an applicant has lower test scores, but qualitative information reveals that they achieved those scores while working multiple jobs to support their family, fundamentally changing how that metric should be interpreted. However, our framework may inadvertently encourage selectors to treat these qualitative insights as supplementary to, rather than integral with, the quantitative assessments.

\subsubsection{The Convergence Problem}
A particularly concerning potential consequence of widespread SOAI adoption is algorithmic convergence. If multiple scholarship programmes implement similar SPF-based approaches, they may begin selecting overlapping pools of candidates who excel according to the same quantitative metrics. This convergence could create a new form of algorithmic bias where certain types of applicants – those who perform well on the specific measures captured by these systems – receive disproportionate opportunities across multiple programmes.

This convergence risk is amplified by the tendency of machine learning systems to optimize for measurable outcomes, potentially overlooking forms of excellence or potential that are not easily quantified. If all programmes converge on similar optimization targets, we risk creating a narrowing effect where diverse forms of human potential are systematically undervalued, ultimately undermining the very diversity goals these systems purport to support.

\subsubsection{Tokenism vs. Meaningful Change}
Perhaps most critically, the diversity measurement approaches developed in this thesis risk enabling sophisticated forms of tokenism. By providing tools that allow programmes to demonstrate measurable improvements in diversity metrics, these systems may satisfy organizational desires to appear inclusive while failing to address the deeper structural inequalities that create exclusion in the first place.

The SOAI paradigm, while oriented toward social values, operates within existing institutional frameworks rather than challenging them. This positioning may inadvertently help powerful institutions defend against calls for more fundamental reform by pointing to their use of socially conscious AI systems as evidence of their commitment to equity. The metrics we provide could become tools for institutional self-congratulation rather than genuine transformation.

Furthermore, by focusing on optimizing selection within existing applicant pools, these approaches do not address the structural barriers that prevent many talented individuals from applying in the first place. A programme might achieve perfect diversity scores according to our metrics while still systematically excluding entire communities who never enter the application process due to information asymmetries, resource constraints, or cultural barriers.

\subsection{Broader Implications for Power Structures}
In a seminal provocation piece, \textcite{Barocas_Hood_Ziewitz_2013} question whether algorithms challenge or enforce existing power structures; in the case of this thesis, the answer is unfortunately complex. While SOAI seeks to improve fairness within selection processes, it operates fundamentally within existing institutional frameworks that themselves perpetuate inequality.

For example, \textcite{Ahmed_2012} questions the role of diversity in enabling powerful institutions' dismissal of calls for real change; by helping these organisations better measure and optimize diversity, this research may inadvertently provide them with sophisticated tools to deflect criticism while maintaining fundamental power imbalances. The scholarships examined in this work – funded by Schmidt, MacArthur, Marshall, Rhodes, and similar benefactors – while providing opportunities for individuals from disadvantaged backgrounds, ultimately serve to entrench their funders in institutional power structures \cite{Ziegler_2008}.

By designing AI systems for these scholarship selection processes, we may be contributing to what \textcite{Ahmed_2012} describes as ``diversity work'' – efforts that create the appearance of inclusion while leaving fundamental power structures intact. The technical sophistication of our approaches may even make this dynamic more problematic, as it becomes harder to critique systems that appear to embody progressive values through their use of advanced AI for social good.

\subsection{Generalizability and Systemic Risks}
The generalizability of SOAI approaches to other selection contexts – hiring, university admissions, programme admissions – raises additional concerns about systemic impact. While this thesis focuses on scholarship selection, the principles and techniques developed here are likely to be adopted more broadly. This expansion could amplify the risks identified above across multiple domains of opportunity allocation.

If SOAI principles become widely adopted across selection contexts, the convergence problem becomes particularly acute. We might see the emergence of a selection-industrial complex where similar algorithmic approaches create systematic advantages for candidates who perform well according to the specific metrics these systems optimize for, while systematically disadvantaging those whose strengths lie outside these frameworks.

\subsection{What Could Go Wrong: Failure Modes}\label{ssec:wrongs}
Several specific failure modes emerge from widespread adoption of the approaches developed in this thesis:

\textbf{Metric Gaming:} As programmes become more sophisticated in measuring diversity and potential, applicants and their advisors may develop strategies to game these metrics, potentially undermining their validity and leading to new forms of strategic behavior that benefit those with access to information about how these systems work.

\textbf{Algorithmic Monoculture:} If multiple programmes adopt similar SOAI approaches, the selection landscape may become dominated by a narrow set of optimization targets, reducing rather than increasing the diversity of opportunities available to different types of candidates.

\textbf{False Precision:} The mathematical sophistication of frameworks like the SPF may create an illusion of objectivity and precision that masks the subjective value judgments embedded within them, making it harder to critique or adjust these systems when they produce problematic outcomes.

\textbf{Institutional Complacency:} By providing tools that allow institutions to demonstrate measurable progress on diversity and fairness metrics, these systems may reduce pressure for more fundamental reforms to selection processes and institutional structures.

\subsection{Reconciling Critique with Pragmatic Impact}
Despite these significant concerns, it is important to acknowledge the pragmatic context within which this research operates. Without this research, the institutional structures examined – Oxford, Schmidt Futures, MacArthur Foundation, Marshall Commission, and Rhodes Trust – would continue operating with their existing selection processes. In the absence of more sophisticated tools, selectors would continue making high-stakes decisions with limited information and potentially greater inconsistency.

While the work in this thesis does not challenge the fundamental power structures that enable massive wealth concentration and elite institution gatekeeping, it does seek to improve the fairness and efficacy of decision-making within these existing frameworks. The programs examined, despite their embeddedness in elite networks, genuinely seek to identify and support scholars who will contribute to solving global challenges and improving their communities.

\subsection{Guarding Against Failure Modes: Deliberation, Contestability, and Iteration}\label{ssec:deliberation_contestability_iteration}
Given the critical failure modes outlined in Section \ref{ssec:wrongs}, particularly the risks of quantitative overemphasis, algorithmic convergence, false precision, and enabling tokenism, we recognise that SOAI systems must be designed not as static, authoritative solutions, but as evolving tools that support ongoing human deliberation and are open to contestation. This requires employing several means to guard against such failure modes:

\begin{itemize}
    \item \textbf{Transparency in Values and Trade-offs:} Systems should make the embedded values (e.g., specific definitions of diversity or potential) and the trade-offs made during their design and configuration transparent to selectors. This includes clearly articulating how different quantitative metrics are weighted and how qualitative information is incorporated or potentially sidelined.
    \item \textbf{Mechanisms for Contestation:} Selectors and other relevant stakeholders (potentially including applicant representatives or external auditors in some contexts) should have avenues to question, critique, and even challenge system outputs or the underlying logic. This could involve features that allow users to flag problematic recommendations, suggest alternative interpretations of data, or adjust system parameters under controlled conditions.
    \item \textbf{Support for Iterative Refinement:} SOAI tools should be built with the expectation that they will require regular review and refinement. This includes designing for the easy updating of models, metrics, and value parameters as organisational goals evolve or as unintended consequences are identified. Processes should be established for periodic audits of system performance against both stated objectives and broader ethical considerations.
    \item \textbf{Interfaces for Qualitative Nuance:} To counteract quantitative overemphasis, interfaces should actively encourage and facilitate the integration of qualitative nuance. This might involve designing dashboards that juxtapose quantitative scores with rich qualitative summaries, or tools that help selectors document and weigh contextual factors that are not easily quantified.
\end{itemize}

Designing for deliberation, contestability, and iteration aims to foster a more responsible and adaptive use of AI in selection. It positions SOAI not as a replacement for human judgment, but as a catalyst for more informed, reflective, and ethically aware decision-making processes. This approach acknowledges that achieving `real change' (as discussed in \ref{ssec:real_change}) and avoiding institutional complacency requires continuous engagement with the complexities and potential pitfalls of AI-driven selection, and a commitment to addressing the structural inequalities that these systems alone cannot resolve.

% [FIXED] ⭐️ Towards this end, I was hoping 8.4 would take a critical approach to the research contributions, rather than discussing scholarships more generally. I think that it is quite essential to take a critical lens to especially the work of Chapter 7.  What unintended consequences could result from / be amplified by this work?  
% For instance, could a possible critique of the new approach proposed in Chapter 7 be further overemphasising quantitative metrics? You make great points that "qualitative" needs to be incorporated with "quantitative" for context, but do not discuss why or how – this could really be expanded.  What exactly is it about the qualitative components that shifts one's reasoning in selection? 
% What about the problem of convergence/amplification; what would happen if all scholarship committees used the approach in Ch 7? Would they all artificially converge on the same candidates, and would this be okay or be essentially overfitting on the quantitative metrics?
% There is also an argument to be said against defending against tokenistic efforts at improving diversity instead of effecting meaningful deep change across demographics. How would SOAI achieve this?

The tension here is between perfect and better: while an ideal solution might dismantle the structures that concentrate educational and career opportunities among elite institutions, the practical reality is that these structures persist. Given their persistence, there is value in ensuring that the opportunities they do provide are distributed as fairly and effectively as possible, even while acknowledging that such improvements may inadvertently legitimize the broader system.

However, this pragmatic justification cannot absolve us of responsibility for the potential harms identified above. Future implementations of SOAI approaches must actively monitor for convergence effects, metric gaming, and tokenistic applications. More fundamentally, any deployment of these systems should be accompanied by transparency about their limitations and ongoing efforts to address the structural inequalities they cannot resolve.

\section{Limitations}
In scenarios outside the selection context, HCI research has generally sought to harmonise the conflicting needs of different user groups through participatory design. Yet, in the case of talent identification, the very nature of the selection process introduces a conflict of incentives between applicants and the selection organisation. Applicants are primarily motivated by the desire to be chosen, while selection organisations, guided by practitioners, seek to optimise the selected cohort according to (biased) measurements of (sometimes idiosyncratic) organisational preferences. This inherent misalignment of objectives raises significant concerns about how to balance the needs of both groups without compromising the overarching social values that SOAI seeks to uphold. Our solution in this thesis has been to centre the organisations and use data from past selections to evaluate their selection decisions based on stated and elicited preferences. However, this solution positions research from the position of the decision-makers, and a lack of engagement with decision subjects may limit the social benefit of the research.

Central to our solution is the notion that the broader social aims of the selection process (e.g., ensuring fairness, diversity, and social benefit) align more closely with practitioners than with applicants and ultimately supersede the immediate interests of both practitioners and applicants. However, we note that some overlap exists between the broader social goals of the selection process and the scholarship applicants' goals. We note that work already exists exploring this from a decision-subject standpoint; \textcite{10.1145/3351095.3372867} find that applicants prefer algorithmic decision-making to human decision-making according to both procedural and distributive notions of fairness. In light of this, SOAI's positioning as a paradigm for building DSTs, rather than a paradigm for building algorithmic decision-making systems, limits the scope of the research. Were it the case that algorithmic decision-making is preferable to algorithmically supported human decision-making, the SOAI paradigm would not be the most effective way to achieve the social aims of selection.

Setting aside the social benefit of the SOAI paradigm as a whole, specifics of SOAI, as it is implemented in this thesis, may limit the work done here. The Decision Matrix framework intentionally elides distinctions between decision points in favour of clarifying a focus on a decision's stage and stakes. This elision was arrived at in concert with participants from Rise, but it may not extend to other decision-making contexts. If it doesn't, though we still call for SOAI work in these contexts, our work may not be directly applicable.

Additionally, the focus on improving selection within existing applicant pools means that SOAI approaches may not address broader questions of access and outreach that determine who applies in the first place. This limitation is particularly significant given that the most profound inequities in opportunity allocation may occur before the selection process even begins.

\section{Future Work}
While the work in this thesis articulates SOAI as a paradigm for all AI design oriented around supporting selection problems, we develop and test this paradigm for three specific families of decision points. A straightforward extension of this work would apply SOAI principles to other decision points in selection processes. Natural candidates include: supporting essay judgements with automated essay scoring, where a large body of literature already seeks to score applicant essays via algorithm \cite{cozma_automated_2018,ramesh_automated_2022,wang_use_2022,elijahthesis}, but automated approaches continue to struggle with marking top or bottom essays \cite{elijahthesis}; supporting testing and test evaluation with automated scoring systems \cite{organisciak_beyond_2023,condon2014international}; and supporting pre-application portions of the outreach process, which was requested by several participants in Chapter \ref{ch:diversity}.

Though SOAI, as we investigate it here, aligns most closely with the interests of selectors, there is a need for human-centric work seeking to determine applicant perceptions of positive social outcomes. While work exists examining applicant perceptions of decisions made about them, \cite{pandey_applicants_2022,horodyski_applicants_2023}, this work often approaches research from a fairness or decision-subject-empowerment perspective. No work exists approaching applicant perspectives from the perspective of the ultimate social benefit of selection. Future work should seek to understand how applicants perceive the social outcomes of selection decisions, and how these perceptions can be used to design more effective AI systems for selection.

Though the Decision Matrix provides a useful framework for categorising decision points in selection processes, it is not exhaustive. Future work should seek to augment the Decision Matrix with additional axes that capture the complexity of selection decisions more fully. In particular, though selectors drew a distinction between individual- and group-level in-process decisions in Chapter \ref{ch:diversity} (and though the design prototypes reflect this distinction), the Decision Matrix does not capture this distinction. Future work should seek to augment the Decision Matrix to distinguish individual- from group-level distinctions and implement more individual-level decision support systems in practice.\footnote{Chapters \ref{ch:genai} and \ref{ch:spf} both avoid individual-level implementations in real decision-making pipelines due to risks associated with introduced unfairness or bias \cite{hartigan_fairness_1989,barocas2023fairness,pmlr-v80-kearns18a,Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024,liang_gpt_2023}. Any work implementing tools at the individual-level should consider these risks first.}

\section{Conclusion}
In this thesis, we pioneer a new paradigm of AI design for selection processes, Selection-Oriented AI (SOAI). Chapters \ref{ch:xai} and \ref{ch:genai} find that existing AI systems often fail to meet the needs of selectors, particularly for in-process decision-making. In response, we propose a new paradigm, SOAI, which seeks to centre the design of AI DSTs not around the selectors but around the social aims of selection they seek to practice. Chapters \ref{ch:diversity} and \ref{ch:spf} apply SOAI principles to design DSTs to support considerations of diversity in selection; we implement a prototype designed to satisfy selector desires and find that it improves both diversity and performance outcomes in selection. We then provide a set of design recommendations for SOAI designers, including focusing on specific social values, identifying decision points with the Decision Matrix, balancing quantitative and qualitative information, and evaluating real change in addition to subjective satisfaction.

More broadly, the use of SOAI to support scholarship-specific selection decisions implies the potential to support and improve related decision-making processes, from other selection contexts (e.g., admissions or hiring) to non-selection decision-making contexts (e.g., programme outreach). With a technology-induced flattening of the world \cite{Friedman_2005}, more candidates from more parts of the world find themselves qualified for opportunities. Add to this the ease of application submission created by GenAI assistants, and it is clear why applications to job, university, and scholarship opportunities have seen a dramatic increase in recent years \cite{Kaashoek2024Impact}. In light of this, we conclude with a call for SOAI across selection contexts; the need has never been more pressing.
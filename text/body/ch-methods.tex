\chapter{\label{ch:methods}Methodology}

\minitoc

\section{Methodological Framework of the Thesis}\label{sec:context_methodology}

This thesis employs a mixed-methods approach that combines computational evaluation with qualitative investigation of social and organisational contexts. This section outlines the key methodological frameworks that inform the research, grounding it in established practices while tailoring them to the unique demands of studying AI in global scholarship selection.

\subsection{Core Research Traditions}\label{ssec:context_core_traditions}
The methodological framework of this thesis is grounded in several key traditions from \textbf{Human-Computer Interaction (HCI)} and related fields. Our approach is rooted in \textbf{Participatory Design (PD)}, which centres the experiences and needs of users (in our case, we primarily consider scholarship selectors and their organisations) \cite{blythe2014research, Knapp_Zeratzky_Kowitz_2016}. This tradition recognises that effective Decision Support Tools (DSTs) cannot be designed in isolation but must emerge through collaborative engagement with stakeholders \cite{braun_using_2006}. To facilitate this, we also incorporate \textbf{Action Research (AR)}, which emphasises conducting ``research with, rather than on, people'' \cite{Hayes_2011, bradbury_action_2003}. This approach is particularly vital in scholarship selection, where organisational dynamics and constraints heavily shape technological interventions \cite{lu_organizing_2023}.

However, engaging only the stakeholders in these decisions risks satisfying individuals while failing to achieve the broader social aims of selection. Thus, guiding these participatory approaches is \textbf{Value Sensitive Design (VSD)}, which ensures that we design algorithmic systems with explicit attention to human values and their implications \cite{VanKleek_Seymour_Binns_Shadbolt_2018}. This is crucial in selection contexts where decisions have profound impacts on individuals' lives.

Finally, though not central to our approach, we adopt a \textbf{critical perspective on algorithmic systems}, following scholars like \textcite{noble2018algorithms} and \textcite{oneill2016weapons} in exploring the risks of implementing algorithmic systems. This perspective requires examining not only technical performance but also the social and political implications of DSTs, helping to reveal how they may reproduce or challenge existing inequalities.

\paragraph{Integrating Methodological Traditions}
While each of these traditions offers a unique lens, the strength of this thesis's methodology lies in their integration. We use Value-Sensitive Design (VSD) as our foundational framework to conceptually identify and prioritise the human values at stake in scholarship selection, such as fairness, diversity, and transparency. This values-centred groundwork then informs our practical engagement. We employ Participatory Design (PD) and Action Research (AR) as engines for translating these values into tangible interventions. Through PD, we collaborate directly with selectors to co-create tools and frameworks grounded in their lived experiences and organisational realities. AR provides the iterative structure for deploying, testing, and refining these interventions in a real-world setting, ensuring our research is not only theoretically sound but also practically relevant and responsive to the evolving needs of our partner organisations. This synergistic approach allows us to move from abstract principles (VSD) to collaborative creation (PD) and finally to real-world impact (AR).

\subsection{Evaluation Methods}\label{ssec:context_evaluation_methods}
To assess both technical performance and socio-technical implications, this thesis employs a multi-faceted evaluation strategy.

For \textbf{computational evaluation}, we use standard machine learning metrics, including accuracy, precision, recall, and F1-scores, to assess algorithmic performance. Recognising the limitations of these metrics in capturing fairness concerns, we also employ fairness-aware evaluation methods, including stratification along demographic lines and calibration analysis \cite{mehrabi2021survey}.

For the \textbf{human-centred evaluation of XAI}, we follow the framework of \textcite{doshi-velez_towards_2017}. This involves both functional evaluation (measuring how explanations affect task performance) and human-grounded evaluation (measuring how explanations align with human processes). This dual approach recognises that technical performance alone is insufficient for understanding the social implications of algorithmic systems.

\subsection{Critical and Reflexive Stance}\label{ssec:context_reflexivity}
Drawing on critical algorithmic studies, the thesis adopts a reflexive approach that examines not only the technical properties of algorithmic systems but their social, political, and ethical implications \cite{seaver2017algorithms}. This includes attention to questions of power, representation, and accountability that are often overlooked in purely technical approaches to algorithm design. These approaches emphasize collaboration and shared ownership of the research process, recognizing that effective technological solutions must emerge from genuine engagement with user communities.

\section{Specific Methods Used}
\subsection{Online Surveys}
The practice of running online surveys to gather quantitative data is well-established and often used both within and without HCI \cite{zhao2023fairness,pillai_adoption_2020,krishna_disagreement_2022,mai_user_nodate,bansal_does_2021,binns_its_2018,dzindolet_role_2003,papenmeier_its_2022}. Chapter \ref{ch:xai} makes use of one such survey. We use Prolific Academic to gather participants and Formr to administer our survey \cite{binns_its_2018,Arslan_formr_2019}. We follow \textcite{caldwell_power_nodate} in designing our survey based on a power analysis of the statistical tests we intend to run on the output data.

\subsection{Design Workshops}
Chapters \ref{ch:xai} and \ref{ch:diversity} both make use of group design workshops to refine and evaluate design prototypes. Both follow an experience-prototype methodology \cite{Buchenau_Suri_2000}, and incorporate a few specific methodologies.

Both chapters follow \textcite{Zimmerman_Forlizzi_2017}'s scenario speed dating approach, which sees participants rapidly applying different design prototypes to (real or hypothetical) scenarios.

\textcite{Gatian_1994} has researchers asking participants to choose a favourite among a series of options as a means of comparison, while \textcite{Griffiths_Johnson_Hartley_2007} brings this method to HCI. Chapter \ref{ch:diversity} makes use of this method.

\subsection{Individual Interviews}
Chapter \ref{ch:diversity} makes use of one-on-one interviews with participants to first elucidate participant understanding of diversity. In these interviews, we incorporate several methods.

\textcite{Knapp_Zeratzky_Kowitz_2016}'s `crazy 8s' exercise sees participants give eight feature requests in eight minutes. Ordinarily, this exercise is done with a writing surface, but we have participants do this verbally.

\textcite{blythe2014research} introduces the concept of design fiction, where participants more detail their ideal app. We adapt this to create a ``magic app'', capable of doing anything the participant desires and asking the participant to describe this app.

\subsection{Quantitative Analysis}
Chapters \ref{ch:xai}, \ref{ch:genai}, and \ref{ch:spf} rely on several standard statistical tests. Primarily, we use Student's t-test \cite{Mishra_Singh_Pandey_Mishra_Pandey_2019}, the Analysis of Variance (ANOVA) \cite{Mishra_Singh_Pandey_Mishra_Pandey_2019}, Pearson's test of correlation \cite{Schober_Boer_Schwarte_2018}, Tukey's Honestly Significant Difference test \cite{Kim_2015}, and the Receiver Operating Characteristic curve \cite{hanley1989receiver}. Additionally, we develop a permutation test in Chapter \ref{ch:spf} based on \textcite{good2013permutation}.

\subsection{Qualitative Analysis}
Chapters \ref{ch:xai} and \ref{ch:diversity} engage in inductive thematic analyses of their qualitative results. In doing so, we follow the methodology introduced by \textcite{braun_using_2006} and developed in \textcite{braun_conceptual_2022,braun_toward_2023,noauthor_thematic_nodate}.

\section{Research Design}
Chapters \ref{ch:xai}, \ref{ch:genai}, \ref{ch:diversity}, and \ref{ch:spf} all detail studies conducted according to different research paradigms and employing different methodologies. Each chapter contains a self-encapsulated section on research design. However, Table \ref{tab:method_subsections} provides a high-level overview of the methods and paradigms employed in each chapter.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    & \textbf{Chapter \ref{ch:xai}} & \textbf{Chapter \ref{ch:genai}} & \textbf{Chapter \ref{ch:diversity}} & \textbf{Chapter \ref{ch:spf}} \\
    \hline
    \textit{Participatory Design} & Yes & & Yes & \\ 
    \textit{Action Research} & & Yes & & Yes \\ 
    \textit{Value-Sensitive Design} & & & Yes & \\ 
    \hline
    \textit{Online Surveys} & Yes & & & \\ 
    \textit{Design Workshops} & Yes & & Yes & \\ 
    \textit{Individual Interviews} & & & Yes & \\ 
    \textit{Quantitative Analysis} & Yes & Yes & & Yes \\ 
    \textit{Qualitative Analysis} & Yes & Yes & Yes & \\
    \hline
    \end{tabular}
    \caption{This table indicates which methods and paradigms are employed in each core research chapter.}
    \label{tab:method_subsections}
\end{table}

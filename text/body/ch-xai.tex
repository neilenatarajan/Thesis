\chapter[XAI]{\label{ch:xai}XAI: Misleading In-Process, but Useful Post-Hoc\footnote{This chapter is based on a paper written in concert with Reuben Binns, Ulrik Lyngs, and Nigel Shadbolt. The paper is currently under review as: Neil Natarajan, Reuben Binns, Ulrik Lyngs, and Nigel Shadbolt. 2024. ``XAI: Misleading In Process, but Useful Post Hoc.'' Under review at CHI 2025.}}

\minitoc

% [FIXED] Scope and motivation – Thematically the connection between Chapter 4 and the rest of the thesis centring on "selection orientated AI" could be made stronger. The introductory Motivation section says xAI methods are used for selection contexts, and "therefore xAI is a good place to start"; however, since the experiment examines xAI in a non-selection context (a salary estimation and decision-making problem) a better solution would be to slightly re-thematise the thesis or chapter.  A couple options here: to cast the experimental method of Ch. 4 in a selection-oriented context (somehow connect the case studies of ch4 as a subproblem of selection), or to slightly broaden the intro and scope of the thesis to beyond the cohort/team selection task, such as  "high-stakes algorithmic assisted decision making about people".

\section{Motivation}
In exploring the array of decision support tools applicable to global scholarship selection, explainable AI (xAI) offers a natural starting point. Scholarship selection processes often involve complex algorithmic scoring of applicants across multiple dimensions - from cognitive assessments to essay evaluations to interview performance. XAI tools are often offered as a fair and responsible way to support a decision-subject's right to explanation, empowering them while improving decision-making in potentially sensitive fields \cite{Goodman_Flaxman_2017}. A wealth of research explores and evaluates xAI in these contexts \cite{molnar_interpretable_2019,barocas_hidden_2020,wachter_counterfactual_2017,Barocas_Hood_Ziewitz_2013,raghavan2020mitigating}. Despite this, much of this research cautions against blind applications of these tools to decision-making processes \cite{Lipton,miller_explainable_2023,kumar_problems_2020,Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024}. In this chapter, we explore the potential for post-hoc notions of interpretability in supporting selection-related decision-making processes, and consider the potential for these tools to be applied in different supporting contexts.

% [FIXED] ⭐️ 4.2 – Introduction – More detailed, careful, critical engagement – This reads very much in a rush – multiple citations are bunched together and summarised without any detail. (I jokingly refer to this practice as "dump truck citations" – a common problem in theses that I often ask students to avoid).  Instead, I would have very much liked to see a more narrative summary of the key discussions around xAI's limitations, e.g. the different shades of "misplaced explainee trust in model outputs" that the most influential of these studies found. Are there different reasons, by different groups? Similarly, Section 4.3.2 and 4.3.5 similarly lack any detail or depth and in our view, fails to properly summarise or compare critiques.  

\section{Introduction}
For comprehensive background on explainable AI, automation bias, and the history of interpretable AI systems, see Chapter \ref{ch:context}, Section \ref{sec:xai_background}. Here we focus on the specific critiques relevant to our research questions.

Despite the promise of xAI in enabling fair and explainable decision-making \cite{Goodman_Flaxman_2017}, post-hoc xAI systems have faced increasing criticism as decision support tools. The critiques center around how these systems affect user trust and decision-making, but they reveal different concerns and mechanisms. Let us examine these critiques in detail.

\textcite{Lipton} offers a foundational critique, arguing that well-intentioned explanation design may yield ``misleading but plausible'' explanations. Their work suggests that the very act of providing an explanation can create a false sense of understanding, leading users to trust the system's outputs even when that trust is unwarranted. This critique is particularly relevant in selection contexts, where the stakes of misplaced trust are high.

Building on this, \textcite{miller_explainable_2023} identifies a more fundamental issue: post-hoc xAI methods often serve to justify the underlying AI models and their outputs rather than enabling users to make their own informed decisions. They argue that these explanations can create a form of ``algorithmic authority'' where the explanation itself becomes a tool for legitimizing the model's decisions, potentially undermining human agency in the decision-making process.

The evidence for these concerns is mixed but concerning. \textcite{lai_human_2019} found that explanations can increase user trust in AI systems, but this trust may not be well-calibrated to the system's actual performance. Similarly, \textcite{jacobs_how_2021} demonstrated that explanations can lead to over-reliance on AI systems, particularly in high-stakes contexts. These findings suggest that the problem of misplaced trust operates through multiple mechanisms: from creating false understanding to establishing algorithmic authority to encouraging over-reliance.

In response to these critiques, the field has seen a significant shift away from post-hoc xAI approaches. \textcite{kumar_problems_2020} argues that post-hoc explanations often fail to provide meaningful insights into model behavior, while \textcite{Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024} demonstrates that these explanations can be manipulated to justify incorrect decisions. This has led to the development of new paradigms, such as \textcite{miller_explainable_2023}'s evaluative AI for decision-makers, which focuses on helping users evaluate model outputs rather than explaining them, and \textcite{karimi_algorithmic_2021}'s causal models for decision subjects, which aim to provide more actionable insights into how decisions are made.

But have we been too hasty in rejecting these older methods? Underlying this shift away from xAI is the assumption that approaches will be deployed to increase trust in particular outputs in decision support contexts, even though such trust may be unwarranted when outputs are wrong. But is this always the case? Might these trust-inducing xAI be usefully deployed elsewhere, such as for post-decision evaluation of models and decision-making processes?

We utilise a decision stage distinction from Chapter \ref{ch:context} to help consider the benefits and risks of post-hoc interpretability tools as DSTs. We distinguish between: the in-process stage, where AI outputs and post-hoc explanations are used to support human decision-makers in confirming or overriding the `primary' decision the model output advises on, and the ex-post stage, where the primary decision has already been made, and the xAI is offered to inform second-order decisions about the decision-making process. (E.g., between application cycles, recruitment and selection practitioners examine their prior decision-making procedures and seek to make decisions that improve them for the next cycle \cite{li2020hiring}.) We seek to answer two research questions (RQs):

\begin{enumerate}
    \item[(RQ1)] Do post-hoc explanations, when used as in-process DSTs, induce unwarranted trust in the explainee?
    \item[(RQ2)] If post-hoc xAI methods induce unwarranted trust in-process, could they still be useful ex-post?
\end{enumerate}

At the in-process stage, we run an online study to discern whether the problem of unwarranted trust is specific to xAI. We investigate \textcite{lundberg_unified_2017,ribeiro_anchors_2018}'s popular methods, SHapley-based Additive exPlanations (SHAP) \cite{lundberg_unified_2017} and Scoped Rules (Anchor) \cite{ribeiro_anchors_2018}, respectively, to see if they induce unwarranted trust. We also investigate a `Confidence' explanation consisting of the model's confidence statistic to determine if the problem of unwarranted trust is unique to explanations per se, or applies more generally to the presentation of any information that could increase positive perceptions of the AI's performance. We ask participants to \emph{estimate a person's salary} \cite{kohavi_scaling_1996} or \emph{predict whether someone will be severely delinquent in making a credit payment} \cite{GiveMeSomeCredit} with the help of an AI output and with or without an explanation. We find that SHAP explanations do increase unwarranted trust in AI outputs, but that Confidence explanations do as well. We find no such effect for Anchor. This suggests the problem of unwarranted trust is not unique to xAI per se and is rather a symptom of generic post-hoc justifications.

Having identified a core problem with some kinds of in-process xAI, we consider whether they have any potentially redeeming features if deployed at the ex-post level through a series of participatory design workshops. We refine our attention to SHAP, as critiques by \textcite{Lipton} and \textcite{miller_explainable_2023} are most germane. We contend that, in an ex-post context, this induction of unwarranted trust is less problematic, as primary decisions have already been made and thus trust in the AI outputs is not at issue. We ask participants to \emph{refine a scholarship selection algorithm} with the help of SHAP-based explanations. Through these workshops, we find that, while SHAP explanations may induce unwarranted trust in specific model outputs, they can still be useful to drive process change in organisations.

Our primary contributions are:

\begin{enumerate}
    \item Quantitative findings indicating that the problem of explanation-induced unwarranted trust extends to generic post-hoc justifications, but that such criticism only applies in-process.
    \item Qualitative findings that post-hoc explanations, properly presented, can make useful ex-post DSTs.
\end{enumerate}

\section[Experimental Study]{Experimental Study: In-Process Decision Support}\label{sec:online}
\subsection{Research Questions}
Our online study seeks to answer RQ1:

\begin{enumerate}
    \item[(RQ1)] Does post-hoc xAI used as an in-process DST induce unwarranted trust in the explainee?
\end{enumerate}

To do this, we compare three alternate conditions: \textcite{lundberg_unified_2017}'s SHAP explanations, \textcite{ribeiro_anchors_2018}'s Anchor-based explanation, which obeys contrastive, selective, and counterfactual paradigms for explanations, and a Confidence condition consisting of the model's intrinsic confidence measurement. We measure trust in the AI system in two ways: attitudinal trust, measured by self-report, and behavioural trust, measured by the participant's decision to follow the AI system's recommendation. We also measure the change in trust from before to after the explanation and compare this change across the three conditions. These measurements are done across two tasks. Each participant sees six cases, with the explanatory and task conditions held constant. Finally, we calculate the correlation between attitudinal and behavioural trust, and between the change in attitudinal and behavioural trust.

\subsection{Methodology}
\subsubsection{Participants}\label{ssec:participants}
Participants were recruited via Prolific Academic's standard sampling method restricted to the United States.\footnote{\url{www.prolific.co}} They were paid at a rate of \$15 per hour. Participants were first shown an information sheet detailing the study's methodology and what was being asked of them. They were then asked to give informed consent. After consenting to participate in the study, participants were routed to Formr, our chosen survey design and hosting platform, to complete the online study.\footnote{\url{www.formr.org}} All data collected was anonymous and was stored on secure servers. Ethics review was performed by the University of Oxford's Central University Research Ethics Committee.

% [FIXED] ⭐️ 4.4.2.2 – Ethics of Tasks – There is a bit of an ethical concern here - inferring a person's salary and/or whether they are likely to experience a "severe credit payment delinquency" based on their demographics could be seen an exercise in stereotyping. For instance, no finite set of attributes (sensitive or not) "define" a person's identity, salary or actions – as there is no "causal" link between these attributes and outcomes. So how should a person realistically tell what the answers might be? One argument could be that these are inherently unknowable.  In this particular case, there is the added factor that there are also sensitive attributes included in the model – meaning people are essentially asked to make a biased (potentially harmful) judgement.  How wass this handled, and was asking people to make an unethical judgement based on a mixture of sensitive and non-sensitive attributes ethical?

\subsubsection{Tasks}\label{ssec:tasks}
We restrict our research question to tasks familiar to lay people with a well-defined but difficult-to-ascertain ground truth. Our two tasks are chosen from a gamut of well-known algorithmic decision-making tasks as two particularly related to \emph{Selection}, articulated in Chapter \ref{ch:context} \cite{10.1111/j.1467-954X.2007.00740.x,Pasquale_2006,Latzer_Hollnbuchner_Just_Saurwein_2014}: \emph{estimating a hypothetical person's salary} based on census information of that individual, and \emph{predicting whether someone will be severely delinquent in making a credit payment}. These tasks mirror key aspects of scholarship selection processes:

\begin{enumerate}
    \item The salary estimation task parallels the evaluation of applicants' socioeconomic status and need-based considerations in scholarship selection \cite{Warikoo_2019}. Just as scholarship selectors must assess applicants' financial need and potential impact of the scholarship, the salary task requires evaluating complex socioeconomic factors to make a determination.
    \item The credit delinquency task reflects the task of predicting the likelihood of `success' (according to program-defined criteria) of choosing a given applicant in scholarship selection, where selectors must assess applicants' future potential or likelihood of giving back \cite{schumann2017diverse}.\footnote{Educational institutions often receive funding from former students, and thus have financial interest in ensuring that students who receive funding are likely to give back to the institution \cite{citation_needed}; this is ordinarily not an explicit factor in scholarship selection, but may nevertheless play a role in decisions made.}
\end{enumerate}

We use two datasets: the Adult dataset collected from the 1994 US Census for the former task and the Give Me Some Credit dataset for the latter task \cite{kohavi_scaling_1996, GiveMeSomeCredit}. These datasets contain a mix of germane and demographic attributes (with some features encoding both), including sensitive attributes. In both tasks, the participant aims to accurately estimate the dependent variable with the help of the AI system and one of several possible explanations of the AI system's estimate (which is possibly just the confidence rating of the model). Note that these tasks engage participants in making decisions based on potentially biasing information, which raises significant ethical considerations. Specifically, inferring personal outcomes like salary or credit delinquency from such attributes can be seen as an exercise in stereotyping, particularly as there are no direct causal links establishing these attributes as definitive predictors of the outcomes. Asking participants to make judgments in these contexts could be construed as encouraging them to make potentially biased or ethically questionable determinations.

These tasks were chosen, however, for precisely this reason. The task of selection requires segregating non-causal features into germane information from accompanying noise (both of which often take the form of sensitive or demographic attributes). As such, these tasks are representative of real-world scenarios, and act as a sandbox to gauge both baseline decision-making capability of human raters and the impact of xAI methods on decision-making. The aim of this study is not to endorse the fairness of these specific predictive tasks or the models themselves, but rather to investigate how xAI methods influence user trust and decision-making when interacting with AI in such ethically complex environments. 

The handling of these ethical issues was a key consideration in the study design: participants were fully informed about the nature of the tasks and the data involved, as described in the informed consent process (see subsection \ref{ssec:participants}). The study protocol, including the use of these tasks and datasets, was reviewed by the University of Oxford's Central University Research Ethics Committee.

In our analyses, we index these tasks as \emph{Salary} and \emph{Credit}. Note that each participant only receives one task to complete throughout all 6 cases.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-shap.png}
        \caption{SHAP explanations for \emph{Salary}}
        \label{fig:shapsalary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-shap-2.png}
        \caption{SHAP explanations for \emph{Credit}}
        \label{fig:shapcredit}
    \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-anchor.png}
        \caption{Anchor explanations for \emph{Salary}}
        \label{fig:anchorsalary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-anchor-2.png}
        \caption{Anchor explanations for $credit$}
        \label{fig:anchorcredit}
    \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-confidence.png}
        \caption{Confidence explanations for \emph{Salary}}
        \label{fig:confidencesalary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{xai/survey-confidence-2.png}
        \caption{Confidence explanations for \emph{Credit}}
        \label{fig:confidencecredit}
    \end{subfigure}
    \caption{This figure shows sample explanations for all cases. Larger images and more detailed descriptions of explanations can be seen in Appendix \ref{app:xaifigures}.}
    \label{fig:online_explanations}
\end{figure}

\subsubsection{Models}
In both tasks, we construct a predictor model using random forests and augment this predictor with three different explanatory conditions. Our random forest classifier achieves $86\%$ test accuracy on the Adult dataset and $93\%$ test accuracy on the Give Me Some Credit dataset. We use a SHAP explainer to produce one of our explanatory conditions, and an Anchor explainer to produce another; our final explanatory condition is an intrinsic explanation produced by the random forest model. Figure \ref{fig:online_explanations} shows sample explanations produced by these methods. These ultimately form three explanatory conditions: SHAP, Anchor, and Confidence. Note that each participant only receives one model of explanation throughout all 6 cases. 

\subsubsection{Design}
Both tasks rely on the same 3-between-by-2-within design using repeated measures to capture the same data before and after the presentation of each explanation. The between-subjects factor determines which model is used to generate the explanation a given participant will receive. The within-subjects factor is the repeated-measures `explanation presence' factor. This is either `before explanation' or `after explanation', indexed $before$ or $after$. A flowchart of the study design can be found in Figure \ref{fig:online_flowchart}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{xai/online_flowchart.png}
    \caption{Participants in the online study are sorted into six buckets, where each bucket is segregated by explanatory condition and task and shown a brief description of the task (i.e., each participant sees only one of the explanations in Figure \ref{fig:online_explanations}). Then, each participant is shown 6 cases. In each case, participants are shown an applicant profile and an AI output. Participants are asked to agree or disagree with the AI output. Then, participants are given explanations based on their explanatory condition scores. They are then asked again to agree or disagree with the AI output.}
    \label{fig:online_flowchart}
\end{figure*}

% [FIXED] ⭐️ Section 4.4.2.5: "We code the participant's estimate as a binary yhuman variable:  … yhuman := How much money does this person make". It's not clear how this is binary. Maybe estimating whether someone areas over a particular salary range? (That does seem familiar for the Adult dataset). A small clarification would be helpful

\subsubsection{Questions and Variables}\label{sssec:q_and_v}
Each participant was shown a brief explanation of the task in question and was then asked to complete the 6 cases, with participants given a random mix of correct and incorrect cases. In each case, participants are first shown a table identifying the subject of the case and an AI output of what determination they should make. They are then asked to estimate the dependent variable and rate both their confidence in the estimate and their trust in the AI output on sliding scales (this is discretised to 20 points).

We code the participant's estimate as a binary $y_{human}$ variable:

\begin{equation}
    y_{human} := \begin{cases}
        \text{Does this person make more than \$100,000 per year?} & (Salary) \\
        \text{Will this person experience severe credit delinquency?} & (Credit)
    \end{cases}
\end{equation}

\noindent The two sliding scale responses are coded as $selfconfidence$ and $trust_{attitudinal}$ and have values between 1 and 20. These are defined as:

\begin{equation}
    confidence := \begin{cases}
        \text{How confident are you in your estimation?} & (Salary) \\
        \text{How confident are you in your prediction?} & (Credit)
    \end{cases}
\end{equation}

\begin{equation}
    trust_{attitudinal} := \begin{cases}
        \text{How much do you trust the AI's estimation?} & (Salary) \\
        \text{How much do you trust the AI's prediction?} & (Credit)
    \end{cases}
\end{equation}

As we ask all questions in both the $before$ and $after$ conditions, we collect six responses from each participant in each case: $y_{human}^{before}$, $selfconfidence^{before}$, $trust_{attitudinal}^{before}$, $y_{human}^{after}$, $selfconfidence^{before}$, and $trust_{attitudinal}^{after}$.  We additionally have the binary variables $y_{True}$ and $y_{AI}$ that are the true value and the AI output of the dependent variable.

In addition to these, we define $agreement^{x}$ as:

\begin{equation}
    agreement^{x} := \begin{cases}
        1 & \text{if } y_{human}^{x} = y_{AI} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

\noindent and $correct_{x}$ as:

\begin{equation}
    correct_{x} := y_{x} = y_{True}
\end{equation}

We define $trust_{behavioural}^{before}$ and $trust_{behavioural}^{after}$ to be the extent to which the participant's confidence agrees with the AI output:

\begin{equation}
    trust_{behavioural}^{x} := \begin{cases}
        confidence^{x}      & \text{if } agreement^{x} \\
        1-confidence^{x}    & \text{otherwise}
    \end{cases}
\end{equation}

Finally, to reason about the change in a variable due to the explanation, we define `$\Delta$' constructs for all variables with a $before$ and an $after$ as:

\begin{equation}
    \Delta variable := variable^{after} - variable^{before}
\end{equation}

\noindent so, e.g.:

\begin{equation}
    \Delta trust_{attitudinal} := trust_{attitudinal}^{after} - trust_{attitudinal}^{before}
\end{equation}

\subsubsection{Data Analysis}
We preregistered many of our analyses. Though we also include some post-hoc analysis below, we wish to delineate between the two types of analyses. The former are listed in full here.

We first wish to test for the presence of unwarranted trust. To do this, we measure the difference between ratings in each condition in the cases where the AI output is incorrect, i.e. where $\neg correct_{AI}$. To do this, we run three one-sided t-tests \cite{caldwell_power_nodate} to determine:

\begin{equation}
    \Delta trust \geq 0 | \neg correct_{AI}
\end{equation}

\noindent for both $\Delta trust_{behavioural}$ and $\Delta trust_{attitudinal}$. Note that this is identical to repeated-measures t-tests on the $trust_{behavioural}$ and $trust_{attitudinal}$ variables where $\neg correct_{AI}$. Following this, as this is also a between-subjects experiment, we wish to compare the varying effects of different explanation methods in this case. Thus, we run two between-subjects ANOVAs \cite{caldwell_power_nodate} on $\Delta trust_{behavioural}$ and $\Delta trust_{attitudinal}$ across the explanatory conditions again filtered on $\neg correct_{AI}$. When these ANOVAs have significant results ($p < 0.05$), we run Tukey's Honestly Significant Difference (HSD) test \cite{caldwell_power_nodate}.

Finally, in our Salary Estimation survey, we observed a strong positive correlation between our two trust variables, though we did not preregister it for that study. Indeed, correlation analysis is a well-documented method for confirming that two measurements indeed measure the same concept \cite{westen_quantifying_2003, morata-ramirez_construct_2013}. Thus, we additionally included the calculation of Pearson's correlation between $trust_{behavioural}$ and $trust_{attitudinal}$ and between $\Delta trust_{attitudinal}$ and $\Delta trust_{behavioural}$ in our preregistration for the Credit Delinquency Prediction survey. 

\paragraph{Power Analysis}
We run power analyses using \textcite{caldwell_power_nodate}'s Superpower. Specifically, we desire sufficiently powerful results in our primary analysis. We select a moderate effect size of interest (Cohen's $f$) of $0.15$ (yielding group means $-0.15$ and $0.15$ with unit variance), and target a power of at least $0.90$. We note that using a one-way t-test at $p = 0.05$ and assuming a sample size of $200$, we get power far above $0.90$. We also test for the ANOVA assuming unit variance and group means of $-0.15$, $0.15$, and $0.15$, respectively. Under these conditions, we achieve a power of $0.90$ with $200$ samples per condition. We ask each participant a total of 6 questions, and the AI output is incorrect in slightly less than half of them. To achieve $200$ samples per condition, therefore, we aim to recruit a total of roughly $66$ participants per condition, or roughly $200$ participants in total.

\paragraph{Preregistration}
We have preregistered analyses for both of our tasks in the OSF registries \cite{natarajan_binns_2022}. 

\subsection{Results}\label{ssec:os_results}
In both tasks, though we originally set $200$ as our target participants, some participants did not complete our task following Prolific Academic's guidelines. Data from these participants were marked incomplete and removed from consideration. 

After this removal, we had a total of $192$ participants complete the Salary Estimation study. These were split randomly into our three explanatory groups. By gender, $115$ were Male, $76$ were Female, and $1$ did not provide gender information. By ethnicity, $137$ were white, $10$ did not provide ethnicity, and the remaining $45$ were split among non-white ethnicities. Our participants were an average of $36.7$ years old, with the youngest being $18$ and the oldest $74$. Each applicant completed an introductory page and six cases. The average completion time for these tasks was $7$ minutes $43$ seconds, the minimum was $2$ minutes $25$, and the maximum was $36$ minutes $46$.

We had a total of $197$ participants complete the Credit Delinquency Prediction study. These were similarly split into groups. By gender, $106$ were Male, $90$ were Female, and $1$ did not provide gender information. By ethnicity, $143$ were white, $11$ did not provide ethnicity, and the remaining $43$ were split among non-white ethnicities. Our participants were an average of $38.4$ years old, with the youngest being $20$ and the oldest $77$. Each applicant completed an introductory page and six cases. The average completion time for these tasks was $7$ minutes $53$ seconds, the minimum was $2$ minutes $17$, and the maximum was $30$ minutes $13$.

\paragraph{SHAP and Confidence Increase Unwarranted Trust}
We first run the one-sided t-tests on the two trust variables ($attitudinal$ and $behavioural$). I.e., we test:

\begin{equation}
    \Delta trust_{x} > 0 | \neg correct_{AI} \text{ for } x \in \{behavioural, attitudinal\}
\end{equation}

\noindent for both \emph{Salary} and \emph{Credit} across SHAP, Anchor, and Confidence. A positive $F$ statistic here indicates $trust^{after} > trust^{before}$ and a negative $F$ statistic indicates $trust^{after} < trust^{before}$, but, as these are one-sided tests, $p$-values will only be meaningful when $F > 0$. This test was preregistered in both of our tasks \cite{natarajan_binns_2022}. Table \ref{tab:delta-trust-t} contains the results of these analyses.  

\begin{table}[htb]
    \centering
    \caption{These one-sided t-tests test for $\Delta trust > 0 | \neg correct_{AI}$ for all explanatory conditions and both tasks. We find that SHAP and Confidence increase unwarranted trust in the AI system.}
    \label{tab:delta-trust-t}
    \begin{tabular}{l l l r r}
        \toprule
        Task & Explanation & Variable & Test Statistic & p Value \\ 
        \midrule
        \emph{Salary} & Anchor & $\Delta trust_{behavioural}$ & $0.509$ & $0.306$ \\
        & & $\Delta trust_{attitudinal}$ & $0.165$ & $0.434$ \\
        & SHAP & $\Delta trust_{behavioural}$ & $\mathbf{3.811}$ & $\mathbf{<0.001}$ \\
        & & $\Delta trust_{attitudinal}$ & $-0.886$ & $0.812$ \\
        & Confidence & $\Delta trust_{behavioural}$ & $\mathbf{2.196}$ & $\mathbf{0.015}$ \\
        & & $\Delta trust_{attitudinal}$ & $0.945$ & $0.173$ \\
        \midrule
        \emph{Credit} & Anchor & $\Delta trust_{behavioural}$ & $1.396$ & $0.082$ \\
        & & $\Delta trust_{attitudinal}$ & $-2.364$ & $0.990$ \\
        & SHAP & $\Delta trust_{behavioural}$ & $1.516$ & $0.066$ \\
        & & $\Delta trust_{attitudinal}$ & $\mathbf{2.475}$ & $\mathbf{0.007}$ \\
        & Confidence & $\Delta trust_{behavioural}$ & $\mathbf{1.835}$ & $\mathbf{0.034}$ \\
        & & $\Delta trust_{attitudinal}$ & $0.940$ & $0.174$ \\
        \bottomrule
    \end{tabular}
\end{table}

This indicates that SHAP and Confidence appear to lead users to trust the AI system more when that system is wrong. We find this result more strongly for behavioural trust than attitudinal trust in all but one test.

Notably, Anchor does not follow this pattern and instead shows no significant increase in either attitudinal or behavioural trust on these one-sided t-tests. (However, as we explore in Section \ref{sec:anchor-attitudinal}, they may show a significant decrease.)

\paragraph{Different Explanation Styles Have Different Effects on Unwarranted Trust}
We have shown already that SHAP and Confidence induce unwarranted trust relative to no explanation; we now show that there is a significant difference in the effect of some explanatory conditions relative to others. To do this, we examine the $\Delta trust_{behavioural}$ and $\Delta trust_{attitudinal}$ variables across explanatory conditions with an ANOVA test. For this test, we filter on $\neg correct_{AI}$. I.e.:

\begin{equation}
    \begin{split}
        \Delta \text{ any}(trust_{x1,x2} \neq trust_{x1,x3}) | \neg correct_{AI} & \text{ for } x1 \in \{behavioural, attitudinal\} \\
        & \text{ and } x2,x3 \in \{SHAP, Anchor, Confidence\}
    \end{split}
\end{equation}

\noindent This test was preregistered in both of our tasks \cite{natarajan_binns_2022}. Table \ref{tab:delta-trust-anova} contains the results of these analyses.

\begin{table}[htb]
    \centering
    \caption{These ANOVAs compare $\Delta trust$ between SHAP, Confidence, and Anchor to indicate where significant differences exist. We find two statistically significant differences, indicating that we should focus post-hoc analyses on these two.}
    \label{tab:delta-trust-anova}
    \begin{tabular}{lrrr}
        \toprule
        Task & Variable & Test Statistic & p Value \\
        \midrule
        \emph{Salary} & $\Delta trust_{behavioural}$ & $\mathbf{3.671}$ & $\mathbf{0.026}$ \\
        & $\Delta trust_{attitudinal}$ & $0.925$ & $0.397$ \\
        \midrule
        \emph{Credit} & $\Delta trust_{behavioural}$ & $0.066$ & $0.936$ \\
        & $\Delta trust_{attitudinal}$ & $\mathbf{6.213}$ & $\mathbf{0.002}$ \\
        \bottomrule
    \end{tabular}
\end{table}

Note from Table \ref{tab:delta-trust-anova} that in the Salary Estimation task, we find no significant results for our ANOVA $trust_{attitudinal}$, but do find significant results for $trust_{behavioural}$. However, in the Credit Delinquency Prediction task, we find significant results for our ANOVA $trust_{attitudinal}$, but none for $trust_{behavioural}$. We examine these two findings separately.

\paragraph{SHAP Increases Behavioural Trust More than Anchor in the Salary Estimation Task}
We now show that:

\begin{equation}
    \Delta trust_{behavioural,SHAP,salary} > \Delta trust_{behavioural,Anchor,salary} | \neg correct_{AI}
\end{equation}

\noindent Note that, while the result of the ANOVA test in c{tab:delta-trust-anova} supports that there are indeed statistically significant differences in the three group means of the $\Delta trust_{behavioural}$ variable, it does not specify which means are greater and which are less. For an indication of which means are greater, following our preregistered protocol for significant ANOVA results, we turn to Tukey's Honestly Significant Difference (HSD) test as a post-hoc test in Table \ref{tab:delta-trust-hsd}. As we found significant results in the ANOVA test of $\Delta trust_{behavioural}$ filtered on $\neg correct_{AI}$, we restrict our post-hoc analysis to this variable.

\begin{table}[htb]
    \centering
    \caption{Tukey's HSD test compares $\Delta trust_{behavioural,x}$ in \emph{Salary} with $\neg correct_{AI}$. We find that SHAP increases behavioural trust in incorrect AI outputs more than Anchor.}
    \label{tab:delta-trust-hsd}
    \begin{tabular}{lllrr}
        \toprule
        Explanation A & Explanation B & Variable & Test Statistic & p Value \\
        \midrule
        SHAP & Anchor & $\Delta trust_{behavioural}$ & $\mathbf{2.310}$ & $\mathbf{0.022}$ \\
        Confidence & Anchor & $\Delta trust_{behavioural}$ & $0.855$ & $0.599$ \\
        SHAP & Confidence & $\Delta trust_{behavioural}$ & $1.455$ & $0.198$ \\
        \bottomrule
    \end{tabular}
\end{table}

As can be seen in Table \ref{tab:delta-trust-hsd}, we observe a significant difference in the mean of $\Delta trust_{behavioural}$ between the SHAP and Anchor conditions with $\neg correct_{AI}$, but we do not observe a significant difference between the other conditions. This indicates that, beyond increasing behavioural trust in incorrect AI outputs, SHAP increases behavioural trust in incorrect AI outputs \emph{more} than Anchor.

\paragraph{Anchor Decreases Unwarranted Attitudinal Trust Relative to SHAP and Confidence in the Credit Delinquency Prediction Task}
Note that we found a large negative F for $\Delta trust_{attitudinal}$ in the Anchor case in the Credit Delinquency Prediction portion of Table \ref{tab:delta-trust-t} – an effect that is not significant due to the one-sidedness of our tests. However, as we found only positive F values for $\Delta trust_{attitudinal}$ in the SHAP and Confidence cases, we might expect that Anchor has a negative effect on $\Delta trust_{attitudinal}$ relative to SHAP and Confidence. Indeed, the result of the ANOVA test in Table \ref{tab:delta-trust-anova} supports that there are indeed statistically significant differences in the three group means of the $\Delta trust_{attitudinal}$ variable in this task, though it again does not specify which groups are different, or how.

For an indication of which means are greater, following our preregistered protocol for significant ANOVA results, we turn again to Tukey's HSD test as a post-hoc test in Table \ref{tab:delta-trust-hsd-2}. As we found significant results in the ANOVA test of $\Delta trust_{attitudinal}$ filtered on $\neg correct_{AI}$, we restrict our post-hoc analysis to this variable. We test:

\begin{equation}
    \begin{split}
        & \Delta trust_{behavioural,x1,credit} > \Delta trust_{behavioural,x2,credit} | \neg correct_{AI} \\
        & \qquad \text{ for } x1,x2 \in \{SHAP, Anchor, Confidence\}
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \Delta \text{ any}(trust_{x1,x2} \neq trust_{x1,x3}) | \neg correct_{AI} & \text{ for } x1 \in \{behavioural, attitudinal\} \\
        & \text{ and } x2,x3 \in \{SHAP, Anchor, Confidence\}
    \end{split}
\end{equation}

\begin{table}[htb]
    \centering
    \caption{Tukey's HSD test compares $\Delta trust_{attitudinal}$ across explanations in \emph{Credit} with $\neg correct_{AI}$. We find that Anchor decreases unwarranted trust relative to SHAP and Confidence.}
    \label{tab:delta-trust-hsd-2}
    \begin{tabular}{lllrr}
        \toprule
        Explanation A & Explanation B & Variable & Test Statistic & p Value \\
        \midrule
        SHAP & Anchor & $\Delta trust_{attitudinal}$ & $\mathbf{1.213}$ & $\mathbf{<0.001}$ \\
        Confidence & Anchor & $\Delta trust_{attitudinal}$ & $\mathbf{1.030}$ & $\mathbf{<0.001}$ \\
        SHAP & Confidence & $\Delta trust_{attitudinal}$ & $0.183$ & $0.708$ \\
        \bottomrule
    \end{tabular}
\end{table}

As can be seen in Table \ref{tab:delta-trust-hsd-2}, we observe a significant difference in the mean of $\Delta trust_{behavioural}$ between the Anchor condition and both other conditions, but we do not observe a significant difference between SHAP and Confidence. This indicates that, relative to both other conditions, Anchor actually \emph{reduces} attitudinal trust in the AI output.

Note that this does not prove that Anchor reduces attitudinal trust relative to no explanation. For this analysis, we will need another t-test. As we did not preregister this test, an analysis of this phenomenon is included in the exploratory proportion of our results below.

\paragraph{Behavioural and Attitudinal Trust are Highly Correlated}
It should be noted that some patterns observed for $trust_{behavioural}$ do not hold for $trust_{attitudinal}$ and vice-versa. However, while they are mathematically distinct constructs, they are both intended to measure the same underlying phenomenon. We apply Pearson's correlation analysis across all explanatory conditions in both the before- and after-cases. We also perform this analysis on $\Delta trust_{attitudinal}$ and $\Delta trust_{behavioural}$. 

For this analysis, we do not filter out positive cases. Rather, we consider all cases together. Results can be seen in Table \ref{tab:trust-correlation}.\footnote{This analysis was only partially preregistered; we did not register this analysis in the Salary Estimation task, but we did in the Credit Delinquency Prediction task \cite{natarajan_binns_2022}.}

\begin{table}[htb]
    \centering
    \caption{Pearson's test shows a high correlation between $trust_{attitudinal}$ and $trust_{behavioural}$ in both tasks. This correlation extends to the relationship between $\Delta trust_{attitudinal}$ and $\Delta trust_{behavioural}$.}
    \label{tab:trust-correlation}
    \begin{tabular}{lllrr}
        \toprule
        Task & Variable A & Variable B & Test Statistic & p Value \\
        \midrule
        \emph{Salary} & $trust_{attitudinal}$ & $trust_{behavioural}$ & $\mathbf{0.630}$ & $\mathbf{<0.001}$ \\
        & $\Delta trust_{attitudinal}$ & $\Delta trust_{behavioural}$ & $\mathbf{0.265}$ & $\mathbf{<0.001}$ \\
        \midrule
        \emph{Credit} & $trust_{attitudinal}$ & $trust_{behavioural}$ & $\mathbf{0.612}$ & $\mathbf{<0.001}$ \\
        & $\Delta trust_{attitudinal}$ & $\Delta trust_{behavioural}$ & $\mathbf{0.179}$ & $\mathbf{<0.001}$ \\
        \bottomrule
    \end{tabular}
\end{table}

Note that, though the attitudinal and behavioural trust variables display different behaviours in other analyses, Table \ref{tab:trust-correlation} indicates that they are indeed highly correlated across both of our tasks. Furthermore, though the correlation between the $\Delta trust$ is more modest, it is still statistically significant. These together leave little doubt that $trust_{attitudinal}$ and $trust_{behavioural}$ measure related, and perhaps even identical, concepts. In other words, when a participant says they trust the AI output, they generally act accordingly.

\paragraph{Anchor Decreases Attitudinal Trust in AI Outputs}\label{sec:anchor-attitudinal}
Having found no significant result indicating the presence of the hypothesised effect of Anchor explanations on unwarranted trust, we explore what effect Anchor explanations have on end users' trust in incorrect AI outputs. For this analysis, we filter on $\neg correct_{AI}$.\footnote{This analysis was not preregistered.}

We noted already that SHAP and Confidence appear to increase trust in cases where the AI output is incorrect. However, we noticed no such result for Anchor. However, we did observe an apparent negative effect on $\Delta trust_{attitudinal}$ in the t-tests for the Credit Delinquency Prediction task, though, as the tests were one-sided, we could not confirm the significance. Thus, we repeat this test as a two-sided test, as shown in Table \ref{tab:delta-trust-t-2}. We also repeat other t-tests on Anchor as two-sided, though, as they all have positive F-values, note that none can yield significant results.

\begin{table}[htb]
    \centering
    \caption{Two-sided t-tests compare $\Delta trust_{x,Anchor} \neq 0 for x \in \{attitudinal, behavioural\}$ in both tasks. We find that Anchor explanations decrease attitudinal trust in incorrect AI outputs in \emph{Credit}, but all other results are inconclusive.}
    \label{tab:delta-trust-t-2}
    \begin{tabular}{lllrr}
        \toprule
        Task & Explanation & Variable & Test Statistic & p Value \\ 
        \midrule
        \emph{Salary} & Anchor & $trust_{behavioural}$ & $0.509$ & $0.611$ \\
        & & $trust_{attitudinal}$ & $0.165$ & $0.869$ \\
        \midrule
        \emph{Credit} & Anchor & $trust_{behavioural}$ & $1.396$ & $0.164$ \\
        & & $trust_{attitudinal}$ & $\mathbf{-2.364}$ & $\mathbf{0.019}$ \\
        \bottomrule
    \end{tabular}
\end{table}

Note here that, on the two-sided t-test, we \textit{do} find that the provision of Anchor explanations decreases participant attitudinal trust in incorrect AI output, at least in the Credit Delinquency Prediction task. However, we do not see a similar effect on behavioural trust. 

\paragraph{Anchor and SHAP Increase Participant Self-Confidence in their Determinations}
Noting that behavioural trust is an index variable constructed from $selfconfidence$, we ask: does providing an Anchor explanation increase participant confidence in their own decisions when the AI output is incorrect? Similarly, we ask this question for both the SHAP and Confidence conditions, following exactly the format of the preregistered one-sided t-tests in Table \ref{tab:delta-trust-t}, but applied to the variable $\Delta selfconfidence$. Again, we filter on $\neg correct_{AI}$.:

\begin{equation}
    \Delta selfconfidence > 0 | \neg correct_{AI}
\end{equation}

Analysis can be seen in Table \ref{tab:delta-confidence-t}.\footnote{This analysis was not preregistered.} Note for clarity that $selfconfidence$ is the variable indicating participant confidence in their own decisions, and Confidence is the condition in which the explanation consists of the AI's own confidence in its suggestion.

\begin{table}[htb]
    \centering
    \caption{One-sided t-tests determine whether $\Delta selfconfidence > 0$ where $\neg correct_{AI}$ for all explanatory conditions and both tasks. We find that Anchor and SHAP increase participant self-confidence in their determinations, while Confidence yields inconclusive results.}
    \label{tab:delta-confidence-t}
    \begin{tabular}{lllrr}
        \toprule
        Task & Explanation & Variable & Test Statistic & p \\
        \midrule
        \emph{Salary} & Anchor & $selfconfidence$ & $\mathbf{2.171}$ & $\mathbf{0.016}$ \\
        & SHAP & $selfconfidence$ & $\mathbf{1.694}$ & $\mathbf{0.046}$ \\
        & Confidence & $selfconfidence$ & $1.047$ & $0.296$ \\
        \midrule
        \emph{Credit} & Anchor & $selfconfidence$ & $\mathbf{1.742}$ & $\mathbf{0.042}$ \\
        & SHAP & $selfconfidence$ & $\mathbf{3.473}$ & $\mathbf{<0.001}$ \\
        & Confidence & $selfconfidence$ & $0.752$ & $0.226$ \\
        \bottomrule
    \end{tabular}
\end{table}

We note that, while Confidence shows no significant effects on either task, participants shown an Anchor or SHAP explanation grow significantly more confident in their prediction, indicating that providing an Anchor or SHAP explanation serves to increase a participant's confidence in their own estimate.

\paragraph{Explanations Impact Trust Differently When the AI Output is Correct}
Note that ideally calibrated trust would involve both distrusting the AI output when it is wrong and trusting it when it is right. To assess the latter, we now turn to an evaluation of what happens in the cases where the AI is correct, i.e. $correct_{AI}$. Namely, we conduct two-sided t-tests on both trust variables in all three cases. Table \ref{tab:delta-trust-t-positives} contains the results of these analyses.\footnote{These analyses were not preregistered.}

\begin{table}[htb]
    \centering
    \caption{These two-sided t-tests compare $\Delta trust$ when $correct_{AI}$. We find that Confidence and SHAP increase trust in the AI system when it is correct, while Anchor decreases attitudinal trust in the AI system, but increases behavioural trust.}
    \label{tab:delta-trust-t-positives}
    \begin{tabular}{lllrr}
        \toprule
        Task & Explanation & Variable & Test Statistic & p Value \\ 
        \midrule
        \emph{Salary} & Anchor & $trust_{behavioural}$ & $0.502$ & $0.616$ \\
        & & $trust_{attitudinal}$ & $\mathbf{-2.337}$ & $\mathbf{0.020}$ \\
        & SHAP & $trust_{behavioural}$ & $0.295$ & $0.768$ \\
        & & $trust_{attitudinal}$ & $-1.385$ & $0.168$ \\
        & Confidence & $trust_{behavioural}$ & $\mathbf{2.410}$ & $\mathbf{0.017}$ \\
        & & $trust_{attitudinal}$ & $\mathbf{3.254}$ & $\mathbf{0.001}$ \\
        \midrule
        \emph{Credit} & Anchor & $trust_{behavioural}$ & $\mathbf{3.013}$ & $\mathbf{0.003}$ \\
        & & $trust_{attitudinal}$ & $\mathbf{-2.487}$ & $\mathbf{0.014}$ \\
        & SHAP & $trust_{behavioural}$ & $0.207$ & $0.836$ \\
        & & $trust_{attitudinal}$ & $\mathbf{3.538}$ & $\mathbf{0.001}$ \\
        & Confidence & $trust_{behavioural}$ & $\mathbf{2.863}$ & $\mathbf{0.005}$ \\
        & & $trust_{attitudinal}$ & $\mathbf{2.461}$ & $\mathbf{0.015}$ \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Confidence Explanations Increase Warranted Trust When the AI Output is Correct}
Note that, for both trust variables and both tasks, the Confidence condition boasts a significant positive $\Delta trust$. In other words, when the AI output is correct, providing confidence in its own prediction increases both behavioural and attitudinal trust towards the AI.

\paragraph{Anchor Explanations Decrease Warranted Attitudinal Trust but Increase Warranted Behavioural Trust When the AI Output is Correct}
In the Anchor case, it is clear that providing Anchor explanations yields a large decrease in $trust_{attitudinal}$. This, along with the finding that Anchor explanations decrease $trust_{attitudinal}$ when $\neg correct_{AI}$, would indicate that Anchor explanations have an overall negative impact on $trust_{attitudinal}$, regardless of case. Despite this, providing Anchor explanations yields an increase in $trust_{behavioural}$ (though this is only significant in the \emph{Credit} case). This suggests that, though participants report lower trust in AI outputs when shown an Anchor explanation, they behave as though their trust in the outputs is appropriately calibrated.\footnote{Though $trust_{attitudinal}$ and $trust_{behavioural}$ are closely correlated overall, this represents one instance in which they appear to reveal differences in participant behaviour.}

\paragraph{SHAP Explanations Increase Warranted Attitudinal Trust in the Credit Delinquency Prediction Task When the AI Output is Correct}
In the SHAP case, we find a large significant positive $F$ for $trust_{attitudinal}$ when $correct_{AI}$ in the credit delinquency prediction task. However, not only is the effect not mirrored in either test of $trust_{behavioural}$, but the same test on the salary estimation task has a negative $F$ statistic.

\subsection{Study Findings}\label{ssec:os_discussion}

% [FIXED] 4.4.4 – "blindly increasing trust" – It might be useful to critically interrogate this statement. Do we know what is going on here?  Are people believing the algorithm because they have no reason not to?  Or because it looks like an authoritative / complex explanation?  Or is a kind of confirmation bias, such as – might SHAP trigger confirmation bias by showing features in a way that draws people's attention to aspects they believe would lead to a particular outcome?

Our results indicate that both SHAP and Confidence induce unwarranted trust in the explainee. I.e., on the \emph{Salary} and \emph{Credit} tasks, neither SHAP nor Confidence serves to correctly calibrate trust in AI outputs. Rather, they blindly increase trust in these outputs, encouraging users to incorrectly agree with the AI outputs. While this confirms the cautionary critique of \textcite{Lipton} as applied to SHAP in our domain, our results relating to Confidence suggest this critique is too narrow. Namely, issues of unwarranted trust do not seem confined to post-hoc xAI but are rather a function of providing post-hoc justification of the model's output. This suggests that even un-optimised notions of interpretability when provided post-hoc as justifications of model outputs, induce unwarranted trust.

We note that our study into these two explanations yields little insight into \emph{why} participants are trusting the AI outputs. Indeed, it is unclear from our findings whether this trust arises because the explanations lend an air of authority or complexity, whether they might induce confirmation bias by highlighting features that seem to support the AI's conclusion (a particular concern for feature-attribution methods like SHAP), or whether participants simply lack a strong impetus to question the AI when an explanation is provided. Our study into the Anchor condition may help to shed light on this question, as we find no similar effect for Anchor. Instead, we find a significant decrease in participants' stated confidence in AI and a simultaneous increase in participant self-confidence in their own decisions. \textcite{miller_explanation_2017} identifies several features that social sciences would suggest make a good explanation. Among them, Anchor explanations are contrastive, counterfactual, and selective, while the SHAP and Confidence conditions lack these properties. It may be that Anchor explanations serve to highlight strange model behaviour that correctly undermines explainee confidence in model outputs. 

In short, the problem seems not to be the use of explanations as justification tools, but rather the use of ``bad'' explanations as justification tools. This raises another question: if SHAP and Confidence are ``bad'' explanations as in-process justification tools, are they ``good'' explanations for something else?

\section[Participatory Design]{Participatory Design: Ex-Post Decision Support}\label{sec:xaicase}
\subsection{Motivation}
In Section \ref{sec:online}'s investigation of post-hoc explainable AI, we found that SHAP-based explanations can lead to unwarranted trust when used to justify decisions. It is clear, at least, that we should not use SHAP-based explanations as a DST in this context. However, we do not suggest that SHAP should be discarded entirely. In fact, though \emph{Salary} and \emph{Credit} both prove unsuitable tasks for applications of SHAP-based explanations as DSTs, we suggest that we might still make use of the explanations. In particular, in use cases where explainee trust in the underlying model is not at issue, SHAP's induction of unwarranted trust need not undermine its utility.

It is clear that when making a `primary' decision (i.e., the same decision the model output seeks to make), human reviewers working from AI outputs are preeminently concerned with whether to agree with or overrule the model's output. However, after making this decision, they are no longer necessarily concerned with whether to agree or disagree with the model's output. Consider the task of \emph{refining a scholarship selection algorithm}, the \emph{Refinement} task from Chapter \ref{ch:context}. Scholarship and talent investment programmes ordinarily select cohorts in a series of application cycles \cite{li2020hiring}. Between application cycles, they seek to examine their previous selection decisions, and possibly modify their processes to improve these decisions in the future \cite{li2020hiring}; if AI algorithms are used to support these decisions, the review will naturally include refinements to the AI algorithms. In this case, though, we are no longer concerned with whether the AI was correct; rather, we are concerned with whether the way the AI informs decision-making is conducive to ideal selection pipelines.

We conducted a human-centric study using SHAP explanations as an ex-post explanation tool to help selectors from Rise (see Appendix \ref{app:programmes}) with the \emph{Refinement} task. Through this participatory design study, we assess SHAP's usefulness based on the insights these explanations provide.

This study aims to answer RQ2:

\begin{enumerate}
    \item[(RQ2)] If post-hoc xAI methods induce unwarranted trust in-process, could they still be useful ex-post?\footnote{Recall the distinction between ex-post and post-hoc. We use the term `post-hoc' to refer to explanation algorithms that are applied after a model; ex-post, in contrast, refers to the explanation tools applied in decision support scenarios where the primary decisions have already been made.}
\end{enumerate}

\noindent In doing so, we restrict our attention specifically to SHAP, as we have already demonstrated its induction of unwarranted trust.

\subsection{Methodology}\label{ssec:cs_methods}
\subsubsection{Rise's Selection Process}
We use the Rise selection process as a case study for our participatory design study. Rise is a scholarship and talent investment programme that selects cohorts of 100 winners and 500 finalists from their pool of applicants. We focus primarily on this final selection of winners from finalists. At this stage, the programme has already undergone a stage of selection in which applicants submit video essays describing a project they completed 
and complete a cognitive assessment. These video essays are then assessed twice: once by a randomly selected group of the applicant's peers (other applicant), and another time by a group of external experts. These assessments are used once more in the final selection of winners.

Additionally, all finalists are asked to participate in a day-long series of remote workshops where they are asked to complete a variety of tasks in front of a panel of external experts (this panel contains a subset of the external experts who assessed the applicants' video essays). The scores from these workshops are used alongside older metrics to produce algorithmically-generated scores and demographic information is used to test the bias of these scores. All scores, as well as a variety of qualitative factors, are then used to select the 100 winners.\footnote{The programme has requested we not disclose the details of their selection process; some additional details can be found in Appendix \ref{ssec:rise}, but many particulars of the program are excluded by design.}

\subsubsection{Our Study}
We test SHAP's usefulness in a human-centred context. To do so, we ran a study with scholarship and talent investment selectors (N=8) from Rise. Though Rise already used algorithmic scoring to support their decision-making, no selectors possessed experience with post-hoc xAI before the study. The study consisted of two participatory design workshops with said selectors – we term these `G1' and `G2'.\footnote{As Rise only employs a small number of easily identifiable selection selectors, to preserve the anonymity of the participants, we do not number or identify participants. Rather, we attribute quotes based on the workshop group.}

Before this study, we obtained informed consent from all participants. As we ran group workshops (and as we do not attribute comments to individual participants), participants were informed that they would not be able to recuse themselves after the study. Participants also gave consent to be recorded, and to have these recordings stored on a secure server. All recording, transcribing, and data analysis was conducted on secure servers. Ethics review was performed by the University of Oxford's Central University Research Ethics Committee.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\textwidth]{xai/case_flowchart.png}
    \caption{Each workshop consisted of a series of cases relating to a past application decision that was flagged by programme reviewers. In each case, participants were shown slides like in Figure \ref{fig:sample_case} and were asked to analyse the algorithm itself and whether the case warrants changes to the algorithm in future years.}
    \label{fig:case_flowchart}
\end{figure}

Both workshops followed an identical protocol. The flow of these workshops is shown in Figure \ref{fig:case_flowchart}, and more detail on the protocol followed can be found in Appendix \ref{app:xaiprotocol}.

In each workshop, participants discussed several cases, each examining a (possibly successful) applicant from a past application cycle who was flagged by programme reviewers for having perplexing algorithm scores relative to other known information. In each case, participants are asked to use visual, SHAP-based explanations to first understand why programme reviewers found these cases worth noting, then to explore why programme reviewers gave the feedback they did and what caused the algorithm's perplexing outputs, and finally to opine on whether the case suggests that changes should be made to the algorithm (or to the selection process as a whole) for future years. The cases themselves have been redacted, as they contain sensitive information about programme applicants, but a sample case can be seen in Figure \ref{fig:sample_case}.

In analysing our data, we follow \textcite{braun_using_2006}'s methodology for reflexive thematic analysis. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\textwidth]{xai/sample_case.png}
    \caption{Each case explores one applicant from past years chosen by past programme reviewers after being flagged as having perplexing algorithm scores. Each case contains the applicant's profile (overall algorithm scores alongside demographic information; the profile is redacted to preserve applicant anonymity), the programme reviewers' comments, and the SHAP-based explanation (the score names are replaced with generic labels to preserve programme anonymity).}
    \label{fig:sample_case}
    
\end{figure}

% [FIXED] Results – A couple notes of required clarification. First, it was not made evident how "context"/""other contextual factors" were presented to participants – who originally did the ratings vs during the actual study. Were socioeconomic attributes not included in the model as a bias diagnostic (as they commonly are for other systems, such as admissions)? Second, the notion of peer vs expert reviewer seems to be introduced here for the first time. It felt like something essential was missing in the lead-in– what is a "peer reviewer" vs "expert" and how were these reviews assessed / included in the model?  Also please clarify the view that the "expert reviewers were less biased". Perhaps the process needs to be made clearer. 


\subsection{Results}\label{sec:cs_results}
Our case study yielded two key themes. Firstly, SHAP Explanations yield useful ex-post insights about feature importance. Second, even though SHAP yields useful information, the accessibility of such information depends on careful presentation. We now cover these in depth.

\paragraph{Ex-Post Insights on Important Features}
In both groups, several useful insights emerged due to the SHAP visualisations. For example, the relationships between scores and contextual factors (e.g., markers of applicants' socioeconomic status) revealed that context plays little to no role in scoring; despite this, an applicant's context has a strong impact on how selectors read scores.\footnote{Meanwhile, we find in Chapter \ref{ch:diversity} that selectors consider contextualising applications a key motivation behind considerations of diversity.} E.g., an applicant with high test scores from a poor region of Kenya is more impressive than one with high test scores from a rich part of the United Kingdom. When discussing one applicant who was selected, but had particularly low algorithmic scores, one participant said: ``This is one of the candidates that... [was from a] different country and [had] very low income'' (G1).

It was also remarked upon that scores calculated based on the assessment of external experts often disagreed with scores calculated based on the assessment of programme applicants. It was discovered here that, contrary to programme expectations, the programme's expert reviews appeared less prone to biases than the peer ones. For one applicant: ``There was a question about why his peer and expert review were so different...I think confirms that it's not actually that they were seeing dramatically different things...his peers were dinging him for not seeming like he needed the award'' (G1). For another: ``I think this applicant has been significantly brought down by peer reviews; [their] scores are substantially lower than those that were, perhaps, given to [another applicant]'' (G1).

Similarly, it was observed that, unlike project reviews, group activities, and test results, the best candidates do not appear to have particularly good interview scores: ``I'm seeing also quite a few top-ranked candidates whose interview score was really low'' (G1). In some cases, this appeared to create a discrepancy between algorithmically generated overall scores and the participants' perceptions of the best candidates: ``[The applicant's] staff reviews imply that [they] should be top 30, and even if you factor in low interview scores...[they] are still pretty low on the algorithm score'' (G2).

\paragraph{Presentation is Key}
Besides insights about the selection process, the workshops yielded direct feedback on how the presentation of SHAP explanations should be improved. One major point was that contextual factors describing an applicant were missing. One participant said, of the explanation: ``It doesn't give me the context'' (G1). Another from the same group said: ``But I think without the context, it's really hard to decipher what's going on here'' (G1). This could be interpreted as participants asking for supporting information. However, when the researchers read out the information in the explanation's caption, this cleared up the participant's confusion: ``Yeah, that makes sense'' (G1). This suggests that, rather than needing more information, participants needed the information presented differently.

Several times, participants were unclear on the meaning of different aspects of explanations: ``Should I be alarmed and I see it going blue in the context of this? It's really hard for me to if you threw this at me...to compare'' (G1). Participants asked for the more complicated information as a ``Pre-reading'' (G2), and asked for simpler information, i.e., ``Maybe just colour coding things that are positive in one colour and then things that are negative in the other colours'' (G2).

One request that several participants echoed was that axes be kept constant, even between different types of scores: ``It's also different scaling. So, that massive bar...does not mean the same thing as the massive bar meant last time'' (G2). Another solution suggested to a similar problem was the provision of benchmark information: ``Like, give me the benchmark for that'' (G1).

\subsection{Participatory Design Study Findings}\label{ssec:cs_discussion}

Such cases underscore the enduring challenge of embedding fairness into selection processes. While the question of whether to factor in markers of disadvantage or to concentrate solely on `task-relevant' factors is a well-established debate in both selection committees and fairness literature \cite{dwork_fairness_2012}, the primary difficulty often resides not in the question itself, but in the cautious and consistent application of any chosen approach. This is especially true when attempting to translate nuanced human considerations of fairness into algorithmic systems. Consequently, if algorithms do not adequately reflect these preferences for handling disadvantage, decision-makers will likely continue to observe discrepancies between algorithmically-driven selections and their own human-made judgments.

% [FIXED]The seemingly offhand comment "Such cases raise difficult questions about fairness, including whether markers of disadvantage should be factored into decision-making, or if decision-makers should focus only on 'task-relevant' factors [64]." – is this a meta-reflection by the researcher, or part of the codes reported from the data? This section is reporting a thematic analysis of the data and slightly misleads to suggest that this was a view of the participants.  Also, this comment suggests that this is not a well-explored question (which it is – by every selection committee ever!)  and it's not perhaps the question that is difficult but perhaps the cautious application of the answer. While an important topic, that should be an entire discussion section dedicated to this.

Exposure to SHAP explanations appears to have yielded useful insights when used as part of an ex-post decision-making process. In particular, participants appeared to find SHAP explanations useful for indicating when information may have been over- or under-used in selectors' holistic review and in the algorithm itself. In this context, where decisions about individuals have already been made, and the organisation is looking into how to go about selecting its next cohort, the possibility of unwarranted trust in a particular output is not a concern. Rather, these explanations can be regarded as probes or provocations, helping decision-makers hone in on particular cases that highlight potential areas for improvement in decision-making, whether through changes in the model itself, or changes in human evaluation processes (e.g., placing greater or lesser weight on certain features). Thus, we can conclude that SHAP explanations may be useful ex-post when trust in the primary output is not at issue. Interestingly, the wolf-husky example from \textcite{ribeiro_why_2016}'s original LIME proposal could be interpreted as being used similarly; using an explanation of an existing classification to guide changes in the model in future (e.g. by adding an edge detection step before classification, to ignore snow in the background). In both cases, the explanation may reveal unwarranted reliance on (or lack of reliance on) a particular part of the feature space.

However, we also find that the SHAP-based waterfall explanations we provide, alone, lack the detail and presentation required. Practitioners desired additional context in the form of benchmarks, demographics, and selector-written comments; additional modes of interaction (e.g., the ability to change the importance of interview scores in the final algorithm score) with our explanatory materials; and points of comparison to clarify their investigation. This allays \textcite{miller_explainable_2023}'s concern that post-hoc xAI methods might discourage explainees from engaging deeply with the facts of the task. Rather, in this case, the explanations served as a platform for explainees to seek additional information to inform their decisions.

% [FIXED] "Practitioners desired additional modes of interaction with our explanatory materials, and points" – what sort and what materials?

\section{Discussion}
\subsection{Implications}
By delineating DSTs by the stage of the decision they inform, we can answer the question: Should we use post-hoc xAI methods?'' separately for in-process and ex-post decisions. While we find them misleading, and thus dangerous, for in-process decisions, Section \ref{ssec:os_discussion} indicates that these misleading tendencies are not limited to post-hoc xAI, and are rather a symptom of the practice of post-hoc justification more broadly. Furthermore, Section \ref{ssec:cs_discussion} indicates that certain ex-post use cases do not necessarily require that explanations appropriately modulate trust. Thus, post-hoc xAI methods might still inform ex-post decision-making (e.g., selection process refinement, evaluations of selector bias). This yields two direct implications for the xAI field:

\begin{enumerate}
    \item While we reiterate caution around post-hoc justification of model outputs \cite{miller_explainable_2023, Lipton, bansal_does_2021, ford_play_2020, jacobs_how_2021}, we extend this caution from xAI methods to any form of post-hoc justification.
    \item We qualify this caution in its application to ex-post decision-making. We encourage a field that has, in large part, moved on from post-hoc notions of interpretability \cite{kumar_problems_2020,barocas_hidden_2020,Lipton,karimi_algorithmic_2021} to engage with and identify ex-post applications for these tools.
\end{enumerate}

\subsection{The Anchor Problem}\label{ssec:anchor_problem}

% [FIXED] Great hypothesis. Are there other possible explanations for the observation?

Suppose a farmer sees what they believe to be a sheep on a hill, and states ``there is a sheep on that hill''. Now, suppose this farmer sees a cleverly disguised goat, but that there is also a sheep on the hill, only invisible to the farmer. In this case, the farmer has a true belief (``there is a sheep on that hill'') and has justification for it (the goat), but the justification is unrelated to the truth of the belief. In a seminal paper on Epistemology, \textcite{Gettier_1963} discusses this class of problem (now called `Gettier Problems') and maintains that, despite the truth of the farmer's belief, that farmer does not know. In keeping with this tradition, \textcite{Cabitza_Fregosi_Campagner_Natali_2024} argue that, if an explainee is presented with a trust-inducing misleading explanation, even if that explanation induces trust in correct output, then the induced trust is misplaced.

In Section \ref{ssec:os_discussion}, we present what we believe is the most likely explanation for why Anchor explanations do not induce unwarranted trust: unlike SHAP and Confidence, these explanations might reveal concerns in the underlying model's local behaviour. However, other explanations exist. For instance, \textcite{miller_explanation_2017} describes desiderata that make explanations well-suited to most explainees: explanations should be contrastive, counterfactual, selective, and social. While Anchor explanations are not social, they are contrastive, counterfactual, and selective. It may be that these explanations' beneficial effects on trust stem not from an ability to reveal concerns in the underlying model, but rather from these subjective desiderata. Another possibility is that the rule-based nature of Anchor explanations, often presented with precision and coverage metrics, might be perceived by users as more complex, less intuitively appealing, or inherently less certain than the feature attributions of SHAP or a simple confidence score. This perceived complexity or brittleness could lead to increased skepticism and a reluctance to fully trust the AI's output, especially when incorrect, thereby reducing unwarranted trust due to the explanation's format rather than its insight into model flaws. In either of these cases, where the reduction in unwarranted trust is not due to the explanation faithfully revealing model issues, Anchor might still mislead \cite{Lipton}.

\subsection{Limitations and Future Work}
One core limitation of our work relates to the choice of tasks. While \emph{Salary}, \emph{Credit}, and \emph{Refinement} are closely related tasks, the distinction between in-process and ex-post decisions may be complicated by other distinctions between the three tasks. Future work should investigate this distinction in other contexts.

Another major limitation of our work stems from Section \ref{ssec:anchor_problem}'s Anchor problem. We recognise here that, though Chapter \ref{ch:context}, Section \ref{sec:xai_background} and Section \ref{ssec:os_discussion} give a compelling theory for the surprising results surrounding Anchor explanations, alternative explanations (such as the possibility that Anchors are not meaningful enough) exist, and we are unable to rule these out in our work. This calls for additional research or verification to provide better support for our novel findings with regard to Anchor explanations.

Finally, differences between the personalities of our online study and participatory design participants may limit the external validity of our results. Similarly, though we choose popular post-hoc xAI methods \cite{barocas_hidden_2020,kumar_problems_2020,weerts_human-grounded_2019,ribeiro_nothing_2016}, our choice of SHAP and Anchor limits applicability to other methods.

\subsection{Conclusion}
\textcite{miller_explainable_2023} likens explanations provided by SHAP and related explanation systems to ``Bluster'', a hypothetical person that always gives a recommendation, even when unsure, and does their best to justify this. They note that, in the context of decision support, such a person is less valuable than ``Prudence'', who asks the decision-maker's opinion first and then provides feedback, as Bluster risks discouraging explainee engagement with the decision. Here, we distinguish in-process and ex-post \emph{decision stages}. We conclude that, while \textcite{miller_explainable_2023}'s conclusion applies straightforwardly to the in-process stage, post-hoc xAI might still drive engagement and inform ex-post decisions, and urge that more be done to identify and apply post-hoc xAI where it is useful. This chapter both hints at the significance of the distinction between in-process and ex-post decisions and hints towards the question: might other DSTs be more useful in-process?

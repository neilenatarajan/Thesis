\chapter{\label{ch:context}Background and Context}

\minitoc

\section{The Challenge of Global Scholarship Selection}\label{sec:context_challenge}

This thesis addresses fundamental challenges in designing and implementing algorithmic decision support tools (DSTs) for global scholarship selection. As these programmes have grown, traditional processes have become increasingly difficult to manage, sparking interest in algorithmic solutions to enhance efficiency and fairness \cite{Latzer_Hollnbuchner_Just_Saurwein_2014}. Scholarship programmes aim to provide long-term societal benefits by selecting the ``best'' applicants, but what ``best'' means depends on the organisation's theory of change—whether it prioritises the future contributions of scholars, the inherent social good of providing opportunity, or the productivity gains from diverse perspectives \cite{DilraboJonbekova_Ruby_2023, Dassin_Marsh_Mawer_2018, noray2023systemic}.

However, applying algorithmic DSTs to this context is uniquely challenging. Unlike in hiring or university admissions, global scholarship selection must contend with extraordinary diversity in applicants' backgrounds, educational systems, and cultural contexts \cite{Warikoo_2019}. While algorithms have much potential to support unique challenges in this new context, attempting to introduce algorithms into the selection process creates significant technical and ethical hurdles. Critics raise valid concerns about algorithmic bias and the dehumanisation of the selection process \cite{dwork_fairness_2012, binns_its_2018}, though human-led processes are equally vulnerable to critique \cite{Ahnaf2023AHPAP}.

This complex landscape raises several key research questions:
\begin{enumerate}
    \item How can we design DSTs to support fair and effective selection across diverse global populations?
    \item How can we provide meaningful explanations that enhance, rather than undermine, human decision-making?
    \item How should we adapt to emerging technologies like generative AI?
    \item How do we balance competing values of merit, diversity, and social impact?
\end{enumerate}
These questions are fundamentally socio-technical, requiring an interdisciplinary approach that draws on computer science, social science, and critical technology studies.

\section{Historical and Societal Foundations}\label{sec:context_historical_societal}
Contemporary challenges in algorithmic selection must be situated within a broader historical context of discrimination, civil rights, and the evolving role of technology in society. We undertake this situation here.

\subsection{The Pursuit of Equity in Selection}\label{ssec:context_equity}
The modern emphasis on diversity and fairness is rooted in decades of social and political struggle. The U.S. Civil Rights Movement of the 1950s and 1960s challenged systemic exclusion, leading to affirmative action policies designed to redress historical inequities \cite{morris1984origins, anderson2010imperative}. This movement, initially focused on the under-representation of women and racial minorities in the U.S., has evolved into a global concept of diversity that encompasses a wide variety of identities \cite{nkomo2019diversity}.

Global scholarship programmes inherit this legacy. Prestigious programmes like the Rhodes Scholarship have had to evolve to address their own exclusionary histories, such as barring women and Black South Africans for many years \cite{Ziegler_2008}. Contemporary programmes like Rise (a Rhodes Trust programme) and Ellison Scholars, with whom this thesis engages, grapple with similar tensions as they seek to operationalise diversity without perpetuating tokenism. This history provides critical context for understanding why modern selectors prioritise both individual merit and diversity within and across cohorts.

\subsection{Technology, Power, and Social Justice}\label{ssec:context_tech_power}
The pursuit of equity is further complicated by the role of technology. As scholars like \textcite{benjamin2019race} and \textcite{noble2018algorithms} have shown, technological systems can encode and perpetuate societal biases, reinforcing existing power structures. Algorithms, while promising objectivity, often reflect the historical inequities present in their training data and can enact ``categorical violence'' by forcing fluid identities into rigid demographic bins \cite{scheuerman2019computers, bowker1999sorting}.

Thus, technology can democratise access to resources, but it can also entrench new forms of exclusion \cite{rheingold2002smart, eubanks2018automating}. Building technology that serves the former requires attention to what \textcite{jasanoff2004states} calls the `co-production' of science, technology, and social order: the ways in which technological systems both shape and are shaped by social values and political structures. The development of DSTs is therefore not a purely technical exercise but also a political process demanding careful attention to power, representation, and accountability.

\section{Some Conceptual Pillars of Selection-Oriented AI}\label{sec:context_pillars}
In designing DSTs for selection via a process we term Selection-Oriented AI, we tackle several conceptual pillars. We do not claim our list of pillars to be complete, but contend here that it is sound; that is, when designing DSTs for selection processes of the kind explored in this thesis, designers should pay attention to the following.

\subsection{Diversity: From Theory to Practice}\label{ssec:context_diversity}
Diversity is a central, yet ambiguous, concept in selection. It draws from multiple intellectual traditions: social psychology distinguishes between demographic and cognitive diversity \cite{page_diversity_2010}; organisational behaviour links diversity to team performance \cite{page_diversity_2017}; and political philosophy values diversity for a flourishing democracy \cite{mill1859liberty, young1990justice}. Economically, diversity is often quantified via entropy metrics and linked to productivity \cite{noray2023systemic}.

These varied motivations lead to a nebulous definition. This thesis starts with \textcite{page_diversity_2010}'s general definition: ``The heterogeneity of elements in a set'', but acknowledges its limitations when applied to people. In practice, diversity considerations often involve nuanced concepts like \textbf{group diversity} (equitable representation), \textbf{intersectional diversity} (recognising overlapping identities), and \textbf{cognitive diversity} (differences in thinking) \cite{Mitchell_Diversity_2020, Crenshaw_Demarginalizing_1989, Hong_Page_2004_Diversity}. As we explore in Chapter \ref{ch:diversity}, selectors often simultaneously navigate these different meanings.

\subsection{Fairness: Competing Ideals in Algorithmic Systems}\label{ssec:context_fairness}
The algorithmic fairness literature provides a critical lens for evaluating selection processes. A key tension exists between \textbf{individual fairness} (treating similar applicants similarly) and \textbf{group fairness} (achieving parity across demographic groups) \cite{dwork_fairness_2012, pmlr-v80-kearns18a}. This manifests in debates over distributive versus procedural justice. While distinct from diversity, fairness is often intertwined in practice; strategies to enhance one can impact the other, creating complex trade-offs that selection systems must navigate \cite{zhao2023fairness}.

\subsection{Explainable AI (XAI): Promise and Peril}\label{ssec:context_xai}
As machine learning models have grown more complex, XAI has emerged to make their decisions more transparent and trustworthy. This quest for interpretability is not new; it dates back to early rule-based expert systems \cite{shortliffe_mycin_1976}. Modern XAI distinguishes between \textbf{intrinsically interpretable models} (like decision trees) and \textbf{post-hoc explanations} (most often applied to ``black-box'' models that lack native interfaces for interpretability) \cite{molnar_interpretable_2019, rudin_stop_2019}.

However, post-hoc explanations come with the risk of \textbf{automation bias}, where users over-rely on automated advice, and the ``trust paradox'', where plausible but misleading explanations can increase unwarranted trust \cite{mosier_automation_1996, lai_human_2019}. As we investigate in Chapter \ref{ch:xai}, this raises critical questions about when and how explanations should be used in high-stakes selection decisions.

\subsection{Generative AI: A New Frontier of Challenge}\label{ssec:context_genai}
The recent rise of powerful generative AI, particularly Large Language Models (LLMs) like GPT-4, has introduced a new layer of complexity to selection \cite{ashish_vaswani_attention_2017, openai_gpt-4_2023}. Students are increasingly using these tools to write application essays, challenging traditional notions of authorship and assessment \cite{dehouche_plagiarism_2021}. While institutions have been quick to issue policies, these often lack clarity and are difficult to enforce, as current detection tools have significant limitations \cite{MikePerkins_JasperRoe_2023, mitchell_detectgpt_2023}. This new reality forces a re-evaluation of the role of written assessments in selection, a central theme we explore in Chapter \ref{ch:genai}.

\section{Situating the Thesis}\label{sec:context_situating}

This thesis is positioned at the intersection of HCI, AI, and social science, engaging with a sparse body of literature on AI in global scholarship selection. While much has been written on AI in hiring or university admissions, the unique challenges of global scholarships—particularly the need to compare applicants across vastly different contexts—remain under-explored. Our work builds on and extends research in algorithmic fairness, explainable AI, and the economics of talent selection \cite{kleinberg2018algorithmic, li2020hiring}, applying these lenses to a novel empirical setting.

To ground this research, we engaged in longitudinal partnerships with two global scholarship programmes: \textbf{Rise} and \textbf{Ellison Scholars}. Both are innovative programmes seeking to leverage AI in their selection processes. Through Action Research, Value-Sensitive Design, and Participatory Design, we worked with selectors from these organisations to identify key challenges and co-design solutions. These collaborations surfaced several families of subordinate decisions that form the empirical core of this thesis: challenges related to explainability (Chapter \ref{ch:xai}), applicant use of generative AI (Chapter \ref{ch:genai}), and cohort diversity (Chapters \ref{ch:diversity} and \ref{ch:spf}). A full list of the decision points addressed can be found in Table \ref{tab:full_decision_list}.

\begin{table}[htbp]
  \centering
  \caption{This table enumerates relevant challenges facing selectors from the Rise and Ellison Scholars selection teams. Challenges are drawn from discussions with selectors, where descriptions are framed in terms of decisions these programs make.}
  \label{tab:full_decision_list}
  \adjustbox{max width=\textwidth}{
  \begin{tabular}{l r p{0.33\linewidth}p{0.33\linewidth}}
      \toprule
      Challenges & Chapter(s) & Description & Supporting Information \\
      \midrule
      \emph{Refinement} & \ref{ch:xai} and \ref{ch:spf} & A programme may refine its scoring algorithm each year to better score applicants. & Explanations of perplexing AI-generated scores; information about implications of scoring methods for cohort diversity \\ 
      \emph{Diligence} & \ref{ch:genai} & A programme may make holistic decisions about when and how to consider applicants. & Information about which essays (and which parts of essays) were written by GenAI; information about whether the GenAI-written passages are hallucinations. \\ 
      \emph{Partners} & \ref{ch:genai} & A programme may determine whether to continue channel partnerships, which encourage and support applicants. & Whether any channel partners' affiliated applicants use GenAI disproportionately. \\
      \emph{Pipeline} & \ref{ch:genai} & A programme may decide whether to modify their application material or process. & Information about the usage of GenAI throughout the application pipeline. \\
      \emph{Gameability} & \ref{ch:genai} & A programme may decide how to modify their application material or process. & Information about how AI-generated essays are scored under the current application process. \\
      \emph{Disqualification} & \ref{ch:genai} & A programme may decide to disqualify an applicant that violates their application guidelines. & Information about whether essays violate application guidelines around GenAI usage. \\
      \emph{Diversity} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make cohort-level decisions regarding the diversity of their cohort. & Information about the diversity of possible cohorts. \\
      \emph{Contribution} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make decisions about which applicants to move forward based on their contribution to diversity. & Information about the impact of including different applicants on cohort diversity. \\
      \bottomrule
  \end{tabular}
  }
\end{table}

\chapter{\label{ch:context}Background and Context}

% Should be unified and expanded to properly set the scene for the problems addressed in the thesis.
% ⭐️ Layout – It is not clear what the narrative structure of Ch 2 is, sections themselves seem disconnected and unordered – please come up with a more sensible ordering. For instance, you could start with defining the problem scope more clearly, then bring in a historical background on positive discrimination (from 6.3.1) up here, including discussing the civil rights movement etc. Background on the interdisciplinary treatment and discussion of diversity / diversities from 6.3.2 could be very welcome here as well. Perhaps actually avoiding defining diversity in Chapter 1 as a Key Term until then might be beneficial to avoid misleading readers that you are sticking only to the simplistic mathematical definition throughout.

% This section might also be an appropriate place where you provide the xAI background from Chapter 4 – connecting this to SOAI by casting the Decision Matrix and SPF from Ch 7 as kinds of xAI?  (Would this work?). Consolidating the background bits would allow each chapter to simply focus on novel findings. 

% ⭐️ Broader Background Expected – The thesis is quite thin on background for a DPhil thesis–DPhil theses are expected to provide a robust background establishing not only directly prior work but also the greater research context, research methods applied, and so on.  Overall as it stands, the experiment of putting "relevant background alongside each chapter" does not help, because a consolidated approach really allows for sufficient breadth of to be achieved in a simple singular place relating key elements, whereas separately these end up seeming individually thin and quite fragmented.  A consolidated and slightly more broader background chapter would be better.

% [TODO] VERIFY CITATIONS
% [TODO] ADD CITATIONS: anderson2010imperative, young1990justice, cotton2023chatting, floridi2023ai, beck2023human, mehrabi2021survey, hayes2011knowing, seaver2017algorithms, harrison1998time, williams2013demography, mill1859liberty, crenshaw1989demarginalizing, collins2002black, bowker1999sorting, rheingold2002smart, eubanks2018automating, jasanoff2004states, barocas_fairness_2016

\minitoc

\section{The Imperative and Complexity of Modern Scholarship Selection}\label{sec:context_problem_scope_value}

This thesis addresses fundamental challenges in the design and implementation of algorithmic decision support tools (DSTs) for global scholarship selection. As scholarship programmes have grown in scale and scope, traditional human-led selection processes have become increasingly difficult to manage effectively \cite{Latzer_Hollnbuchner_Just_Saurwein_2014}. This has led to a burgeoning interest in algorithmic solutions that promise to enhance both the efficiency and fairness of these critical selection processes.

Scholarship programmes, by their nature, offer substantial long-term benefits to their chosen scholars. The underlying rationale often posits that these benefits accrue not only to the individuals but also to society at large \cite{DilraboJonbekova_Ruby_2023,Dassin_Marsh_Mawer_2018}. Theories explaining this societal benefit vary: some emphasize the future contributions of scholars, who may be empowered and disposed to address global problems or bring returns to their communities \cite{Dassin_Marsh_Mawer_2018}. Concerns such as 'brain drain' are also noted in this context. Other theories contend that the act of providing scholarships to deserving recipients is inherently pro-social. For instance, \textcite{minkin2023diversity} argue that society benefits from a diversity of perspectives, which can be fostered by enabling access to higher education for those who might otherwise be excluded. Evidence also suggests that a broader range of perspectives can yield tangible benefits like increased productivity \cite{autor2008does,noray2023systemic}. Beyond organizational gains, the social mobility facilitated by scholarship programmes is often considered an intrinsic societal good \cite{Dassin_Marsh_Mawer_2018}. Under any of these frameworks, selecting the ``best'' applicants is paramount, though the definition of ``best'' is contingent upon the operative theory of change and societal goals.

However, the application of algorithmic DSTs to scholarship selection presents unique challenges that extend beyond those encountered in other algorithmic decision-making contexts. Unlike more standardized applications in areas such as hiring or university admissions, global scholarship selection must contend with extraordinary diversity across applicants' backgrounds, educational systems, cultural contexts, and opportunities \cite{Warikoo_2019}. This diversity, while often a valued outcome of selection processes, creates significant technical and ethical hurdles for algorithmic support systems. The use of DSTs is not without controversy; critics raise concerns about potential biases \cite{dwork_fairness_2012} and the dehumanisation of the selection process \cite{binns_its_2018}. It is important to note, however, that evolving discussions about fairness in selection render older, human-led selection processes equally vulnerable to critique \cite{Ahnaf2023AHPAP,pmlr-v80-kearns18a}.\footnote{More discussion on the value-laden aspects of selection can be found in Chapter \ref{ch:discussion}. In particular, we revisit fairness in Chapter \ref{ssec:fairness}; we also discuss the position of this research in structures of power in Chapter \ref{sec:reflexivity}.}

The central research questions that emerge from this complex landscape concern how to design DSTs that can:
\begin{enumerate}
  \item Support fair and effective selection across diverse global populations.
  \item Provide meaningful explanations that enhance rather than undermine human decision-making.
  \item Adapt to emerging technological challenges such as the proliferation of generative AI.
  \item Balance competing values of merit, diversity, and social impact.
\end{enumerate}

These challenges are not merely technical but are fundamentally socio-technical, requiring deep engagement with the social, political, and ethical dimensions of selection processes. This thesis therefore adopts an interdisciplinary approach, drawing on computer science, social science, and critical technology studies to understand and address these multifaceted challenges.

\section{Historical and Societal Foundations}\label{sec:context_historical_societal}

Understanding contemporary challenges in algorithmic selection requires situating them within broader historical contexts of discrimination, civil rights movements, and the evolving role of technology in relation to social equity and power structures.

\subsection{The Evolution of Selection Practices and the Pursuit of Equity}\label{ssec:context_evolution_equity}
The modern emphasis on diversity and fairness in scholarship selection emerges from decades of social movements and policy reforms. Beginning with the U.S. Civil Rights Movement of the 1950s–1960s, activists challenged systemic exclusion based on race, gender, and socioeconomic status, leading to affirmative action policies in education. These policies sought to redress historical inequities by considering applicants' backgrounds in selection processes—a precursor to contemporary debates about diversity metrics. Despite its global reach, contemporary discourse on diversity derives (in large part) from the political context of the United States in the latter half of the 20th century \cite{nkomo2019diversity}. Civil rights activists identified gender, race, disability, and other forms of group identity as loci of discrimination and oppression, and constructed political actions around these identities \cite{morris1984origins}. This yielded civil rights laws, including equal treatment laws to protect applicants from discrimination. \textcite{nkomo2019diversity} argue that this initial, U.S.-centric perspective on anti-discrimination, focused on the under-representation of women and racial minorities, has evolved into a more global concept of diversity encompassing a variety of identities. With an increase in social pressure for representation, organisations increasingly prioritise diversity in their selection procedures \cite{hsieh2019allocation,minkin2023diversity}.

The concept of positive discrimination—or affirmative action—emerged from these civil rights struggles as a means of addressing historical injustices and their ongoing effects \cite{anderson2010imperative}. This approach recognizes that formal equality before the law is insufficient to address the cumulative effects of past discrimination and ongoing systemic barriers \cite{young1990justice}. In the context of education and scholarship selection, positive discrimination policies have sought to ensure that talented individuals from historically marginalized groups have access to opportunities that might otherwise be unavailable to them.

Global scholarship programmes inherited this legacy but face unique challenges in balancing meritocracy with representativeness. For instance, the Rhodes Scholarship's exclusionary history (barring women until 1977 and Black South Africans until 1991) underscores how even prestigious programmes must evolve to address past injustices \cite{Ziegler_2008}. Contemporary programmes like Rise and Ellison Scholars, with which this thesis engages (see Section \ref{ssec:context_empirical_setting}), grapple with similar tensions, seeking to operationalize diversity without perpetuating tokenism or essentialism. This historical context provides critical grounding for understanding why modern selectors prioritize both individual merit and cohort-level diversity—a tension explored in later chapters, particularly through the SOAI framework.

\subsection{Technology, Power, and Social Justice}\label{ssec:context_tech_power_justice}
The pursuit of equity through selection has not fully addressed the historical injustices embedded in technological systems and power structures. As \textcite{benjamin2019race} argues in discussing the "New Jim Code," technological systems often encode and perpetuate racial hierarchies, with ostensibly neutral technologies serving to reinforce existing power structures. This is particularly evident in algorithmic systems, where \textcite{noble2018algorithms} demonstrates how search engines and other technologies can reproduce and amplify racial and gender biases. Similarly, \textcite{oneill2016weapons} shows how mathematical models and algorithms can systematically disadvantage already marginalized groups, while \textcite{winner1980artefacts} reminds us that technological artefacts themselves can embody political values and power relations. Latour's actor-network theory also provides a critical lens for analyzing AI's role in selection, suggesting that algorithms are not passive tools but active agents in shaping social outcomes.

These insights suggest that achieving true diversity and fairness requires not just equal treatment under the law, but also a critical examination of how technological systems and power structures perpetuate historical injustices. In the context of scholarship selection, this means recognizing that algorithmic DSTs are not neutral tools but socio-technical systems that can either challenge or reinforce existing inequalities \cite{barocas_fairness_2016}. While algorithms promise objectivity, they often encode historical biases (e.g., training data reflecting past inequities, such as the underrepresentation of Global South applicants) and can enact categorical violence by forcing fluid identities into rigid demographic bins \cite{scheuerman2019computers, bowker1999sorting}.

The relationship between technology and social change is complex and contested. While some scholars emphasize technology's potential to democratize access to opportunities and level playing fields \cite{rheingold2002smart}, others highlight how technological systems can entrench existing power structures and create new forms of exclusion \cite{eubanks2018automating}. This tension is particularly relevant in the context of global scholarship selection, where algorithmic DSTs hold the promise of making selection processes more fair and inclusive while simultaneously risking the automation of bias and the reinforcement of global inequalities. Understanding this tension requires attention to what \textcite{jasanoff2004states} call the "co-production" of science, technology, and social order—the ways in which technological systems both shape and are shaped by social relations, cultural values, and political structures. This perspective emphasizes that the development and deployment of algorithmic DSTs is not merely a technical exercise but a fundamentally social and political process that requires careful attention to issues of power, representation, and accountability.

\section{Core Conceptual Pillars}\label{sec:context_conceptual_pillars}

The design and evaluation of DSTs for scholarship selection rest upon several core concepts. This section elucidates these pillars, drawing from diverse disciplinary perspectives.

\subsection{Diversity: Meanings, Measurement, and Interdisciplinary Insights}\label{ssec:context_diversity}
The concept of diversity sits at the heart of many contemporary debates about fairness and inclusion in selection processes. However, the term's ubiquity often masks significant conceptual ambiguities and practical challenges.

Diversity as a concept draws from multiple intellectual traditions. From social psychology comes an understanding of diversity as difference in observable attributes (demographic diversity) or underlying values and perspectives (deep-level diversity) \cite{harrison1998time}. From organizational behavior comes evidence that diverse teams can outperform homogeneous ones under certain conditions, though the relationship is complex and context-dependent \cite{williams2013demography}. From political philosophy comes a normative understanding of diversity as intrinsically valuable for democratic societies. \textcite{mill1859liberty} argues that exposure to a diversity of opinions and ways of life is essential for individual and social flourishing. \textcite{young1990justice} develops this insight in her argument for a "politics of difference" that recognizes and affirms group differences rather than seeking to transcend them through appeals to universal principles. Sociologically, diversity emphasizes representational equity and power dynamics \cite{benjamin2019race,noble2018algorithms}, while economics often quantifies diversity via entropy metrics and explores its links to productivity gains \cite{noray2023systemic, page_diversity_2010}. Philosophy also debates whether diversity intrinsically benefits epistemic communities \cite{page_diversity_2010}.

With this wealth of disparate motivations, it is unsurprising that a singular definition of diversity remains elusive. \textcite{page_diversity_2010} offers a helpful generic definition: ``The heterogeneity of elements in a set about a class that takes different values, such as species in an eco-environment, or ethnicity in a population''. While suitably broad, this definition lacks the specificity required to build supporting technologies \cite{hupont2021diverse,page_diversity_2010}. The challenge of defining diversity becomes even more complex when we move beyond simple demographic categories to consider intersectionality—the ways in which multiple dimensions of identity interact to create unique experiences of privilege and marginalization \cite{crenshaw1989demarginalizing}. \textcite{collins2002black} argues that understanding these intersections is crucial for developing more inclusive approaches to diversity that don't privilege some differences while ignoring others.

The variety of approaches to measuring diversity reflects the ambiguity of its definition. For instance, a common way to measure diversity, following \textcite{page_diversity_2010}'s definition, is to report percentages of different elements, e.g., demographics in a population. But in scientific fields like biology and ecology, diversity is often measured with different methods, yielding occasionally conflicting results \cite{xu2020diversity}. Measuring diversity in human societies presents unique challenges that go beyond those faced in non-human contexts. Human diversity encompasses complex, socially constructed categories that resist simple quantification. As \textcite{scheuerman2019computers} notes, attempts to reduce human diversity to measurable attributes often fail to capture the nuanced, intersectional nature of human identity. Furthermore, the measurement of diversity raises important questions about representation and voice: Who has the authority to define relevant categories of difference? How do we account for the dynamic and contextual nature of identity? How do we balance the need for measurement with respect for individual privacy and self-determination? These questions become particularly acute in the context of algorithmic systems, where categories and measurements become embedded in code and can have far-reaching consequences \cite{bowker1999sorting}.

These interdisciplinary perspectives converge in scholarship selection, where "diversity" might simultaneously refer to: (1) Demographic mirroring of applicant pools; (2) Cognitive variety in problem-solving approaches; or (3) Redistribution of opportunities to marginalized groups. The \textbf{Diversity Triangle} framework (Chapter 6) operationalizes these definitions, but their historical roots in civil rights activism and critiques of technological determinism \cite{winner1980artefacts} must be articulated here to contextualize later empirical findings.

\subsection{Fairness in Algorithmic Systems}\label{ssec:context_fairness}
The algorithmic fairness literature provides a scaffold for evaluating selection processes, but its axioms often conflict in practice. A key tension exists between \textbf{individual fairness} (ensuring similar applicants receive similar outcomes) and \textbf{group fairness} (achieving parity across demographic categories). In scholarship contexts, this manifests as debates surrounding:
\begin{itemize}
    \item \textbf{Distributive justice}: Should resources be allocated based on need, merit, or demographic representation?
    \item \textbf{Procedural justice}: Do applicants perceive the process as transparent and unbiased?
\end{itemize}
The work of \textcite{dwork_fairness_2012} on fairness constraints and \textcite{pmlr-v80-kearns18a} on multigroup fairness informs the SOAI paradigm's hybrid approach discussed later in this thesis. By requiring systems to satisfy selector-defined fairness metrics while tracking real-world outcomes (e.g., scholar success rates), SOAI aims to bridge theoretical ideals with operational realities.

Another complication arises when we consider the relationship between diversity and similar notions from AI ethics like fairness. While these concepts are distinct in their traditional definitions, they can become intertwined in practice, particularly in the context of algorithmic decision-making. \textcite{zhao2023fairness} argue that ``fairness works can be re-interpreted through the lens of diversity, and strategies enhancing diversity have proven efficacious in improving fairness''. However, the relationship between diversity and fairness is not always straightforward. Pursuing diversity may sometimes conflict with other values such as merit or individual fairness \cite{dwork_fairness_2012}. Understanding these tensions and trade-offs is crucial for designing selection systems that can balance competing values in principled ways.

\subsection{Explainable AI (XAI): Motivations, Methods, and Limitations}\label{ssec:context_xai}
The development of increasingly complex machine learning models has raised fundamental questions about transparency and accountability in algorithmic decision-making. Explainable AI (XAI) has emerged as a response to these concerns, promising to make algorithmic decisions more interpretable and trustworthy.

The quest for interpretable AI systems predates modern machine learning by decades. Early expert systems, developed in the 1970s and 1980s, were explicitly designed to be interpretable through rule-based reasoning \cite{shortliffe_mycin_1976}. These systems encoded human expertise into explicit rules and could explain their decisions by tracing through these rules. This approach contrasted sharply with statistical modeling, where a statistician would select a model with a known structure based on domain knowledge and theoretical considerations \cite{breiman_statistical_2001}. Modern XAI techniques inherit strengths and limitations from these precursors, evolving through distinct waves: \textbf{First-wave XAI} (1980s–2000s) comprised rule-based systems with limited adaptability; \textbf{Second-wave XAI} (2010s) focused on post-hoc explanations for black-box models; and \textbf{Third-wave XAI} (2020s) strives for intrinsically interpretable architectures.

The rise of more complex machine learning models has intensified questions about what it means for a model to be "interpretable." \textcite{lipton_mythos_2018} identifies several distinct notions of interpretability: \emph{Transparency} (understanding internal mechanisms), \emph{Simulatability} (mental simulation of decision process), \emph{Decomposability} (explaining individual components), and \emph{Algorithmic transparency} (understanding training/optimization). These notions highlight a key tension: should interpretability be intuitive for non-technical users, or technically precise and mathematically rigorous? \textcite{rudin_stop_2019} advocates for the latter, while \textcite{miller_explanation_2017} draws from social science to argue for intuitive, human-like explanations.

Before examining XAI's effects on trust, it is crucial to understand automation bias: the human tendency to over-rely on automated systems \cite{mosier_automation_1996, parasuraman_automation_2000, skitka_automation_1999}. This bias manifests as errors of commission (following incorrect automated advice) or omission (failing to act when automation fails) \cite{mosier_automation_1996}, suggesting that observed trust in AI may be partly due to this general tendency, not just explanation quality. In scholarship selection, post-hoc explanations (e.g., highlighting essay features influencing scores) risk this "trust paradox"—over-reliance on superficially plausible justifications.

Explanation methods can be categorized by when and how explanations are generated. \textcite{molnar_interpretable_2019} distinguishes between intrinsically interpretable models (e.g., decision trees, linear models) and post-hoc explanations (applied after training to often non-interpretable models).\footnote{While the field often uses terms like "explainable" vs "interpretable" or "black box" vs "glass box," we follow \textcite{molnar_interpretable_2019}'s distinction between intrinsically interpretable models and post-hoc explanations. In particular, we avoid the "glass box" framing to emphasize the stage at which the explanation is considered – thus, even if an explanation can access a model's hidden states, if that explanation is not a feature of the model's design, it qualifies as post-hoc.} \textcite{friedrich_taxonomy_2011} identifies three main paradigms for explanation outputs: Feature-Based (e.g., SHAP, LIME highlighting feature importance), Example-Based (similar cases or counterfactuals), and Rule-Based (decision rules or conditions).

Critiques of XAI are multifaceted. Mathematically, \textcite{doshi-velez_towards_2017} proposes evaluating methods by properties like sensitivity and consistency. \textcite{kumar_problems_2020} argues many popular methods like SHAP lack crucial properties such as contrastiveness. However, mathematical critiques alone don't capture real-world performance. Human-centered evaluations reveal complex effects on trust and decision-making; for instance, \textcite{lai_human_2019} finds different explanation types can variably affect trust, sometimes increasing it even for incorrect models. The SOAI framework (Chapter 4) addresses some of these issues by embedding explainability directly into the Decision Matrix, ensuring explanations align with decision stakes and stages.

\subsection{The Challenge of Generative AI in Academic and Selection Contexts}\label{ssec:context_genai}
The emergence of powerful generative AI (genAI) systems has fundamentally transformed the landscape of academic assessment and evaluation, creating new challenges for institutions seeking to assess individual capability and knowledge, including within selection processes.

Since \textcite{ashish_vaswani_attention_2017} introduced the transformer architecture, AI has made rapid progress. More recently, large language models (LLMs) like BERT \cite{jacob_devlin_bert_2018} and GPT-3 \cite{brown_language_2020} have demonstrated the ability to generate human-like text. The releases of GPT-3.5 and GPT-4o have made these models more powerful and ubiquitous, and students are increasingly using them to write essays and other application materials \cite{openai_gpt-4_2023,dehouche_plagiarism_2021}. The rapid adoption of these technologies has outpaced the development of institutional policies and assessment practices \cite{cotton2023chatting}. Unlike previous forms of academic support tools, generative AI can produce original text on any topic, making traditional plagiarism detection methods less effective \cite{mitchell_detectgpt_2023}.

Many research institutions are making their stances on pedagogical integrity clear. \textcite{h_holden_thorp_chatgpt_2023}, in an editorial from \emph{Science}, declares that ``the word `original' is enough to signal that text written by ChatGPT is not acceptable: It is, after all, plagiarized from ChatGPT''. However, while institutions face pressure to publish universal guidelines, researchers themselves are free to debate the theoretical and ethical implications of genAI usage \cite{lav_r_varshney_limits_2020,h_holden_thorp_chatgpt_2023,yu_huang_reflection_2023}. \textcite{MikePerkins_JasperRoe_2023} point out a problematic lack of clarity in policies that allow genAI for some uses but not others. Under many such guidelines, detection of genAI is not sufficient to determine whether programme policy has been violated, as the nature of genAI usage must be compared to programme guidelines. These policy challenges reflect deeper questions about the nature of authorship, creativity, and assessment in an age of AI assistance \cite{floridi2023ai}. As \textcite{beck2023human} argues, the integration of AI into writing processes challenges traditional notions of individual authorship and requires new frameworks for understanding collaborative human-AI creativity.

\section{Methodological Framework of the Thesis}\label{sec:context_methodology}

This thesis employs a mixed-methods approach that combines computational evaluation with qualitative investigation of social and organizational contexts. This section outlines the key methodological frameworks that inform the research, grounding it in established practices while tailoring them to the unique demands of studying AI in global scholarship selection.

\subsection{Core Research Traditions}\label{ssec:context_core_traditions}
The methodological framework is grounded in several key traditions:
\begin{itemize}
    \item \textbf{Human-Computer Interaction (HCI) and Participatory Design (PD):} Following \textcite{blythe2014research} and \textcite{Knapp_Zeratzky_Kowitz_2016}, this thesis adopts PD methodologies that center the experiences and needs of users—in this case, scholarship selectors. This approach recognizes that effective DSTs cannot be designed in isolation from their contexts of use, but must emerge through collaborative engagement with stakeholders \cite{braun_using_2006}. The thesis also employs participatory design and action research methodologies (see below) to ensure that research questions and findings are grounded in the real-world experiences of scholarship selectors \cite{hayes2011knowing}.
    \item \textbf{Action Research (AR):} Building on \textcite{Hayes_2011} and \textcite{bradbury_action_2003}, this thesis incorporates AR methodologies that emphasize "research with, rather than on, people." This approach is particularly important in the context of scholarship selection, where power dynamics and organizational constraints significantly shape the possibilities for technological intervention \cite{lu_organizing_2023}.
    \item \textbf{Value Sensitive Design (VSD):} The thesis draws on VSD frameworks \cite{VanKleek_Seymour_Binns_Shadbolt_2018} to ensure that algorithmic systems are designed with explicit attention to human values and their implications. This is crucial in selection contexts, where decisions have profound impacts on individuals' lives and opportunities.
    \item \textbf{Critical Algorithm Studies:} Following scholars such as \textcite{noble2018algorithms} and \textcite{oneill2016weapons}, this thesis adopts a critical perspective on algorithmic systems that examines not only their technical performance but their social and political implications. This perspective is essential for understanding how algorithmic DSTs may reproduce or challenge existing inequalities.
\end{itemize}

\subsection{Evaluation Methods}\label{ssec:context_evaluation_methods}
To assess both technical performance and socio-technical implications, the thesis employs:
\begin{itemize}
    \item \textbf{Computational Evaluation:} Standard machine learning evaluation metrics including accuracy, precision, recall, and F1-scores are used to assess algorithmic system performance. Recognizing the limitations of these metrics in capturing fairness concerns, the research also employs fairness-aware evaluation methods including demographic parity, equalized odds, and calibration analysis \cite{mehrabi2021survey}.
    \item \textbf{Human-Centered Evaluation of XAI:} Following \textcite{doshi-velez_towards_2017}, the thesis employs both functionally-grounded evaluation (measuring how explanations affect task performance) and human-grounded evaluation (measuring how explanations align with human cognitive processes). This multi-faceted approach recognizes that technical performance alone is insufficient for understanding the social implications of algorithmic systems.
\end{itemize}

\subsection{Critical and Reflexive Stance}\label{ssec:context_reflexivity}
Drawing on critical algorithm studies and science and technology studies (STS), the thesis adopts a reflexive approach that examines not only the technical properties of algorithmic systems but their social, political, and ethical implications \cite{seaver2017algorithms}. This includes attention to questions of power, representation, and accountability that are often overlooked in purely technical approaches to algorithm design. These approaches emphasize collaboration and shared ownership of the research process, recognizing that effective technological solutions must emerge from genuine engagement with user communities.


\section{Situating the Research: AI in Selection, Related Work, and Empirical Context}\label{sec:context_situating_research}

This final section positions the thesis within existing literature on AI in selection contexts, clarifies its specific contributions by identifying gaps in related work, and describes the empirical setting of the research conducted with partner scholarship programmes.

\subsection{AI and Decision Support in Talent Selection}\label{ssec:context_ai_talent_selection}
The application of AI systems to support human decision-making in selection contexts represents a growing area of research and practice, with significant implications for fairness, efficiency, and social outcomes. While there is good reason to express scepticism towards predictive technology in recruitment, \textcite{Vereschak_Alizadeh_Bailly_Caramiaux_2024} demonstrate that, as long as AI systems prove trustworthy and well-designed, decision-makers will engage with and rely on these systems. Apprehension with these systems ranges from concerns about bias to disquiet about the distance between applicant and organisation \cite{Lashkari_Cheng_2023}.

New research explores a framing of individual applicant aptitude and overall group diversity \cite{noray2023systemic}, and many applications demonstrate that technology might improve both. For example, \textcite{bergman2021seven} show that replacing traditional testing mechanisms with prediction algorithms allows placement of students into remedial classes that improves student performance and increases minority representation in non-remedial courses. However, the deployment of algorithmic systems in selection contexts also raises significant concerns about fairness and accountability. \textcite{barocas_big_2016} argue that big data applications can systematically exclude members of vulnerable groups, while \textcite{pasquale2015black} highlights the challenges of ensuring accountability when algorithmic systems operate as "black boxes."

This thesis also contributes to a growing economics literature studying the influence and potential of algorithms. Most research in this area has focused on measuring the effects of AI tools both at the economy-wide (`macro') level \cite{acemoglu2022automation,babina2024artificial} and in more specific (`micro') contexts like bail decisions and radiology \cite{albright2023hidden,kleinberg2015prediction,stevenson2019algorithmic}. Work like \textcite{kleinberg2018algorithmic} introduces algorithms that colleges can use to calculate the most meritorious cohort at any threshold of representation for a single underrepresented group. Similarly, \textcite{li2020hiring} demonstrates that, within a Fortune 500 tech company, diversity and productivity of hires can be improved by using exploration-based applicant screening algorithms. The economic perspective on algorithmic selection systems emphasizes both their potential benefits in terms of efficiency and scale, and their risks in terms of market concentration and inequality \cite{autor2020work}. Understanding these economic dimensions is crucial for designing selection systems that can achieve social goals while maintaining economic sustainability.

\subsection{Related Work and Contributions}\label{ssec:context_related_work}
This thesis engages primarily with literature seeking to use AI tools to support decision-making in global scholarship selection processes. Unfortunately (or perhaps fortunately), this particular niche of literature is fairly sparse. While many tools do exist, especially those seeking to automate the job of the scholarship selector entirely, we are forced to look beyond this niche for a larger body of related literature. Primarily, we do this by considering other selection contexts (e.g., universities or recruiters). Note that while this body of literature is large, and contains work ranging from automated essay scoring to intellect testing \cite{cozma_automated_2018,condon2014international}, our work is only tangentially related to these fields. Some work explores applicant perceptions of scholarship selection processes \cite{10.1145/3351095.3372867}, but this work is primarily interested in the decision subjects and does not engage with the design of selection processes.

More closely related is the body of algorithmic fairness literature engaging with recruitment, much of which seeks to ensure that AI tools do not discriminate against protected classes \cite{dwork_fairness_2012}. However, as we explore in this chapter, disanalogies between recruitment and global scholarship selection limit the applicability of this work. Similarly, much work explores the impact of algorithms on educational outcomes. For example, \textcite{NISSENBAUM1998237} explore the risk of algorithmic involvement in education dehumanising the experience. However, though these works are conducted in the same environment, they do not touch on selection itself.

This chapter also explores a model of selection as subordinate decisions and investigations supporting the ultimate decision about which subset of applicants to select as scholars. Many of these subordinate decisions relate to their own bodies of literature; these are explored in the relevant chapters of this thesis.

\subsection{Engaging with Scholarship Programmes: Rise and Ellison Scholars}\label{ssec:context_empirical_setting}
Unsurprisingly, little research explores the unique and novel context of AI in global scholarship selection. In part, this is due to the necessity of longitudinal research with or on organisations. The novelty of these programmes, combined with an understandable desire to avoid the scrutiny arising from public identification in research, has made research with or on these programmes challenging.\footnote{Critics may point to the Rhodes Scholarship, founded in 1902, as an example of a long-standing global scholarship organisation \cite{Ziegler_2008}. However, the programme's history of intentional exclusionism challenges its claim to global reach; the Rhodes Scholarship did not accept women until legally mandated in 1977 \cite{Ziegler_2008}, and continued to discriminate against Black South Africans until the legalisation of the African National Congress and the end of Apartheid in 1991 \cite{Ziegler_2008}. Furthermore, though the Rhodes Scholarship does select a range of applicants from around the world, they do so via regional committees that compare applicants from similar backgrounds, thus sidestepping the central difficulty of global selection.}

Fortunately, this thesis engages with two such organisations, Rise and Ellison Scholars. Both are already seeking to embed AI and computational DSTs into their processes (e.g., Ellison Scholars already uses an AI-based automated essay scoring system). Both programmes have asked that they not be identified in public-facing research, and thus we request that reviewers not share details on either programme and to protect the identities of the programmes where possible. For more details on the programmes, see Appendix \ref{app:programmes}.

\begin{table}[htbp]
  \centering
  \caption{This table enumerates relevant challenges facing selectors from the Rise and Ellison Scholars selection teams. Challenges are drawn from discussions with selectors, where descriptions are framed in terms of decisions these programs make.}
  \label{tab:full_decision_list}
  \adjustbox{max width=\textwidth}{
  \begin{tabular}{l r p{0.33\linewidth}p{0.33\linewidth}}
      \toprule
      Challenges & Chapter(s) & Description & Supporting Information \\
      \midrule
      \emph{Refinement} & \ref{ch:xai} and \ref{ch:spf} & A programme may refine its scoring algorithm each year to better score applicants. & Explanations of perplexing AI-generated scores; information about implications of scoring methods for cohort diversity \\ 
      \emph{Diligence} & \ref{ch:genai} & A programme may make holistic decisions about when and how to consider applicants. & Information about which essays (and which parts of essays) were written by genAI; information about whether the genAI-written passages are hallucinations. \\ 
      \emph{Partners} & \ref{ch:genai} & A programme may determine whether to continue channel partnerships, which encourage and support applicants. & Whether any channel partners' affiliated applicants use genAI disproportionately. \\
      \emph{Pipeline} & \ref{ch:genai} & A programme may decide whether to modify their application material or process. & Information about the usage of genAI throughout the application pipeline. \\
      \emph{Gameability} & \ref{ch:genai} & A programme may decide how to modify their application material or process. & Information about how AI-generated essays are scored under the current application process. \\
      \emph{Disqualification} & \ref{ch:genai} & A programme may decide to disqualify an applicant that violates their application guidelines. & Information about whether essays violate application guidelines around genAI usage. \\
      \emph{Diversity} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make cohort-level decisions regarding the diversity of their cohort. & Information about the diversity of possible cohorts. \\
      \emph{Contribution} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make decisions about which applicants to move forward based on their contribution to diversity. & Information about the impact of including different applicants on cohort diversity. \\
      \bottomrule
  \end{tabular}
  }
\end{table}

We engage these programmes, variously, in Action Research (AR), Value Sensitive Design (VSD), and Participatory Design (PD). In working with the Rise and Ellison Scholars programmes to support solutions to the central \emph{Selection} decision, the programmes expressed interest in supporting many subordinate decisions and challenges. While some of them, such as automating the essay scoring process, fall outside the scope of this thesis, we isolate three families of challenges that engage with AI or HCI literature and are thus of both programme and research interest. These families are challenges supported by explainable AI algorithms, challenges arising from applicant usage of genAI, and challenges relating to the diversity of selected cohorts. Table \ref{tab:full_decision_list} enumerates challenges of interest to us that are addressed in subsequent chapters of this thesis. 
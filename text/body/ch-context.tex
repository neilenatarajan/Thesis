\chapter{\label{ch:context}Background and Context}

% Should be unified and expanded to properly set the scene for the problems addressed in the thesis.
% ⭐️ Layout – It is not clear what the narrative structure of Ch 2 is, sections themselves seem disconnected and unordered – please come up with a more sensible ordering. For instance, you could start with defining the problem scope more clearly, then bring in a historical background on positive discrimination (from 6.3.1) up here, including discussing the civil rights movement etc. Background on the interdisciplinary treatment and discussion of diversity / diversities from 6.3.2 could be very welcome here as well. Perhaps actually avoiding defining diversity in Chapter 1 as a Key Term until then might be beneficial to avoid misleading readers that you are sticking only to the simplistic mathematical definition throughout.

% This section might also be an appropriate place where you provide the xAI background from Chapter 4 – connecting this to SOAI by casting the Decision Matrix and SPF from Ch 7 as kinds of xAI?  (Would this work?). Consolidating the background bits would allow each chapter to simply focus on novel findings. 

% ⭐️ Broader Background Expected – The thesis is quite thin on background for a DPhil thesis–DPhil theses are expected to provide a robust background establishing not only directly prior work but also the greater research context, research methods applied, and so on.  Overall as it stands, the experiment of putting "relevant background alongside each chapter" does not help, because a consolidated approach really allows for sufficient breadth of to be achieved in a simple singular place relating key elements, whereas separately these end up seeming individually thin and quite fragmented.  A consolidated and slightly more broader background chapter would be better.

% [TODO] VERIFY CITATIONS
% [TODO] ADD CITATIONS: anderson2010imperative, young1990justice, cotton2023chatting, floridi2023ai, beck2023human, mehrabi2021survey, hayes2011knowing, seaver2017algorithms, harrison1998time, williams2013demography, mill1859liberty, crenshaw1989demarginalizing, collins2002black, bowker1999sorting, rheingold2002smart, eubanks2018automating, jasanoff2004states, barocas_fairness_2016

\minitoc

\section{Problem Scope and Research Context}\label{sec:problem_scope}

This thesis addresses fundamental challenges in the design and implementation of algorithmic decision support tools (DSTs) for global scholarship selection. As scholarship programmes have grown in scale and scope, traditional human-led selection processes have become increasingly difficult to manage effectively \cite{Latzer_Hollnbuchner_Just_Saurwein_2014}. This has led to growing interest in algorithmic solutions that promise to enhance both the efficiency and fairness of selection processes.

However, the application of algorithmic DSTs to scholarship selection presents unique challenges that extend beyond those encountered in other algorithmic decision-making contexts. Unlike standardized applications of algorithmic decision-making in areas such as hiring or university admissions, global scholarship selection must contend with extraordinary diversity across applicants' backgrounds, educational systems, cultural contexts, and opportunities \cite{Warikoo_2019}. This diversity, while being a valued outcome of selection processes, creates significant technical and ethical challenges for algorithmic support systems.

The central research questions that emerge from this context concern how to design DSTs that can:
\begin{enumerate}
\item Support fair and effective selection across diverse global populations
\item Provide meaningful explanations that enhance rather than undermine human decision-making
\item Adapt to emerging technological challenges such as generative AI
\item Balance competing values of merit, diversity, and social impact
\end{enumerate}

These challenges are not merely technical but fundamentally socio-technical, requiring deep engagement with the social, political, and ethical dimensions of selection processes. This thesis therefore adopts an interdisciplinary approach, drawing on computer science, social science, and critical technology studies to understand and address these challenges.

\section{Methodological Foundations}\label{sec:methods_background}

This thesis employs a mixed-methods approach that combines quantitative evaluation of algorithmic systems with qualitative investigation of their social and organizational impacts. The methodological framework is grounded in several key traditions:

\subsection{Human-Computer Interaction and Participatory Design}
Following \textcite{blythe2014research} and \textcite{Knapp_Zeratzky_Kowitz_2016}, this thesis adopts participatory design (PD) methodologies that center the experiences and needs of users—in this case, scholarship selectors. This approach recognizes that effective DSTs cannot be designed in isolation from their contexts of use, but must emerge through collaborative engagement with stakeholders \cite{braun_using_2006}.

\subsection{Action Research}
Building on \textcite{Hayes_2011} and \textcite{bradbury_action_2003}, this thesis incorporates Action Research (AR) methodologies that emphasize "research with, rather than on, people." This approach is particularly important in the context of scholarship selection, where power dynamics and organizational constraints significantly shape the possibilities for technological intervention \cite{lu_organizing_2023}.

\subsection{Value Sensitive Design}
The thesis draws on Value Sensitive Design (VSD) frameworks \cite{VanKleek_Seymour_Binns_Shadbolt_2018} to ensure that algorithmic systems are designed with explicit attention to human values and their implications. This is crucial in selection contexts, where decisions have profound impacts on individuals' lives and opportunities.

\subsection{Critical Algorithm Studies}
Following scholars such as \textcite{noble2018algorithms} and \textcite{oneill2016weapons}, this thesis adopts a critical perspective on algorithmic systems that examines not only their technical performance but their social and political implications. This perspective is essential for understanding how algorithmic DSTs may reproduce or challenge existing inequalities.

\section{The Social Value of Selection}\label{sec:social_value}
Scholarship programmes offer long-term benefits to their chosen scholars, often under the theory that providing these benefits improves not only the welfare of the scholars themselves but of society as a whole \cite{DilraboJonbekova_Ruby_2023,Dassin_Marsh_Mawer_2018}. Theories of the mechanisms of this social benefit vary. Some theories rely on the future actions of the chosen scholars. For example, \textcite{Dassin_Marsh_Mawer_2018} argue that scholars are often empowered and disposed to devote themselves to solving global problems. \textcite{Dassin_Marsh_Mawer_2018} also note that these scholars may bring additional returns to their communities, thereby improving the welfare of a broad group of people (though they also express concern over `brain drain', where these scholars do not return to their communities). In contrast, others contend that the mere provision of scholarships to the correct recipients is itself pro-social. \textcite{minkin2023diversity} note that society benefits from making space for a breadth of perspectives; providing scholarships to those who would otherwise be unable to afford higher education may create that breadth of perspectives. Some evidence suggests that this broader range of perspectives brings additional benefits in the form of increased productivity \cite{autor2008does,noray2023systemic}. Besides the gain to organisations, though, some argue that the social mobility brought about by the existence of scholarship programmes yields inherent benefits to society \cite{Dassin_Marsh_Mawer_2018}. Under any of these theories, selecting the ``best'' applicants as scholars is clearly in society's best interest. However, as we explore throughout this thesis, different theories of change yield different definitions of ``best''.

Traditional selection is a human-led process, and many suggest it should remain that way \cite{Latzer_Hollnbuchner_Just_Saurwein_2014}. However, as the number of applicants to scholarship programmes grows, the need for scalable selection processes has grown; with traditional selection processes unequipped to handle these new challenges, organisations are forced to innovate, often turning to algorithmic solutions that offer to solve these difficult problems \cite{Latzer_Hollnbuchner_Just_Saurwein_2014}. This has led to the development of Decision Support Tools (DSTs) to support selection processes. These DSTs range from simple tools like automated essay scoring to more complex tools like AI-driven selection algorithms. The use of these tools is not without controversy. Critics argue that these tools may be biased \cite{dwork_fairness_2012}, or may dehumanise the selection process \cite{binns_its_2018}. However, while certain features of these tools may succumb to some critiques, evolving discussions about fairness in selection render older, human-led selection processes equally vulnerable to critique \cite{Ahnaf2023AHPAP,pmlr-v80-kearns18a}.\footnote{More discussion on the value-laden aspects of selection can be found in Chapter \ref{ch:discussion}. In particular, we revisit fairness in Chapter \ref{ssec:fairness}; we also discuss the position of this research in structures of power in Chapter \ref{sec:reflexivity}.}

\section{Historical Context: Civil Rights, Technology, and Power}\label{sec:historical_context}

Understanding contemporary challenges in algorithmic selection requires situating them within broader historical contexts of discrimination, civil rights movements, and the role of technology in perpetuating or challenging social inequalities.

\subsection{The Civil Rights Movement and Positive Discrimination}\label{ssec:civil_rights}
Despite its global reach, contemporary discourse on diversity derives (in large part) from the political context of the United States in the latter half of the 20th century \cite{nkomo2019diversity}. Civil rights activists identified gender, race, disability, and other forms of group identity as loci of discrimination and oppression, and constructed political actions around these identities \cite{morris1984origins}. This yielded civil rights laws, including equal treatment laws to protect applicants from discrimination (e.g., in employment). \textcite{nkomo2019diversity} argue that this initial, U.S.-centric perspective on anti-discrimination in the workplace, which focused on the under-representation of women and racial minorities, has evolved into a more global concept of diversity encompassing a variety of identities. With an increase in social pressure for representation, organisations increasingly prioritise diversity in their selection procedures \cite{hsieh2019allocation,minkin2023diversity}.

The concept of positive discrimination—or affirmative action—emerged from these civil rights struggles as a means of addressing historical injustices and their ongoing effects \cite{anderson2010imperative}. This approach recognizes that formal equality before the law is insufficient to address the cumulative effects of past discrimination and ongoing systemic barriers \cite{young1990justice}. In the context of education and scholarship selection, positive discrimination policies have sought to ensure that talented individuals from historically marginalized groups have access to opportunities that might otherwise be unavailable to them.

\subsection{Historical Injustice in Technological Systems}\label{ssec:historical_injustice}
However, this evolution has not fully addressed the historical injustices embedded in technological systems and power structures. As \textcite{benjamin2019race} argues, technological systems often encode and perpetuate racial hierarchies, with ostensibly neutral technologies serving to reinforce existing power structures. This is particularly evident in algorithmic systems, where \textcite{noble2018algorithms} demonstrates how search engines and other technologies can reproduce and amplify racial and gender biases. Similarly, \textcite{oneill2016weapons} shows how mathematical models and algorithms can systematically disadvantage already marginalized groups, while \textcite{winner1980artefacts} reminds us that technological artefacts themselves can embody political values and power relations.

These insights suggest that achieving true diversity requires not just equal treatment under the law, but also critical examination of how technological systems and power structures perpetuate historical injustices. In the context of scholarship selection, this means recognizing that algorithmic DSTs are not neutral tools but socio-technical systems that can either challenge or reinforce existing inequalities \cite{barocas_fairness_2016}.

\subsection{Technology and Social Change}
The relationship between technology and social change is complex and contested. While some scholars emphasize technology's potential to democratize access to opportunities and level playing fields \cite{rheingold2002smart}, others highlight how technological systems can entrench existing power structures and create new forms of exclusion \cite{eubanks2018automating}. This tension is particularly relevant in the context of global scholarship selection, where algorithmic DSTs hold the promise of making selection processes more fair and inclusive while simultaneously risking the automation of bias and the reinforcement of global inequalities.

Understanding this tension requires attention to what \textcite{jasanoff2004states} call the "co-production" of science, technology, and social order—the ways in which technological systems both shape and are shaped by social relations, cultural values, and political structures. This perspective emphasizes that the development and deployment of algorithmic DSTs is not merely a technical exercise but a fundamentally social and political process that requires careful attention to issues of power, representation, and accountability.

\section{Diversity: Concepts, Measurements, and Challenges}\label{sec:diversity_background}

The concept of diversity sits at the heart of many contemporary debates about fairness and inclusion in selection processes. However, the term's ubiquity often masks significant conceptual ambiguities and practical challenges.

\subsection{Theoretical Foundations of Diversity}\label{ssec:diversity_theory}
Diversity as a concept draws from multiple intellectual traditions. From social psychology comes an understanding of diversity as difference in observable attributes (demographic diversity) or underlying values and perspectives (deep-level diversity) \cite{harrison1998time}. From organizational behavior comes evidence that diverse teams can outperform homogeneous ones under certain conditions, though the relationship is complex and context-dependent \cite{williams2013demography}.

From political philosophy comes a normative understanding of diversity as intrinsically valuable for democratic societies. \textcite{mill1859liberty} argues that exposure to a diversity of opinions and ways of life is essential for individual and social flourishing. \textcite{young1990justice} develops this insight in her argument for a "politics of difference" that recognizes and affirms group differences rather than seeking to transcend them through appeals to universal principles.

\subsection{Defining Diversity}\label{ssec:defining_diversity}
With the wealth of disparate motivations for diversity, it is unsurprising that its definition is often unclear. \textcite{page_diversity_2010} offers a helpful generic definition of diversity: ``The heterogeneity of elements in a set about a class that takes different values, such as species in an eco-environment, or ethnicity in a population''. While suitably broad, this definition lacks the specificity required to build supporting technologies \cite{hupont2021diverse,page_diversity_2010}.

The challenge of defining diversity becomes even more complex when we move beyond simple demographic categories to consider intersectionality—the ways in which multiple dimensions of identity interact to create unique experiences of privilege and marginalization \cite{crenshaw1989demarginalizing}. \textcite{collins2002black} argues that understanding these intersections is crucial for developing more inclusive approaches to diversity that don't privilege some differences while ignoring others.

\subsection{Measuring Diversity: Challenges and Approaches}\label{ssec:measuring_diversity}
The variety of approaches to measuring diversity reflects the ambiguity of its definition. For instance, the natural way to measure diversity, on \textcite{page_diversity_2010}'s definition, is to report percentages of those different elements, e.g., demographics in a population. But in scientific fields like biology and ecology, diversity is often measured with different methods and occasionally conflicting results \cite{xu2020diversity}.

However, measuring diversity in human societies presents unique challenges that go beyond those faced in non-human contexts. Human diversity encompasses complex, socially constructed categories that resist simple quantification. As \textcite{scheuerman2019computers} notes, attempts to reduce human diversity to measurable attributes often fail to capture the nuanced, intersectional nature of human identity.

Furthermore, the measurement of diversity raises important questions about representation and voice. Who has the authority to define relevant categories of difference? How do we account for the dynamic and contextual nature of identity? How do we balance the need for measurement with respect for individual privacy and self-determination? These questions become particularly acute in the context of algorithmic systems, where categories and measurements become embedded in code and can have far-reaching consequences \cite{bowker1999sorting}.

\subsection{The Relationship Between Diversity and Fairness}\label{ssec:diversity_fairness}
Another complication arises when we consider the relationship between diversity and similar notions from AI ethics like fairness. While these concepts are distinct in their traditional definitions, they can become intertwined in practice, particularly in the context of algorithmic decision-making. \textcite{zhao2023fairness} argue that ``fairness works can be re-interpreted through the lens of diversity, and strategies enhancing diversity have proven efficacious in improving fairness''.

However, the relationship between diversity and fairness is not always straightforward. Pursuing diversity may sometimes conflict with other values such as merit or individual fairness \cite{dwork_fairness_2012}. Understanding these tensions and trade-offs is crucial for designing selection systems that can balance competing values in principled ways.

\section{Explainable AI: Foundations and Critiques}\label{sec:xai_background}

The development of increasingly complex machine learning models has raised fundamental questions about transparency and accountability in algorithmic decision-making. Explainable AI (XAI) has emerged as a response to these concerns, promising to make algorithmic decisions more interpretable and trustworthy.

\subsection{The Quest for Interpretable AI}\label{ssec:interpretable_ai}
The quest for interpretable AI systems predates modern machine learning by decades. Early expert systems, developed in the 1970s and 1980s, were explicitly designed to be interpretable through rule-based reasoning \cite{shortliffe_mycin_1976}. These systems encoded human expertise into explicit rules and could explain their decisions by tracing through these rules. This approach contrasted sharply with statistical modeling, where a statistician would select a model with a known structure based on domain knowledge and theoretical considerations \cite{breiman_statistical_2001}.

However, the rise of more complex machine learning models has raised fundamental questions about what it means for a model to be "interpretable." \textcite{lipton_mythos_2018} identifies several distinct notions of interpretability that have emerged:

\begin{enumerate}
    \item \emph{Transparency}: Understanding the internal mechanisms of the model itself
    \item \emph{Simulatability}: The ability to mentally simulate the model's decision process
    \item \emph{Decomposability}: The ability to explain individual components and parameters
    \item \emph{Algorithmic transparency}: Understanding the training process and optimization
\end{enumerate}

These different notions highlight a key tension: should interpretability be intuitive and accessible to non-technical users, or should it be technically precise and mathematically rigorous? \textcite{rudin_stop_2019} argues for the latter, suggesting that interpretability should be grounded in formal mathematical properties. In contrast, \textcite{miller_explanation_2017} draws from social science to argue that explanations should be intuitive and follow human explanation patterns.

\subsection{Automation Bias and Trust in AI Systems}\label{ssec:automation_bias}
Before examining the specific effects of xAI explanations on trust, it is crucial to understand the baseline phenomenon of automation bias - the tendency of humans to over-rely on automated systems and their outputs. This bias has been extensively documented across various domains, from aviation to healthcare to decision support systems \cite{mosier_automation_1996, parasuraman_automation_2000, skitka_automation_1999}.

Automation bias manifests in two primary ways: errors of commission, where users follow incorrect automated advice, and errors of omission, where users fail to take action when the automated system fails to provide advice \cite{mosier_automation_1996}. This bias is particularly relevant in the context of xAI, as it suggests that any observed trust in AI systems may be partially attributable to this general tendency rather than to the specific characteristics of the explanations provided.

\subsection{A Taxonomy of Explanation Methods}\label{ssec:explanation_taxonomy}
One fundamental distinction lies in when and how explanations are generated. \textcite{molnar_interpretable_2019} distinguishes between intrinsically interpretable models and post-hoc explanations. Intrinsically interpretable models, such as decision trees or linear models, are designed from the ground up to be understandable, with their structure directly reflecting their decision-making process. In contrast, post-hoc explanations are applied after a model has been trained, attempting to make sense of decisions made by models that weren't designed for interpretability.\footnote{While the field often uses terms like "explainable" vs "interpretable" or "black box" vs "glass box," we follow \textcite{molnar_interpretable_2019}'s distinction between intrinsically interpretable models and post-hoc explanations. In particular, we avoid the "glass box" framing to emphasize the stage at which the explanation is considered – thus, even if an explanation can access a model's hidden states, if that explanation is not a feature of the model's design, it qualifies as post-hoc.}

The design philosophy of explanation outputs offers another way to categorize these methods. \textcite{friedrich_taxonomy_2011} identifies three main paradigms:

\begin{enumerate}
    \item Feature-Based explanations, which highlight the importance of different input features in the model's decision. These include methods like SHAP and LIME, which assign importance scores to features.
    \item Example-Based explanations, which provide similar cases or counterfactuals to help users understand the decision boundary.
    \item Rule-Based explanations, which present decision rules or conditions that led to the model's output.
\end{enumerate}

\subsection{Critiques of Explainable AI}\label{ssec:xai_critiques}
The evaluation of xAI methods has generated a rich body of critiques, which can be understood through different lenses and methodologies. One major strand of critique focuses on the mathematical properties of explanation methods. \textcite{doshi-velez_towards_2017} provides a framework for evaluating explanation methods based on their mathematical properties, such as sensitivity to input perturbations and consistency across similar inputs. \textcite{kumar_problems_2020} extends this analysis, arguing that many popular methods like SHAP lack crucial properties like contrastiveness.

However, mathematical critiques alone don't capture how these methods function in real-world decision-making contexts. This has led to a growing body of human-centered evaluations that reveal complex patterns in how explanations affect trust and decision-making. For instance, \textcite{lai_human_2019} finds that different types of explanations can have varying effects on trust, with some increasing trust even when the model is incorrect.

\section{Generative AI and Academic Integrity}\label{sec:genai_background}

The emergence of powerful generative AI systems has fundamentally transformed the landscape of academic assessment and evaluation, creating new challenges for institutions seeking to assess individual capability and knowledge.

\subsection{The Rise of Generative AI}\label{ssec:genai_rise}
Since \textcite{ashish_vaswani_attention_2017} introduced transformer architecture, AI has made rapid progress. More recently, large language models (LLMs) like BERT and GPT3 have demonstrated the ability to generate human-like text \cite{jacob_devlin_bert_2018,brown_language_2020}. The releases of GPT3.5 and GPT4o have made these models more powerful and ubiquitous, and students are increasingly using them to write essays \cite{openai_gpt-4_2023,dehouche_plagiarism_2021}.

The rapid adoption of these technologies has outpaced the development of institutional policies and assessment practices \cite{cotton2023chatting}. Unlike previous forms of academic support tools, generative AI can produce original text on any topic, making traditional plagiarism detection methods less effective \cite{mitchell_detectgpt_2023}.

\subsection{Institutional Responses and Policy Challenges}\label{ssec:institutional_responses}
Many research institutions make their stances on pedagogical integrity clear. \textcite{h_holden_thorp_chatgpt_2023}, in an editorial from \emph{Science}, declares that ``the word `original' is enough to signal that text written by ChatGPT is not acceptable: It is, after all, plagiarized from ChatGPT''. However, while research institutions face pressure to publish universal guidelines, researchers themselves are free to debate the theoretical and ethical implications of genAI usage \cite{lav_r_varshney_limits_2020,h_holden_thorp_chatgpt_2023,yu_huang_reflection_2023}.

\textcite{MikePerkins_JasperRoe_2023} point out a problematic lack of clarity in policies that allow genAI for some uses, but not others. Under many of these guidelines, detection of genAI is not sufficient to determine whether programme policy has been violated, as the nature of the usage of genAI must be compared to the programme guidelines.

These policy challenges reflect deeper questions about the nature of authorship, creativity, and assessment in an age of AI assistance \cite{floridi2023ai}. As \textcite{beck2023human} argues, the integration of AI into writing processes challenges traditional notions of individual authorship and requires new frameworks for understanding collaborative human-AI creativity.

\section{AI and Decision Support in Selection Contexts}\label{sec:ai_selection}

The application of AI systems to support human decision-making in selection contexts represents a growing area of research and practice, with significant implications for fairness, efficiency, and social outcomes.

\subsection{Algorithmic Decision-Making in Talent Selection}\label{ssec:algorithmic_selection}
While there is good reason to express scepticism towards predictive technology in recruitment, \textcite{Vereschak_Alizadeh_Bailly_Caramiaux_2024} demonstrate that, as long as AI systems prove trustworthy and well-designed, decision-makers will engage with and rely on these systems. Apprehension with these systems ranges from concerns about bias to disquiet about the distance between applicant and organisation \cite{Lashkari_Cheng_2023}.

New research explores a framing of individual applicant aptitude and overall group diversity \cite{noray2023systemic}, and many applications demonstrate that technology might improve both. For example, \textcite{bergman2021seven} show that replacing traditional testing mechanisms with prediction algorithms allows placement of students into remedial classes that improves student performance and increases minority representation in non-remedial courses.

However, the deployment of algorithmic systems in selection contexts also raises significant concerns about fairness and accountability. \textcite{barocas_big_2016} argue that big data applications can systematically exclude members of vulnerable groups, while \textcite{pasquale2015black} highlights the challenges of ensuring accountability when algorithmic systems operate as "black boxes."

\subsection{Economic Perspectives on Selection}\label{ssec:economic_perspectives}
This thesis contributes to a growing economics literature studying the influence and potential of algorithms. Most research in this area has focused on measuring the effects of AI tools both at the economy-wide (`macro') level \cite{acemoglu2022automation,babina2024artificial} and in more specific (`micro') contexts like bail decisions and radiology \cite{albright2023hidden,kleinberg2015prediction,stevenson2019algorithmic}.

Work like \textcite{kleinberg2018algorithmic} introduces algorithms that colleges can use to calculate the most meritorious cohort at any threshold of representation for a single underrepresented group. Similarly, \textcite{li2020hiring} demonstrates that, within a Fortune 500 tech company, diversity and productivity of hires can be improved by using exploration-based applicant screening algorithms.

The economic perspective on algorithmic selection systems emphasizes both their potential benefits in terms of efficiency and scale, and their risks in terms of market concentration and inequality \cite{autor2020work}. Understanding these economic dimensions is crucial for designing selection systems that can achieve social goals while maintaining economic sustainability.

\section{Research Methods and Approaches}\label{sec:research_methods}

This thesis employs a mixed-methods approach that combines computational evaluation with qualitative investigation of social and organizational contexts. This section outlines the key methodological frameworks that inform the research.

\subsection{Computational Evaluation Methods}
The thesis employs standard machine learning evaluation metrics including accuracy, precision, recall, and F1-scores to assess the performance of algorithmic systems. However, recognizing the limitations of these metrics in capturing fairness concerns, the research also employs fairness-aware evaluation methods including demographic parity, equalized odds, and calibration analysis \cite{mehrabi2021survey}.

\subsection{Human-Centered Evaluation}
Following \textcite{doshi-velez_towards_2017}, the thesis employs both functionally-grounded evaluation (measuring how explanations affect task performance) and human-grounded evaluation (measuring how explanations align with human cognitive processes). This multi-faceted approach recognizes that technical performance alone is insufficient for understanding the social implications of algorithmic systems.

\subsection{Participatory and Action Research Methods}
The thesis employs participatory design and action research methodologies to ensure that research questions and findings are grounded in the real-world experiences of scholarship selectors. These approaches emphasize collaboration and shared ownership of the research process, recognizing that effective technological solutions must emerge from genuine engagement with user communities \cite{hayes2011knowing}.

\subsection{Critical and Reflexive Approaches}
Drawing on critical algorithm studies and science and technology studies, the thesis adopts a reflexive approach that examines not only the technical properties of algorithmic systems but their social, political, and ethical implications \cite{seaver2017algorithms}. This includes attention to questions of power, representation, and accountability that are often overlooked in purely technical approaches to algorithm design.

\section{Related Work}
This thesis engages primarily with literature seeking to use AI tools to support decision-making in global scholarship selection processes. Unfortunately (or perhaps fortunately), this particular niche of literature is fairly sparse. While many tools do exist, especially those seeking to automate the job of the scholarship selector entirely, we are forced to look beyond this niche for a larger body of related literature. Primarily, we do this by considering other selection contexts (e.g., universities or recruiters). Note that while this body of literature is large, and contains work ranging from automated essay scoring to intellect testing \cite{cozma_automated_2018,condon2014international}, our work is only tangentially related to these fields. Some work explores applicant perceptions of scholarship selection processes \cite{10.1145/3351095.3372867}, but this work is primarily interested in the decision subjects and does not engage with the design of selection processes.

More closely related is the body of algorithmic fairness literature engaging with recruitment, much of which seeks to ensure that AI tools do not discriminate against protected classes \cite{dwork_fairness_2012}. However, as we explore in this chapter, disanalogies between recruitment and global scholarship selection limit the applicability of this work.

Similarly, much work explores the impact of algorithms on educational outcomes. For example, \textcite{NISSENBAUM1998237} explore the risk of algorithmic involvement in education dehumanising the experience. However, though these works are conducted in the same environment, they do not touch on selection itself.

This chapter also explores a model of selection as subordinate decisions and investigations supporting the ultimate decision about which subset of applicants to select as scholars. Many of these subordinate decisions relate to their bodies of literature; these are explored in the relevant chapters.

\section{On Working with Scholarship Programmes}
Unsurprisingly, little research explores this unique and novel context. In part, this is due to the necessity of longitudinal research with or on organisations. The novelty of these programmes, combined with an understandable desire to avoid the scrutiny arising from public identification in research, has made research with or on these programmes challenging.\footnote{Critics may point to the Rhodes Scholarship, founded in 1902, as an example of a long-standing global scholarship organisation \cite{Ziegler_2008}. However, the programme's history of intentional exclusionism challenges its claim to global reach; the Rhodes Scholarship did not accept women until legally mandated in 1977 \cite{Ziegler_2008}, and continued to discriminate against Black South Africans until the legalisation of the African National Congress and the end of Apartheid in 1991 \cite{Ziegler_2008}. Furthermore, though the Rhodes Scholarship does select a range of applicants from around the world, they do so via regional committees that compare applicants from similar backgrounds, thus sidestepping the central difficulty of global selection.}

Fortunately, this thesis engages with two such organisations, Rise and Ellison Scholars. Both are already seeking to embed AI and computational DSTs into their processes (e.g., Ellison Scholars already uses an AI-based automated essay scoring system). Both programmes have asked that they not be identified in public-facing research, and thus we request that reviewers not share details on either programme and to protect the identities of the programmes where possible. For more details on the programmes, see Appendix \ref{app:programmes}.

\begin{table}[htbp]
  \centering
  \caption{This table enumerates relevant challenges facing selectors from the Rise and Ellison Scholars selection teams. Challenges are drawn from discussions with selectors, where descriptions are framed in terms of decisions these programs make.}
  \label{tab:full_decision_list}
  \adjustbox{max width=\textwidth}{
  \begin{tabular}{l r p{0.33\linewidth}p{0.33\linewidth}}
      \toprule
      Challenges & Chapter(s) & Description & Supporting Information \\
      \midrule
      \emph{Refinement} & \ref{ch:xai} and \ref{ch:spf} & A programme may refine its scoring algorithm each year to better score applicants. & Explanations of perplexing AI-generated scores; information about implications of scoring methods for cohort diversity \\ 
      \emph{Diligence} & \ref{ch:genai} & A programme may make holistic decisions about when and how to consider applicants. & Information about which essays (and which parts of essays) were written by genAI; information about whether the genAI-written passages are hallucinations. \\ 
      \emph{Partners} & \ref{ch:genai} & A programme may determine whether to continue channel partnerships, which encourage and support applicants. & Whether any channel partners' affiliated applicants use genAI disproportionately. \\
      \emph{Pipeline} & \ref{ch:genai} & A programme may decide whether to modify their application material or process. & Information about the usage of genAI throughout the application pipeline. \\
      \emph{Gameability} & \ref{ch:genai} & A programme may decide how to modify their application material or process. & Information about how AI-generated essays are scored under the current application process. \\
      \emph{Disqualification} & \ref{ch:genai} & A programme may decide to disqualify an applicant that violates their application guidelines. & Information about whether essays violate application guidelines around genAI usage. \\
      \emph{Diversity} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make cohort-level decisions regarding the diversity of their cohort. & Information about the diversity of possible cohorts. \\
      \emph{Contribution} & \ref{ch:diversity} and \ref{ch:spf} & A programme may make decisions about which applicants to move forward based on their contribution to diversity. & Information about the impact of including different applicants on cohort diversity. \\
      \bottomrule
  \end{tabular}
  }
\end{table}

We engage these programmes, variously, in AR, VSD, and PD. In working with the Rise and Ellison Scholars programmes to support solutions to the central \emph{Selection} decision, the programmes expressed interest in supporting many subordinate decisions and challenges. While some of them, such as automating the essay scoring process, fall outside the scope of this thesis, we isolate three families of challenges that engage with AI or HCI literature and are thus of both programme and research interest. These families are challenges supported by explainable AI algorithms, challenges arising from applicant usage of genAI, and challenges relating to the diversity of selected cohorts. Table \ref{tab:full_decision_list} enumerates challenges of interest to us. 